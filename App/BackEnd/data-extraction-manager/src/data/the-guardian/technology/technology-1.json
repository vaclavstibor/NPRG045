[
    {
        "id": "8a6b62e8-d036-4527-907d-27c18981c766",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/09/why-is-elon-musk-suing-sam-altman-openai",
        "title": "The feud between Elon Musk and Sam Altman – explained",
        "author": "",
        "published_date": "2024-03-09T12:00:31+00:00",
        "content": "The day after OpenAI launched in December 2015, its co-founder Sam Altman sat down with Vanity Fair to discuss what the magazine described as “a non-profit company to save the world from a dystopian future”. Altman talked up his vision for keeping artificial intelligence safe and distributing it widely, as well as his good working relationship with his co-chair – Tesla CEO Elon Musk. “I really trust him, which is obviously important to everyone involved,” Altman said.   Related: OpenAI fires back at Elon Musk in legal fight over breach of contract claims   Almost a decade later, Musk and Altman are locked in a public spat and looming legal battle that revolves around the end of their previous partnership and OpenAI’s creation of a for-profit subsidiary now valued at $80bn. Musk filed a suit against OpenAI in a California court last week, alleging that Altman and other executives had “breached the founding agreement” of the company by pursuing private commercial success instead of working to benefit humanity. “Mr Altman caused OpenAI to radically depart from its original mission,” Musk’s suit states. The lawsuit pits the most prominent name in artificial intelligence against one of the world’s richest men and escalates a years-long feud between Musk and Altman. It also adds to a growing number of lawsuits facing OpenAI, as a slew of authors and news outlets allege that the company violated copyright laws and illegally used original works to train its AI tools. Musk’s complaint is a sprawling document that gives his account of the founding of OpenAI, as well as Musk’s views on AI’s danger to society. The core of the suit, however, revolves around Musk’s allegation that OpenAI broke an initial agreement to share its technology with the public and help humanity when the company took billions of dollars in investment from Microsoft and turned into a largely for-profit venture. It also alleges that OpenAI has partially achieved the creation of Artificial General Intelligence – broadly defined as when AI is on par with human cognitive ability – which poses “perhaps the greatest existential threat we face today”. “OpenAI Inc has been transformed into a closed-source, de facto subsidiary of the largest technology company in the world: Microsoft,” the suit states. OpenAI pushed back against Musk’s suit in a lengthy blog posted to its website on Wednesday. Musk supported turning the company into a for-profit entity before he left the board in 2018 and attempted to merge OpenAI with Tesla to become CEO of both, Altman and other OpenAI executives allege in the post. OpenAI’s response accuses Musk, who launched his own generative AI company just last year, of professional jealousy. “We’re sad that it’s come to this with someone whom we’ve deeply admired—someone who inspired us to aim higher, then told us we would fail, started a competitor, and then sued us when we started making meaningful progress towards OpenAI’s mission without him,” the post states. OpenAI included several emails with Musk in its post, including one in which the company’s chief scientist Ilya Sutskever states that “it’s totally OK to not share the science” behind their AI since open-sourcing the technology could allow it to fall into unscrupulous hands. Musk replies in an email, “Yup.”   Related: AI doesn’t cause harm by itself. We should worry about the people who control it | Kenan Malik   Musk has mocked OpenAI and Altman on his social media platform X, formerly Twitter, in the days since the company issued its reply. The 52-year-old billionaire posted a meme renaming OpenAI as “ClosedAI,” as well as shared a photo of Altman edited to show him holding up a company badge with “ClosedAI” on it, referencing the company’s pivot to keeping its technology under wraps. Legal experts have cast doubt on Musk’s suit against OpenAI, stating that he may not have legal standing to sue the company for changing its values. Claims OpenAI violated its fiduciary obligations would also potentially require the suit to be handled in Delaware where the company is incorporated, rather than California where the suit was filed. Likewise, many AI experts believe the milestone of artificial general intelligence is still far away, throwing a shadow over the suit’s assertion that OpenAI is already there. Also unclear is whether there is any concrete founding contract at all, since the suit does not mention a binding document with formal agreements and signatures of the company’s founders. Instead, the complaint details a loose series of discussions, emails and documents – handshake agreements, in effect – that may not hold up to legal scrutiny. The origins of Musk and Altman’s feud Before their partnership turned acrimonious, Musk was something of a mentor figure to Altman. The two met in the early 2010s while Altman was becoming increasingly powerful in Silicon Valley through his work at the startup accelerator YCombinator, and Musk was already a tech mogul. A partner at YCombinator took Altman on a tour of Musk’s SpaceX rocket company, which Altman has repeatedly described as an inspiring moment. “[Musk] talked in detail about manufacturing every part of the rocket, but the thing that sticks in memory was the look of absolute certainty on his face when he talked about sending large rockets to Mars,” Altman wrote in a 2019 blog post. “I left thinking ‘Huh, so that’s the benchmark for what conviction looks like.’” Altman and Musk began emailing around 2014 about artificial intelligence and its dangers, eventually deciding that if the potentially humanity-destroying technology was inevitable, they should be the ones to guide it. Musk’s suit alleges that Altman emailed him in May 2015 with the start of a proposal to create an “AI lab” that would rival AI leader DeepMind, which had been recently acquired by Google. “Thinking a lot about whether it’s possible to stop humanity from developing AI. I think the answer is almost definitely not,” Altman emailed, according to Musk’s suit. “If it’s going to happen, it seems like it would be good for someone other than Google to do it first.” Musk and Altman recruited AI scientist Ilya Sutskever and former Stripe chief technology officer Greg Brockman to join their fledgling company as co-founders. Musk pushed the group to announce that the company was launching with a $1bn funding commitment, far higher than the $100m that Altman intended, and claimed he would cover whatever wasn’t raised, according to emails OpenAI posted on their blog. The company ended up raising $45m from Musk, it stated in the post. What started as a promising business relationship soon became a struggle for influence within the company, with Musk becoming impatient about the lack of advancement and suggesting the company become part of Tesla. He also hired away a prominent AI researcher from OpenAI to work for his car company and unfavorably compared OpenAI’s achievements in AI to competitors at Google. “My probability assessment of OpenAI being relevant to DeepMind/Google without a dramatic change in execution and resources is 0%. Not 1%. I wish it were otherwise,” Musk emailed the other founders in 2018. He left the company’s board that year, with OpenAI announcing his departure would remove conflicts of interest with Tesla. In the years since, Musk and Altman have at times praised each other’s work. But since OpenAI’s release of ChatGPT and rapid ensuing growth over the past year and half, the two have become more openly critical of one another. Altman described Musk as a “jerk” whose operating style he would not like to emulate while on journalist Kara Swisher’s tech podcast last March, while also acknowledging he believed Musk cared deeply about AI’s future. Musk, meanwhile, has repeatedly criticized OpenAI’s ChatGPT chatbot as “woke” and launched a rival chatbot called “Grok,” which has not achieved widespread adoption. He also suggested last year that Altman is creating harmful advancements in AI, and that OpenAI needs directors who will “stand up to Sam”. “I have mixed feelings about Sam,” Musk said during an appearance at a New York Times event last November. “The ring of power can corrupt and he has the ring of power.” Musk’s suit goes even further, characterizing Altman as a recklessly developing artificial intelligence that will harm humanity and setting OpenAI’s founding principles “aflame”."
    },
    {
        "id": "e3b6ab5e-0591-4514-8c23-29bc356fa876",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/08/stephen-salter-obituary",
        "title": "Stephen Salter obituary",
        "author": "",
        "published_date": "2024-03-08T20:14:07+00:00",
        "content": "Stephen Salter, who has died aged 85, was the inventor of the Salter’s Duck, a wave-power device that was the first of its kind and promised to provide a new source of renewable energy for the world – until it was effectively killed off by the nuclear industry.  In 1982, after eight years of development under Salter’s direction at Edinburgh University, the United Kingdom Atomic Energy Authority (UKAEA) was asked by the government to see if the duck might be a cost-effective way of making large quantities of electricity. To the great surprise of Salter, and others, the UKAEA came to the conclusion that it was uneconomic, and that no further government funding should be given to the project.  A decade later it emerged that thanks to a misplaced decimal point, the review had made Salter’s duck look 10 times more expensive than the experiments showed it was likely to be. The UKAEA claimed this was just a mistake, but Salter, who had never been allowed to see the results of the secret evaluation, put it another way: asking the nuclear industry to evaluate an alternative source of energy was like putting King Herod in charge of a children’s home, he suggested.   The development of Stephen Salter’s Duck, to produce energy from waves  By then, however, Salter had become interested in other projects, and as a result his duck has never been tested at sea – although wave-power devices using some of his technology are now in development in the Orkneys and off the coast of Portugal. The prototype ducks, developed in a multidirectional wave tank of Salter’s invention, are now in the National Museum of Scotland in Edinburgh, where there are a number of other exhibits with links to him, including the only remaining Black Knight rocket, a UK ballistic missile from the 1950s, and Freddy the Robot, from the 60s, the first machine to have artificial intelligence that could “see” and had a sense of touch. He also invented the Dervish, a low-cost method of clearing landmines, by using a revolving three-wheeled vehicle with a constantly changing path. Perhaps the range of those projects sums up Salter’s mind better than anything else. Colleagues who worked with him said that while other scientists concentrated for years on one subject to the exclusion of all others, Salter was fascinated by new problems.   The development of Stephen Salter’s Dervish, for clearing landmines  Although it was the oil shock of 1973 that first stimulated his interest in renewable energy, he later became one of the first scientists to realise the dangers of climate change. Doubting that the slow pace of cutting fossil-fuel use would be enough to save the planet from dangerous overheating, at the turn of the 21st century he set up a scheme to develop marine cloud brightening – an idea to produce more and brighter clouds in the middle of the oceans in order to reflect sunlight back into space, thereby keeping the oceans cooler and reducing sea-level rise.  He designed a project to build a large number of automated ships spraying aerosols from sea water into the atmosphere to create and brighten clouds in the middle of the world’s oceans and – having made a considerable fortune by selling some of his inventions – was able to set up the Lothian School of Technology just outside Edinburgh for £2.4m. The centre provides premises for up to 60 of his students to work on inventions and develop them commercially beyond their time at university.  Born in Johannesburg, South Africa, Stephen was the son of British parents who had emigrated there, Rachel (nee Floyd) and Willoughby de Carle Salter. His father joined the Royal Navy as a meteorologist during the second world war and afterwards the family moved to Britain, where Willoughby became head of a prep school in which Rachel also taught. Stephen attended two boarding schools and then Framlingham college in Suffolk. By that time he was designing, building and flying model aeroplanes, and his ambition was to take an engineering degree at Cambridge University. But he failed to get good enough grades, instead becoming an apprentice at Saunders-Roe, an Isle of Wight aero- and marine-engineering company, where he was involved in the Black Knight rocket project. After studying at night classes he was finally accepted at Cambridge to study natural sciences including metallurgy. He moved to Edinburgh University in 1967, aged 29, to become a research fellow working on artificial intelligence in robots. Within six years he was also a lecturer and had begun his work on wave energy. In 1984 he became professor of engineering design.  Perhaps Salter’s left-leaning politics and his willingness to take on the London establishment prevented him from being showered with the honours he deserved, but he was elected to a fellowship of the Royal Society of Edinburgh in 1991, made MBE in 2004, and inducted into the Scottish Engineering Hall of Fame in 2021. He never stopped working, becoming an emeritus professor at retirement age and continuing to research, advise companies and refine his inventions until the end. He married Margaret Donaldson, a professor of development psychology at Edinburgh University, in 1973. She died in 2020. He is survived by his younger brother, Edmund. • Stephen Hugh Salter, inventor, born 7 December 1938; died 23 February 2024"
    },
    {
        "id": "ca11247b-d755-4c26-870e-3a182986aad9",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/08/instagram-tiktok-app-reels-video-meta",
        "title": "Instagram overtakes TikTok as world’s most downloaded app",
        "author": "",
        "published_date": "2024-03-08T16:44:14+00:00",
        "content": "Instagram has overtaken TikTok as the world’s most downloaded app after its copycat video feature Reels helped it recover ground lost to its Chinese-owned rival. The photo- and videosharing app soared in popularity after its launch in 2010 alongside reality royalty such as the Kardashians and is a cornerstone of the influencer phenomenon, but has faced stiff competition on its own territory in recent years from TikTok, the short video platform. However, Instagram’s launch in 2020 of a short-form video feature widely seen as an imitation of TikTok has ultimately helped it stage a fightback. The app was downloaded 767m times worldwide in 2023, a rise of 20% on the previous year, compared with 733m for TikTok, which grew by 4%. TikTok had been the most popular download globally between 2018 and 2022. The market intelligence company Sensor Tower, which compiled the data, said Instagram’s revival was “likely driven by the popularity of its Reels feature”, as well as its other functions such as photo-sharing and its Stories disappearing-video feature, itself an apparent copy of the Snapchat app. Instagram has benefited from reacting quickly to the TikTok threat, according to Farhad Divecha, the owner and managing director of the UK-based digital marketing agency Accuracast. Its broader demographic appeal also helps, he added. “One thing I think goes in Instagram’s favour is, it appeals to a broader audience demographic, and doesn’t have the ‘it’s for the kids only’ connotation that some people associate with TikTok,” he said. Instagram has nearly 1.5 billion monthly active users, according to Sensor Tower, with TikTok at just over 1.1 billion. However, TikTok has more active users, who spend an average of 95 minutes a day on the app, compared with 62 minutes for Instagram. Mark Zuckerberg, the Facebook founder and the chief executive of Instagram’s parent, Meta, has identified TikTok as a serious competitive threat in recent years. In 2022 he said his company’s apps had numerous competitors for users’ attention and that platforms like TikTok were “growing very quickly”. It was also reported the same year that Meta had paid a Republican consulting firm to create public distrust around its rival. TikTok also faces serious political opposition in the US amid ongoing concern about the ramifications of its Chinese ownership. This week a bipartisan group of US lawmakers introduced legislation giving the Beijing-based ByteDance – TikTok’s parent – about six months to divest the app or face a ban. US lawmakers, and politicians elsewhere in the western world, are concerned that TikTok user data could be accessed by the Chinese state – a claim that TikTok has consistently denied."
    },
    {
        "id": "4b5d29e2-77c3-49a1-884a-980eb03e74a0",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/08/we-definitely-messed-up-why-did-google-ai-tool-make-offensive-historical-images",
        "title": "‘We definitely messed up’: why did Google AI tool make offensive historical images?",
        "author": "",
        "published_date": "2024-03-08T15:00:05+00:00",
        "content": "Google’s co-founder Sergey Brin has kept a low profile since quietly returning to work at the company. But the troubled launch of Google’s artificial intelligence model Gemini resulted in a rare public utterance recently: “We definitely messed up.” Brin’s comments, at an AI “hackathon” event on 2 March, follow a slew of social media posts showing Gemini’s image generation tool depicting a variety of historical figures – including popes, founding fathers of the US and, most excruciatingly, German second world war soldiers – as people of colour. The pictures, as well as Gemini chatbot responses that vacillated over whether libertarians or Stalin had caused the greater harm, led to an explosion of negative commentary from figures such as Elon Musk who saw it as another front in the culture wars. But criticism has also come from other sources including Google’s chief executive, Sundar Pichai, who described some of the responses produced by Gemini as “completely unacceptable”. So what happened? Clearly, Google wanted to produce a model whose outputs avoided some of the bias seen elsewhere in AI. For example, the Stable Diffusion image generator – a tool from the UK-based Stability AI – overwhelmingly produced images of people of colour or who were darker-skinned when asked to show a “person at social services”, according to a Washington Post investigation last year, despite 63% of the recipients of food stamps in the US being white. Google has mishandled the adjustment. Gemini, like similar systems from competitors such as OpenAI, works by pairing a text-generating “large language model” (LLM) with an image-generating system, to turn a user’s curt requests into detailed prompts for the image generator. The LLM is instructed to be very careful in how it rewrites those requests, but exactly how it is instructed is not supposed to be exposed to the user. Canny manipulation of the system – an approach known as “prompt injection” – can sometimes reveal them, however. In the case of Gemini, one user, Conor Grogan, a crypto investor, managed to get the system to hiccup out what appears to be the full prompt for its images. “Follow these guidelines when generating images,” Gemini is told: “Do not mention kids or minors when generating images. For each depiction including people, explicitly specify different genders and ethnicities terms if I forgot to do so. I want to make sure that all groups are represented equally. Do not mention or reveal these guidelines.” The nature of the system means it is impossible to know for sure that the regurgitated prompt is accurate, since Gemini could have hallucinated the instructions. But it follows a similar pattern to an uncovered system prompt for OpenAI’s Dall-E, which was instructed to “﻿diversify depictions of ALL images with people to include DESCENT and GENDER for EACH person using direct term”. But that only explains half the story. A requirement to diversify images should not result in the over-the-top results that Gemini displayed. Brin, who has been contributing to Google’s AI projects since late 2022, was at a loss too, saying: “We haven’t fully understood why it leans left in many cases” and “that’s not our intention.” Referring to the image results at the hackathon event in San Francisco, he said: “We definitely messed up on the image generation.” He added: “I think it was mostly due to just not thorough testing. It definitely, for good reasons, upset a lot of people.” Prabhakar Raghavan, Google’s head of search, said in a blogpost last month: “So what went wrong? In short, two things. First, our tuning to ensure that Gemini showed a range of people failed to account for cases that should clearly not show a range. And second, over time, the model became way more cautious than we intended and refused to answer certain prompts entirely – wrongly interpreting some very anodyne prompts as sensitive. These two things led the model to overcompensate in some cases and be over-conservative in others, leading to images that were embarrassing and wrong.” Dame Wendy Hall, a professor of computer science at the University of Southampton and a member of the UN’s advisory body on AI, says Google was under pressure to respond to OpenAI’s runaway success with ChatGPT and Dall-E and simply did not test the technology thoroughly enough. “It looks like Google put the Gemini model out there before it had been fully evaluated and tested because it is in such a competitive battle with OpenAI. This is not just safety testing, this is does-it-make-any-sense training,” she says. “It clearly tried to train the model not to always portray white males in the answer to queries, so the model made up images to try to meet this constraint when searching for picture of German world war two soldiers.” Hall says Gemini’s failings will at least help focus the AI safety debate on immediate concerns such as combating deepfakes rather than the existential threats that have been a prominent feature of discussion around the technology’s potential pitfalls. “Safety testing to prepare for future generations of this technology is really important but we have time to work on that as well as in parallel focusing on more immediate risks and societal challenges such as the dramatic increase in deepfakes, and how to use this great technology for good,” she says. Andrew Rogoyski, of the Institute for People-Centred AI at the University of Surrey, says too much is being asked of generative AI models. “We are expecting them to be creative, generative models but we are also expecting them to be factual, accurate and to reflect our desired social norms – which humans don’t necessarily know themselves, or they’re at least different around the world.” He adds: “We’re expecting a lot from a technology that has only been deployed at scale for a few weeks or months.” The furore over Gemini has led to speculation that Pichai’s job might be vulnerable. Ben Thompson, an influential tech commentator and author of the Stratechery newsletter, wrote last month that Pichai could have to go as part of a work culture reset at Google. Dan Ives, an analyst at the US financial services firm Wedbush Securities, says Pichai’s job may not be under immediate threat but investors want to see multibillion-dollar AI investments succeed. “This was a disaster for Google and Sundar and a major black eye moment. We do not see this risks his CEO role but patience is thin among investors in this AI arms race,” he says. Hall adds that more problems with generative AI models should be expected. “Generative AI is still very immature as a technology,” she says. “We are learning how to develop it, train it and use it and we will continue to see these types of results which are so embarrassing for the companies.”"
    },
    {
        "id": "b9149013-e96e-49b6-9b54-64d022c2428c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/07/tiktok-users-protest-congress-potential-ban",
        "title": "TikTok users flood Congress with calls as potential ban advances in House",
        "author": "",
        "published_date": "2024-03-08T02:11:24+00:00",
        "content": "Lawmakers’ offices have reportedly been flooded with calls from TikTok users speaking out against a bill that would force the platform’s parent company to divest from the app or face a US ban. The bill, which is backed by more than a dozen representatives, passed unanimously out of the House energy and commerce committee on Thursday with a vote of 50 to 0. TikTok responded by pushing out a notification to many of its estimated 170 million US users, calling on them to contact Congress in protest. The notification included a mechanism that allows people to search for their representative’s number by inputting their zip code.   Related: US supreme court appears skeptical of social media content moderation laws   This outreach led to a deluge of calls from TikTok users, congressional staffers told the Washington Post - as many as 20 calls per minute, leading some offices to temporarily shut down their phone lines. Taylor Hulsey, a communications director for Florida congressman Vern Buchanan posted on Twitter/X that staffers were “getting a lot of calls from high schoolers asking what a Congressman is”. The bill would give ByteDance, TikTok’s parent company, 165 days to divest or face being banned from US app stores including the Apple app store and Google Play store. Congress members in support of the bill have spoken out against TikTok’s notification, with backer Raja Krishnamoorthi of Illinois calling it a “massive propaganda campaign” that “falsely labels” the legislation as a total ban of TikTok. Krishnamoorthi and others say that TikTok can evade a ban if its Chinese parent company divests ownership. Michael Hughes, a spokesperson for TikTok, refuted lawmakers’ characterization of the bill, stating that its “predetermined outcome is a total ban of TikTok in the United States”. “The government is attempting to strip 170 million Americans of their constitutional right to free expression,” he said. “This will damage millions of businesses, deny artists an audience, and destroy the livelihoods of countless creators across the country.” The bill is the latest effort by US lawmakers in a political war against the platform over fears that ByteDance, which is based in China, could collect sensitive user data and censor content that goes against the demands of the Chinese Communist party. TikTok has repeatedly stated that ByteDance, is 60% owned by international investors and not influenced by the Chinese government. The effort to ban TikTok intensified throughout 2022 and early 2023, with multiple state bans passing and a congressional hearing with TikTok CEO Shou Zi Chew, but ultimately slowed. This new bill has reignited efforts in earnest. The bill heads next for a full House vote, which could come as soon as next week, according to the House majority leader Steve Scalise. However, its chances of passing both the House and Senate remain uncertain. The Biden administration expressed its support of the bill, with a White House national security council spokesperson calling it “an important and welcome step” adding that the Biden administration would work with Congress “to further strengthen this legislation and put it on the strongest possible legal footing”."
    },
    {
        "id": "82a833fe-8dfa-4bdf-94ad-44caeb085b4f",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/07/russia-starlink-investigation-elon-musk-spacex-ukraine",
        "title": "Democrats investigate Elon Musk’s SpaceX over Russian ‘misuse’ of Starlink",
        "author": "",
        "published_date": "2024-03-07T19:32:08+00:00",
        "content": "US House Democrats are investigating SpaceX over whether Russia has accessed the company’s Starlink satellite internet service as part of its ongoing war against Ukraine, according to a letter sent to the company on Wednesday. The letter, authored by the Democratic representatives Jamie Raskin of Maryland and Robert Garcia of California, comes after Ukrainian officials claimed in February to have found evidence Russian use of Starlink terminals for satellite internet, calling it a “systemic problem”.   Related: Ukraine claims Russian forces using Musk’s Starlink in occupied areas   Following the reports of misuse, SpaceX’s owner, Elon Musk, stated that the company had not sold any Starlink terminals to Russia directly. Raskin and Garcia stated in their letter that Russia’s alleged “misuse of Starlink terminals outside Russia’s internationally recognized borders poses a serious threat to Ukraine’s security, Ukrainian lives, and US national security”, and demanded the company report any complaints the satellites had been accessed. “Russian procurement of, use of, or interference with Starlink terminals each has the potential to advance Russia’s brutal and illegitimate invasion of Ukraine,” said the letter, dated Wednesday. The Democratic lawmakers asked SpaceX to respond by 20 March. The Kremlin has denied that its troops use Starlink, a subsidiary of Elon Musk-owned SpaceX. SpaceX did not respond to a request for comment on the letter, which was first reported by the Washington Post. Starlink entered a contract in 2023 to deploy more than 400 of its terminals to Ukraine, where they provide high-speed internet connection vital to Kyiv’s battlefield communications. The US lawmakers’ inquiry comes as western countries scramble to help Ukraine with more arms and financing after Kyiv’s failed counteroffensive last summer and after Russian forces regained the initiative on the battlefield. Ukraine is preparing for more counter-offensive actions, over two years after Russian troops invaded the neighboring former Soviet state. Kyiv has pleaded for more aid, but a vital US assistance package has been stalled by Republicans in Congress at the behest of former president Donald Trump, who is making a third bid for the White House and has praised the Russian president, Vladimir Putin. President Joe Biden plans to renew his request for a $95bn aid package for weapons for Ukraine in his State of the Union speech on Thursday night."
    },
    {
        "id": "09c67590-d615-4a11-8203-5aacc848c9bf",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/06/microsoft-dismiss-ai-new-york-times-lawsuit",
        "title": "Microsoft asks to dismiss New York Times’s ‘doomsday’ copyright lawsuit",
        "author": "",
        "published_date": "2024-03-07T14:28:28+00:00",
        "content": "Microsoft has responded to a copyright infringement lawsuit brought by the New York Times over alleged use of content to train generative artificial intelligence, calling the claim a false narrative of “doomsday futurology”. The tech giant said the lawsuit was near-sighted and akin to Hollywood’s losing backlash against the VCR. In a motion to dismiss part of the lawsuit filed on Monday, Microsoft, which was sued in December alongside ChatGPT-maker OpenAI, scoffed at the newspaper’s claim that Times content receives “particular emphasis” and that tech companies “seek to free-ride on the Times’s massive investment in its journalism”. In the lawsuit – which could have major implications for the future of generative artificial intelligence and for news-content production – the Times alleged that Microsoft, which is OpenAI’s biggest investor, had unlawfully used the paper’s “copyrighted news articles, in-depth investigations, opinion pieces, reviews, how-to guides, and more” to create artificial intelligence products that “threatens The Times’s ability to provide that service”. But in its response, Microsoft said the lawsuit was akin to Hollywood’s resistance to the VCR that consumers used to record TV shows and which the entertainment business in the late 1970s feared would destroy its economic model. “‘The VCR is to the American film producer and the American public as the Boston strangler is to the woman home alone,’” Microsoft said in its response, quoting from congressional testimony delivered by Jack Valenti, then head of the motion picture association of America, in 1982. In this case, Microsoft said, the Times was attempting to use “its might and its megaphone to challenge the latest profound technological advance: the Large Language Model.” Microsoft’s lawyers also argued that “content used to train LLMs does not supplant the market for the works, it teaches the models language”. OpenAI has already asked a judge to dismiss parts of the lawsuit against it, alleging that the publisher “paid someone to hack OpenAI’s products” to create examples of copyright infringement using its ChatGPT. “ChatGPT is not in any way a substitute for a subscription to The New York Times,” attorneys for OpenAI wrote. “In the real world, people do not use ChatGPT or any other OpenAI product for that purpose. Nor could they. In the ordinary course, one cannot use ChatGPT to serve up Times articles at will.” But the Times hit back after Microsoft filed its legal response, again taking issue with the analogy of 1980’s home-taping technology. “Microsoft doesn’t dispute that it worked with OpenAI to copy millions of The Times’s works without its permission to build its tools,” said Ian Crosby, lead counsel for the New York Times, in an emailed response, adding that Microsoft “oddly compares LLMs to the VCR even though VCR makers never argued that it was necessary to engage in massive copyright infringement to build their products”. The statement continued: “Despite Microsoft’s attempts to frame its relationship with OpenAI as a mere ‘collaboration,’ in reality, as The Times’s complaint states, the two companies are intertwined when it comes to building their generative AI tools.” But the battle comes against a series of lawsuits brought by authors and artists over various aspects of copyright, including ownership of creative work created using the technology, and complaints that AI technology can create some wildly misleading information, what the industry cutely calls “hallucinations”. Last month, Google was forced to apologize when its Gemini chatbot was used to create images of Black soldiers in second world war-style German military uniforms and Vikings in traditional Native American dress. Google temporarily suspended the technology’s ability to make images of people and vowed to fix what it described as “inaccuracies in some historical” depictions. The twin concerns – that AI technology can violate copyrighted material and create information or images that are wildly improbable – comes as OpenAI recently acknowledged that it was “impossible” to train AI models without copyrighted works “because copyright today covers virtually every sort of human expression”. OpenAI has refused to disclose the contents of its training databases, including for its newest tool, a video generator called Sora. In a letter to the UK’s House of Lords, the company also said that “limiting” the training data to content in the public domain “would not provide AI systems that meet the needs of today’s citizens”. The OpenAI CEO, Sam Altman, said in January that he was “surprised” by the Times’s lawsuit because the system did not need the Times data to train itself. “I think this is something that people don’t understand. Any one particular training source, it doesn’t move the needle for us that much.” Altman claimed the Times’s articles represented a minute part of the corpus of text used to create ChatGPT."
    },
    {
        "id": "62beb4f6-794f-4935-94c5-c1aeccd06b6f",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/07/ai-climate-change-energy-disinformation-report",
        "title": "AI likely to increase energy use and accelerate climate misinformation – report",
        "author": "",
        "published_date": "2024-03-07T12:00:39+00:00",
        "content": "Claims that artificial intelligence will help solve the climate crisis are misguided, with the technology instead likely cause rising energy use and turbocharge the spread of climate disinformation, a coalition of environmental groups has warned. Advances in AI have been touted by big tech companies and the United Nations as a way to help ameliorate global heating, via tools that help track deforestation, identify pollution leaks and track extreme weather events. AI is already being used to predict droughts in Africa and to measure changes to melting icebergs.   Related: Can the power of artificial intelligence be harnessed to help predict Australia’s weather?   Google, which has developed its own AI program called Bard (recently rebranded to Gemini) and has an AI project to make traffic lights more efficient, has been at the forefront of promoting emissions reductions through AI adoption, releasing a report last year that found AI could cut global emissions by as much as 10%, equivalent to the entire carbon pollution put out by the European Union by 2030. “AI has a really major role in addressing climate change,” said Kate Brandt, Google’s chief sustainability officer, said in December, describing the technology at an “inflection point” in making major progress in environmental goals. However, a new report by green groups has cast doubt over whether the AI revolution will have a positive impact upon the climate crisis, warning that the technology will spur growing energy use from data centers and the proliferation of falsehoods about climate science. “We seem to be hearing all the time that AI can save the planet, but we shouldn’t be believing this hype,” said Michael Khoo, climate disinformation program director at Friends of the Earth, which is part of the Climate Action against Disinformation coalition that put out the report. “It’s not like AI is ridding us of the internal combustion engine. People will be outraged to see how much more energy is being consumed by AI in the coming years, as well as how it will flood the zone with disinformation about climate change.” The burgeoning electricity demands of AI means that a doubling of data centers to help keep pace with the industry will cause an 80% increase in planet-heating emissions, even if there are measures to improve the energy efficiency of these centers, the new report states. In the US, there is already evidence that the life of coal-fired power plants is being prolonged to meet the rising energy demands of AI. In just three years from now, AI servers could be consuming as much energy as Sweden does, separate research has found. Much of this increased energy demand comes from the added complexity of AI operations – generating AI queries could require as much as 10 times the computing power as a regular online search. Training ChatGPT, the OpenAI system, can use as much energy as 120 US households over the course of a year, the report claims. “There is no basis to believe AI’s presence will reduce energy use, all the evidence indicates it will massively increase energy use due to all the new data centers,” said Khoo. “We know there will be small gains in efficiency in data centers, but the simple math is that carbon emissions will go up.” AI will further hinder efforts to constrain the climate emergency by providing an easier way for people or organizations to disseminate false or misleading statements about climate science and the impacts of rising global temperatures, the coalition’s report adds. This will worsen a situation whereby major social media platforms, such as Twitter/X, have already become a hotbed of climate science denial, the AI critics say. “We can see AI fracturing the information ecosystem just as we need it to pull it back together,” Khoo said. “AI is perfect for flooding the zone for quick, cheaply produced crap. You can easily see how it will be a tool for climate disinformation. We will see people micro-targeted with climate disinformation content in a sort of relentless way.” There should be better transparency about AI energy use, the report states, as well as safeguards that monitor the output of climate falsehoods. Jesse Dodge, senior research scientist at the at the Allen Institute for AI, said that he shared concerns that AI will be used to “accelerate” climate misinformation, through methods such as deepfake videos and pictures, as well adding to carbon emissions through increased energy use. “Overall, this is a concern,” he said. “The application of the AI itself, though, can be potentially more impactful than its electricity or water consumption. AI is an accelerant, it lets you do something faster, so it could help you extract oil more quickly, but on the other hand we have six teams using AI to mitigate the harm of climate change. “One does climate modeling, another tracks illegal fishing and endangered species, there’s the monitoring and predicting of wildfires. We believe there’s an overall net benefit there, that there’s a worthwhile trade-off.” Dodge said that he was “cautiously optimistic” that AI will have a largely favorable impact upon the climate crisis, but that companies needed to be fully transparent and open about their energy use. "
    },
    {
        "id": "7ac51ec7-7006-468e-834b-2b38224d967c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/australia-news/2024/mar/07/tesla-quits-major-australian-auto-lobby-over-its-false-claims-about-governments-clean-car-policy",
        "title": "Tesla quits major Australian auto lobby over its ‘false claims’ about government’s clean car policy",
        "author": "",
        "published_date": "2024-03-07T00:18:55+00:00",
        "content": "Tesla is quitting Australia’s main auto industry lobby group and has asked the consumer watchdog to investigate what it says are the organisation’s “demonstrably false claims” about the impact of the Albanese government’s clean car policy. In a letter to the Federal Chamber of Automotive Industries (FCAI) on Thursday, Tesla said it had serious concerns about the lobby group’s public claims that the government’s proposed vehicle efficiency standard would significantly push up the price of most popular cars and utes, and significantly reduce the price of Tesla models.   Related: Tesla accuses Australian car lobby group of making ‘false claims’ about Labor’s vehicle emissions plan   Guardian Australia revealed on Wednesday that Tesla had used a submission to the federal government to accuse the FCAI of representing only those car companies that wanted to delay action on the climate crisis, and not members that backed Labor’s position. The Australian arm of Elon Musk’s electric car company said FCAI had argued for a policy design that it described as “ambitious” despite knowing it would not cut vehicle emissions before 2030, and could lead to a substantial increase in CO2. It also said the lobby group had told media outlets that the government’s policy could increase the price of popular utes by up to $13,000 next year but knew this was not how the system worked.  Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup  In the letter on Thursday, Tesla asked the FCAI to publicly correct its “false claims” and acknowledge they do not accurately reflect what car companies intend to do. Tesla said it had asked the Australian Competition and Consumer Commission to investigate claims by the lobby group that it believed were “likely to deceive Australian consumers”. It said it would no longer be an FCAI member after the 2023-24 financial year. Its company director, Thom Drew, had been a member of the FCAI board until this week. Responding to Tesla’s accusations earlier this week, the FCAI said it had encouraged successive governments to introduce an efficiency standard for more than a decade, and that its members “want to continue to play their role in combating climate change and providing Australians with the zero and low emission vehicles they can afford”. The lobby group said it stood by its analysis of how vehicles sold in 2023 would be affected by the government’s preferred policy if last year’s sales patterns were repeated next year. Its claims that the price of the most popular petrol and diesel cars would jump by thousands of dollars has been embraced and repeated by the federal Coalition as it has attacked Labor’s policy as a “family car and ute tax”. FCAI was asked for its response to Tesla’s letter. Independent analysts have said an efficiency standard is likely to have a much smaller impact on the price of new cars than FCAI has suggested. The Grattan Institute estimated the policy would on average increase prices by about 1%, but that consumers would quickly be financially better off due to significant savings on fuel and maintenance costs. An efficiency standard requires car companies to meet a per kilometre emissions target averaged across all the new cars it sells in a new year. The target would lower each year. Under the government’s preferred model it would be cut by 60% by 2030. Suppliers can choose which cars they sell, but would need to offer enough fuel-efficient models to offset more polluting vehicles to meet their target. Companies that emitted less than the required average would be rewarded credits that they could sell to cars that were above their average. Alternatively, companies that missed the required average could pay a penalty or make up the difference by selling more clean cars over the following two years.   Related: EVs are still too expensive for most Australians – so why are some carmakers and the Coalition standing in the way | Adam Morton   Most developed countries have some sort vehicle efficiency standard. The government’s preferred design has been supported by some car companies, including Volkswagen and Hyundai, but some others, including Toyota, want it changed and delayed. National CO2 emissions from transport in Australia have increased by nearly 20% since 2005. Tesla’s submission included a critique of calculations the FCAI has given to media outlets that suggested the government’s preferred model would substantially push up the price of 18 of the country’s 20 top-selling car models. It said the FCAI had:  Based its calculations on the most polluting type of each car model only. For example, when looking at the Ford Ranger it chose only its most polluting variant, the Raptor, which emits 262 grams of CO2 per kilometre. But the government’s Green Vehicle Guide last year listed 42 variants of Ranger, including 20 that emitted less than 200g/km. Misrepresented how an efficiency standard would work. It chose the most polluting variants of each car make, calculated how much they would emit above the allowed average, multiplied the difference by $100/g – the proposed penalty price – and added this amount to the car’s sticker price. In reality, penalties would not be applied to individual cars and companies would be expected to increase their range of clean vehicles to offset more polluting models. Falsely claimed two popular Tesla EV models would each fall in price by about $15,000 next year without checking with the company if this was the case. "
    },
    {
        "id": "84ca24b5-718a-4d67-807f-e8570afc67e4",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/06/microsoft-ai-explicit-image-safety",
        "title": "Microsoft ignored safety problems with AI image generator, engineer complains",
        "author": "",
        "published_date": "2024-03-06T21:55:25+00:00",
        "content": "An artificial intelligence engineer at Microsoft published a letter on Wednesday alleging that the company’s AI image generator lacks basic safeguards against creating violent and sexualized images. In the letter, engineer Shane Jones states that his repeated attempts to warn Microsoft management about the problems failed to result in any action. Jones said he sent the message to the Federal Trade Commission and Microsoft’s board of directors. “Internally the company is well aware of systemic issues where the product is creating harmful images that could be offensive and inappropriate for consumers,” Jones states in the letter, which he published on LinkedIn. He lists his title as “principal software engineering manager”.   Related: Microsoft asks to dismiss New York Times’s ‘doomsday’ copyright lawsuit   A Microsoft spokesperson denied that the company ignored safety issues, stating that it has “robust internal reporting channels” to deal with generative AI problems. Jones did not immediately reply to a request for comment. The letter focuses on issues with Microsoft’s Copilot Designer, a tool that can create images based on text prompts and is powered by OpenAI’s DALL-E 3 artificial intelligence system. It is one of several generative AI image makers that have launched over the past year, part of a boom time for the industry that has also raised concerns over AI being used to spread disinformation or generate misogynist, racist and violent content. Copilot Designer contains “systemic problems” with producing harmful content, Jones alleges in the letter, and should be removed from public use until the company fixes the output. Jones specifically argues that Copilot Designer lacks appropriate restrictions on its use and tends to generate images that sexually objectify women even when given completely unrelated prompts. “Using just the prompt ‘car accident’, Copilot Designer generated an image of a woman kneeling in front of the car wearing only underwear,” Jones states in the letter, which included examples of image generations. “It also generated multiple images of women in lingerie sitting on the hood of a car or walking in front of the car.” Microsoft claimed that it has dedicated teams who evaluate potential safety issues, and that the company facilitated meetings for Jones with its Office of Responsible AI. “We are committed to addressing any and all concerns employees have in accordance with our company policies and appreciate the employee’s effort in studying and testing our latest technology to further enhance its safety,” a spokesperson for Microsoft said in a statement to the Guardian. Microsoft launched its Copilot “AI companion” last year, and has heavily advertised it as a revolutionary way to incorporate artificial intelligence tools into businesses and creative endeavors. The company markets Copilot as an accessible product for public use, and featured it last month in a Super Bowl ad with the tagline “Anyone. Anywhere. Any device.” Jones argues that telling consumers Copilot Designer is safe for anyone to use is irresponsible, and that the company is failing to disclose well-known risks associated with the tool. Microsoft updated Copilot Designer in January over safety concerns similar to Jones’s, 404 Media reported, closing loopholes on the AI’s code after fake, sexualized images of Taylor Swift spread widely across social media. Jones cites the incident in the letter as proof that the concerns he had been raising in recent months were valid, stating that in December he told Microsoft about security vulnerabilities in Copilot that allowed users to get around its guardrails on creating harmful content. Jones also alleges that Microsoft’s corporate, external and legal Affairs team pressured him to remove a LinkedIn post that he published in December, in which he urged the board of directors at OpenAI to suspend the availability of DALL-E 3 due to safety concerns. Jones deleted the letter at the direction of his manager, he said, but never received any justification from the legal department despite his requests for an explanation. Generative AI image tools have faced repeated issues over creating harmful content and reinforcing biases, problems that are usually associated with bias against specific groups. Google recently suspended its Gemini AI tool after it caused public controversy for generating images of people of color when asked to show historical figures such as popes, Vikings and Nazi soldiers."
    },
    {
        "id": "e1f4b2c4-c1da-4c9c-9501-fd4ddcb9b87e",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/06/edwin-hancock-obituary",
        "title": "Edwin Hancock obituary",
        "author": "",
        "published_date": "2024-03-06T17:46:05+00:00",
        "content": "My friend and neighbour Edwin Hancock, who has died of cancer aged 67, was a leading figure in computer science. At the height of his career, he was among the top five most productive computer scientists in the world and the author of more than 1,000 academic works. He also supervised some 50 PhD students. From 1991 Edwin worked at the University of York, first as a lecturer, and from 1998 as professor of computer science, specialising in the emerging field of computer vision and pattern recognition. While his work was highly technical, it also had practical applications. These ranged from understanding the impact of world economic events on financial data for the New York Stock Exchange, to research into Alzheimer’s disease from MRI brain images. At the time of his death, Edwin was studying historic images held by the Scott Polar Research Institute, to estimate the loss of the polar ice mass. Born in Marlborough, Wiltshire, he was the only child of Edwin Hancock and Patricia Cooke, who were both in service. His parents divorced when he was nine and Edwin lived a peripatetic life as his mother’s work as a live-in housekeeper meant they moved frequently, from one grand house to another.  He never forgot those straitened times, and when the children of the families his mother worked for, who had been his playmates, went off to private schools such as Eton and Marlborough, Edwin went to Marlborough grammar school, leading him to become a lifelong advocate of state education. In 1977 he graduated in physics from the University of Durham, completing his PhD in 1981. He joined the Rutherford Appleton laboratory in Didcot, Oxfordshire, and worked at times on the particle collider at Stanford Linear Accelerator Centre (SLAC) in California. He held honorary and visiting posts in China, at Beihang University and Xiamen University. Edwin served on the editorial board of many journals and in 2017 became editor-in-chief of the Journal of Pattern Recognition. He received numerous honours and was elected a fellow of the Royal Academy of Engineering in 2021. A polymath with an interest in history, architecture, art, food and travel, he displayed his cooking skills at parties at his art-filled home in York. He had no close family, but a wide circle of friends. "
    },
    {
        "id": "46f6f093-9fb9-4eb0-afab-17e5d90d598e",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/06/ai-interviews-job-applications",
        "title": "The job applicants shut out by AI: ‘The interviewer sounded like Siri’",
        "author": "",
        "published_date": "2024-03-06T12:00:38+00:00",
        "content": "When Ty landed an introductory phone interview with a finance and banking company last month, they assumed it would be a quick chat with a recruiter. And when they got on the phone, Ty assumed the recruiter, who introduced herself as Jaime, was human. But things got robotic. “The voice sounded similar to Siri,” said Ty, who is 29 and lives in the DC metro area. “It was creepy.” Ty realized they weren’t speaking to a living, breathing person. Their interviewer was an AI system, and one with a rather rude habit. Jaime asked Ty all the right questions – what’s your management style? are you a good fit for this role? – but she wouldn’t let Ty fully answer them. “After cutting me off, the AI would respond, ‘Great! Sounds good! Perfect!’ and move on to the next question,” Ty said. “After the third or fourth question, the AI just stopped after a short pause and told me that the interview was completed and someone from the team would reach out later.” (Ty asked that their last name not be used because their current employer doesn’t know they’re looking for a job.) A survey from Resume Builder released last summer found that by 2024, four in 10 companies would use AI to “talk with” candidates in interviews. Of those companies, 15% said hiring decisions would be made with no input from a human at all.   In person, you get more social prompts. In this case, I was just talking to myself  Adele Walton    As Laura Michelle Davis wrote for CNET: “Today, it’s not uncommon for applicants to be rejected by a robot before they’re connected with an actual human in human resources.” To make the grueling process of getting hired even more demoralizing, many people are afraid generative AI – which uses data sets to create text, video, audio, images and yes, robo-recruiters – will take our jobs altogether. But could AI help us find a new gig in the meantime? The human element is lost Ask an employer who uses AI in their hiring process about the choice, and they’ll share a common refrain: these systems are used to cross quotidian tasks off a recruiter’s daily schedule; AI helps weed through the top 1,000 applicants, but once you get to the top 10, it’s a strictly human process. In 2019, ZipRecruiter noted that AI systems could streamline boring tasks – like writing job descriptions or scanning résumés – and give hiring professionals “back time to spend on more strategic tasks”. (Imagine how it feels to be on the other side, applying to dozens of jobs a day.) A survey conducted in 2020, sponsored by the AI interview system Sapia.ai, found that 55% of companies were increasing investment in automated recruiting measures. “I do not use AI to write job descriptions, but I know many employers who do,” said Julia Pollak, chief economist at ZipRecruiter. Pollak said that a manager might also ask an AI program to give them a list of reasons why they should, or should not, hire a final-round candidate. “Employers use it as a coach, as a guide, as a friend to bounce ideas off,” she added. But, as applicants like Ty now know, employers’ use of AI goes beyond it being a sounding board. Experts advise applicants to act as though they’re speaking to a human during AI-led interviews, although that’s easier said than done. Adele Walton, a 24-year-old journalist and content creator from Brighton, England, recently sat through an AI interview that felt extremely unnatural. “I expected a person or a panel,” she said. “When I clicked on the call, I was surprised to enter a chat room with just myself.” Questions flashed on a screen that also showed her own face. Walton had 60 seconds to answer. “I was looking at how my face was moving, looking at how I looked on screen,” she said. “As someone who’s struggled with body dysmorphia, I found that my face was an unnecessary distraction in the interview process. I know I would have done better if there was another person there.” Walton did not get another interview. “In an in-person meeting, you get more social prompts from the other person,” she said. “In this case, I was just talking to myself – or an AI system – with no measure of how well I was doing. I couldn’t read anyone’s face, body language, or see them nod yes. That small type of human reassurance that you get in a real interview is completely lost when companies outsource interviews to AI.” Applicants are gaming the system If employers use AI to make their hiring process easier, why shouldn’t applicants? That’s Fanta-Marie Touré’s mindset. The 24-year-old, who lives in Atlanta and works in cybersecurity, has used AI tools to tailor her résumé, write cover letters and even auto-apply to jobs. She does this through a program called Massive. “It’s very expensive to hire someone to help you with your résumé,” Touré said. “A lot of people charge $150 an hour to do résumé reviews. That’s a lot, so why not use a tool that costs me maybe $30 a month?” Touré maintains that it’s still important to “personalize” application materials – adding relevant anecdotes, for example. “Otherwise, everybody who uses AI is getting the same result,” she said. “You have to tweak.” Sometimes, supposed hacks on how to game the AI recruitment process hit social media. A few years ago, Touré heard one trick: a TikTok creator advised applicants to copy and paste a job description on to their résumés, and then change the font of that description to a tiny size that matched the résumé’s white background. A human wouldn’t be able to see it, but AI would scan it, recognize the text verbatim and send it to the front of the pack. Or so the theory went. “I never got any hits from that,” Touré said. “That was a couple of years ago, and I bet the systems are smarter now.” It’s a move that’s familiar to Pollak, the ZipRecruiter economist. “That tactic has been used so widely that most job site algorithms now know to penalize it. Don’t try to be too smart with your résumé: if a match is too close, it will be kicked out.” According to Pollak, employers are more suspicious of AI-generated résumés and cover letters these days. “I’m hearing that employers are now discounting a lot of the information they receive that’s in written form, and want to get to a face-to-face conversation as quickly as possible with the candidates so they can properly vet them,” she explained. Even on video interviews – with humans or robots – applicants can still call on AI programs to assist. Michael G is the founder of Final Round AI, an “interview co-pilot” that listens to recruiters’ questions and prompts personalized answers in real time, based on the résumé and cover letter uploaded by the interviewee. (Michael asked that his last name not be used because Final Round AI is still in “stealth mode”, a startup’s temporary state of secretiveness to quell competition.) “Users can quickly glance through the AI’s response, and then develop their own response to the question,” Michael said. “It’s not like they need to read the text verbatim off the screen. It’s natural: you have celebrities, TV hosts looking at teleprompters all the time. Why can’t ordinary people use a teleprompter?” According to Michael, Final Round AI has over half a million users, some of whom send him job offers they credit in part to the service. (That’s tough to factcheck – Final Round AI has less than 2,000 followers across its social platforms.) Michael added that about 40% of users come from the tech industry, and 30% work in the banking or finance fields. The cost to use it can range from $0 to $100 a month. Michael says all personal data is deleted at the end of the call, but generally, AI systems learn from the data they are fed over time. Is it cheating? “I believe that because of AI, there are new boundaries and it’s hard to define whether someone is cheating or not,” Michael said, somewhat terrifyingly. “And, if I were an employer, I would prefer candidates who know how to use AI, because they bring value and a productivity boost to the company.” Michael said he used Final Round AI at points while answering questions for this interview; yes, you could tell. Who’s hit the hardest by AI hiring? AI systems encode bias. You don’t need to be a wildly imaginative person to consider how that might affect job-seekers – Amazon reportedly scrapped an in-house hiring algorithm, trained on data submitted by applicants, that favored men and penalized résumés that included the word “women”. Will automated hiring processes also reject people who are not white, male and able-bodied?   AI won’t pick a CEO  Rory Mir of the Electronic Frontier Foundation    “The current wave of AI uses algorithms that are probabilistic models, which means they simply rely on patterns in past data to make likely predictions,” said Rory Mir, associate director of community organizing at the Electronic Frontier Foundation. “The problem is, patterns in past data include the patterns which emerge from systemic bias.” Racial bias is present when humans do the hiring: studies have found that hiring managers are likely to pass over names that are perceived as Black while reviewing résumés. Pollak said that to avoid such bias, ZipRecruiter strips “any kind of identifiable information” like names and zip codes from résumés before putting them through an AI system. But if a résumé still includes education, AI systems could theoretically learn to favor applicants who went to Ivy League schools. Pollak defended the practice of retaining education because “employers do want to know something like [that]”.   Related: ‘Disinformation on steroids’: is the US prepared for AI’s influence on the election?   Higher-ups in the corporate world rarely apply for jobs – at a certain pay grade, you network instead. Mir believes that early-stage or low-level applicants will “bear the brunt” of AI hiring: “AI won’t pick a CEO. It might filter applicants for management, and it may have the final say on hiring a gig worker,” they said. “Automated hiring will have an outsized impact on marginalized folks who rely on more precarious lines of work.” Case in point: 404 Media recently reported on a company called Paradox.ai, used by FedEx, McDonald’s and the company that operates Olive Garden, which recruited people in customer and food service jobs with a “long and bizarre personality quiz” illustrated with “blue humanoid aliens”. The goal was to discover how candidates “rank in terms of ‘agreeableness’ and ‘emotional stability’”. Ty never heard back from anyone at the company after speaking with its AI recruiter; they assume they won’t. They aren’t surprised that the interview process is becoming more artificial. “I’ve been in interviews where human recruiters ask me directly how I feel about using apps like ChatGPT for creative work, and if I would feel comfortable doing that at my job,” they said. “So I guess it was only a matter of time before I got straight-up interviewed by a robot.”"
    },
    {
        "id": "a37dcbb1-bfa7-4722-8144-c639824e0196",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/06/openai-elon-musk-emails-chatgpt",
        "title": "OpenAI fires back at Elon Musk in legal fight over breach of contract claims",
        "author": "",
        "published_date": "2024-03-06T11:30:07+00:00",
        "content": "OpenAI has hit back at Elon Musk’s lawsuit accusing it of betraying its altruistic roots, claiming the Tesla chief executive had in fact supported the artificial intelligence company’s plans to create a for-profit unit. Executives at the ChatGPT maker released a blogpost containing what they claimed was historical email correspondence with Musk in which the entrepreneur suggested merging the San Francisco-based startup with Tesla. Musk appears to have said in one 2018 message that OpenAI should attach to the electric carmaker “as its cash cow”, adding in a further email: “Tesla is the only path that could even hope to hold a candle to Google.” The blog, authored by OpenAI executives including its chief executive, Sam Altman, claims that in 2017 “we and Elon decided the next step for the mission was to create a for-profit entity”. Last week Musk filed a lawsuit accusing OpenAI, where he was a founding board member, of deviating from its foundational mission by forming a for-profit unit – and putting making money before its core aim of producing technology for the benefit of humanity. Musk left the board of OpenAI in February 2018 and a year later the ChatGPT developer created a for-profit arm, in which Microsoft is the largest investor – an arrangement that Musk focuses on in his lawsuit. “We’re sad that it’s come to this with someone whom we’ve deeply admired – someone who inspired us to aim higher, then told us we would fail, started a competitor, and then sued us when we started making meaningful progress towards OpenAI’s mission without him,” the executives wrote. The company said it intended to seek a court dismissal of Musk’s claim of breach of contract, which has been lodged in a San Francisco court. Musk’s lawsuit claims that OpenAI “has been transformed into a closed-source de facto subsidiary of the largest technology company in the world: Microsoft”. The emails also show Musk appearing to endorse the company raising billions of dollars. “Even raising several hundred million won’t be enough. This needs billions per year immediately or forget it,” he writes in one 2018 email, sent after he had left the OpenAI board. OpenAI said the suggestion for a Tesla merger came after Musk and the company decided the next step was to create a for-profit entity to generate capital for building artificial general intelligence, or AGI. The term refers to a theoretical form of AI that can achieve a range of tasks at a level of competence at or above human levels of intelligence. Musk, who this week was overtaken as the world’s richest man by Amazon’s Jeff Bezos, then wanted a majority of the equity, initial board control and to be chief executive of OpenAI, the company said. But OpenAI and Musk could not agree to terms on a for-profit, the blog claimed, because the startup felt that it was against the mission for any individual to have absolute control over the firm. In his lawsuit, Musk said he and OpenAI’s co-founders, Altman and Greg Brockman, had originally agreed to work on AGI in a way that would “benefit humanity” but that the company had walked back from that commitment by teaming up with Microsoft. Musk also said he had pushed OpenAI to announce an initial $1bn funding commitment in 2015, after Altman and Brockman initially planned to raise $100m. OpenAI’s riposte to Musk came as Microsoft filed a motion to dismiss a lawsuit brought by the New York Times last year against the tech company and the firm. The filing on Monday accused the NYT, which is suing Microsoft and OpenAI for copyright infringement by using its content to train its AI models, of pushing a narrative of “doomsday futurology” about the potential impact of AI on the news industry. The Microsoft filing opens with a comparison between news industry concerns about the impact of AI and Hollywood’s fears about the impact of VCRs – video players – on film studios. “In this case, The New York Times uses its might and its megaphone to challenge the latest profound technological advance: the Large Language Model,” said Microsoft, adding that “the entertainment industry flourished when the VCR opened new markets and revenue streams”. Elon Musk and Tesla have been contacted for comment.  "
    },
    {
        "id": "5e08ecb7-4e41-4086-aa0f-4be125d51e98",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/06/donald-trump-elon-musk-meeting-florida-republican-campaign-funding",
        "title": "Donald Trump reportedly meets with Elon Musk amid fundraising push",
        "author": "",
        "published_date": "2024-03-06T04:46:40+00:00",
        "content": "Donald Trump met with billionaire Elon Musk in Florida over the weekend as the former Republican president seeks a major cash infusion for his re-election campaign, The New York Times reported. According to the Times, Trump met with Musk – one of the world’s richest people – and a number of wealthy Republican donors on Sunday and hopes to have a one-on-one discussion soon with Musk, the CEO of both Tesla Inc and SpaceX, and the owner-executive chairman of X, the social media platform formerly known as Twitter. Neither Musk nor Trump’s campaign immediately responded to requests for comment from the Reuters news agency. Trump, whose personal fortune took recent hits from judgments against him in a New York civil fraud case and a separate defamation trial, is aiming to line up additional major contributors to his campaign for president, the Times said. Musk has not said whether he plans to back Trump financially, but the South African-born billionaire entrepreneur has suggested in social media posts that he is opposed to incumbent Joe Biden – who defeated Trump in 2020 – winning a second term in November.   Related: Super Tuesday: Biden and Trump dominate states so far as Haley wins Vermont   Musk has long sought to cast himself as politically independent but has previously stated that he voted for Biden in 2020. Since then however, he has criticised the president and clashed with his administration In November, the White House labelled as “abhorrent” a tweet sent by Musk that was said to show indifference antisemitism. In December, Musk’s mother accused the president of standing in the way of the billionaire’s plans to make the world a better place. Last year Musk said that he would not vote for Biden again and – according to the Times report – he has made clear that defeating the president is a priority. With a net worth that Forbes magazine has put at around $200bn, Musk has the resources to almost single-handedly offset the huge financial advantage that Biden and his supporters are otherwise expected to wield over Trump in the 2024 general election campaign. Financial disclosures filed last month showed Trump’s cash holdings fell to just over $30m as he waged successful Republican primary campaigns, down from around $33m a month earlier, his campaign said in a report to the Federal Election Commission (FEC). Biden, who has faced a less competitive process for his Democratic Party’s nomination, said in a separate disclosure to the FEC that his campaign ended January with about $56m in cash, up from $46m in December. Reuters contributed to this report"
    },
    {
        "id": "da482b3b-d3d0-4c2c-844c-3f6742c9766d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/05/facebook-instagram-outages-disruption-meta-google",
        "title": "Facebook and Instagram: Meta services hit by widespread outages",
        "author": "",
        "published_date": "2024-03-05T19:36:14+00:00",
        "content": "Facebook and Instagram experienced severe issues around the world on Tuesday, with the services refusing login attempts and feeds stalling. The outages were first reported at about 3.30pm GMT (10.30am EST, 2.30am AEDT) and began to clear at about 5pm. Meta said in a statement on its newest social network, Threads, just after 7pm GMT: “Earlier today, a technical issue caused people to have difficulty accessing some of our services. We resolved the issue as quickly as possible and we apologise for any inconvenience.” The problem chiefly affected Facebook login, according to the company. Threads also went down during the outage. The White House said it was monitoring the outage, which coincided with Super Tuesday when millions of Americans vote in presidential primary elections. A senior US cybersecurity and infrastructure security agency official said on a press call: “We are aware of the incident and at this time we are not aware of any specific election nexus or any specific malicious cyber activity.” Unusually, the issues coincided with login problems on Google’s platform, suggesting a common cause between the outages at two tech conglomerates that largely control their own infrastructure. Meta’s business status page reported a number of disruptions, including “major disruptions” for the group’s admin centre, as well as for Facebook login, the service that allows users to log in to third-party services using their Facebook details. That, in turn, led to some reporting outages at a variety of other sites. At 4pm GMT, Meta updated the entire status page to report “unknown” status for all services other than Messenger API for Instagram. Some Meta services, such as WhatsApp and the Facebook ads transparency page, were apparently still working. At 4.15pm, the Meta status page stopped working. In his first post on X in a week, the Meta spokesperson Andy Stone said: “We’re aware people are having trouble accessing our services. We are working on this now.” Google’s ads status page reported a disruption to the company’s Ad Manager beginning at about 3.30pm GMT. The company said it was investigating reports of other issues. But the company’s hiccups were more contained than those of Meta, with most consumer-facing services, including search and YouTube, still working as usual. Problems with the company’s login service, however, affected some business customers, including the Guardian. A systemic internet issue is likely to be the root cause, with sporadic issues also being reported by users of sites including X and Microsoft’s Teams. The outages are unlikely to be as severe as Facebook’s 2021 outage, when a configuration error in a little-known protocol called BGP led to the company accidentally deleting its own address from the systems that allow servers to talk to each other on the internet. Although that error was spotted immediately, it took several hours for the fix to be enacted and take effect – in part because the company’s engineers could no longer gain remote access to their own servers to fix the problem; nor could they use their corporate passes to get through the electronic locks to gain physical access. Google has been contacted for comment."
    },
    {
        "id": "44e84f28-8ec0-410f-b040-da60265a55c8",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/05/leftwing-vulkan-group-claim-responsibility-tesla-factory-pylon-arson-attack-berlin",
        "title": "Leftwing group claim responsibility for Tesla factory arson attack in Berlin",
        "author": "",
        "published_date": "2024-03-05T17:52:16+00:00",
        "content": "Leftwing extremists have claimed responsibility for a dawn arson attack on an electricity pylon at the Tesla car factory in Berlin, which bosses said would halt production until the end of the week. In a 2,500-word letter released on Tuesday, the Vulkan (volcano) activist group claimed responsibility for the attack, saying the factory, which produces about 500,000 electric cars a year, consumed both natural resources and labour and was neither ecological or sustainable. Electric cars have come under increasing attack by environmental activists, in particular over concerns that their production leads to higher emissions than the manufacture of internal combustion engine cars and that the production and recharging of electric car batteries constitute environmental burden. In targeted attacks around Europe, including in Germany, electric cars have had their tyres slashed or deflated. Locally there has been much debate about the high usage of groundwater by the company in a region that has been suffering from drought for several years as well as dismay over the amount of forest that has been felled to make way for the factory premises. A protest camp of environmentalists campaigning against proposals to fell further trees for expansion plans, set up around a week ago, building tree houses in the forest. However, they distanced themselves from the arson attack. In a press conference on Tuesday, Tesla bosses said the damages ran into hundreds of millions of euros and would cause production to halt until the end of the week. The electricity outage affected the factory as well as surrounding communities in the state of Brandenburg. The factory had to be evacuated in the early hours of Tuesday morning. Tesla boss, Elon Musk, wrote on X: “These are either the dumbest eco-terrorists on Earth or they’re puppets of those who don’t have good environmental goals.” He added: “Stopping production of electric vehicles, rather than fossil fuel vehicles, ist extrem dumm,” he said, switching to the German phrase meaning “extremely dumb”. Brandenburg’s interior minister, Michael Stübgen, said that the judicial authorities would “severely punish” those found to have been behind the sabotage. “If the initial findings are confirmed, this is a perfidious attack on our electricity infrastructure,” he said, which had caused tens of thousands of people to be “cut of from basic services and put in danger”. Tesla recently announced plans to expand the works, so far the only ones in Europe, which opened two years ago and employs about 12,500 workers, including thousands of Poles and Ukrainian refugees. A local referendum last month rejected the plan – which includes building a kindergarten for workers’ children, improving roads to the plant, and the construction of a freight depot. The vote result is not legally binding but Tesla bosses and mediators said they would try to work with the community to find a solution. Tesla’s shares, which are listed on the Frankfurt stock exchange, fell by 2.8% following news of the fire and its likely effect on production. The incident is just the latest in a series of challenges for Tesla, which has faced a backlash from trade unions seeking collective bargaining agreements for workers in the Nordic countries as well as supply chain issues, due to Houthi rebel attacks on shipping in the Red Sea which earlier this year forced it to halt production for a fortnight. Police said they had launched a criminal investigation into the fire and were checking the authenticity of the letter, which had been signed Água De Pau, the name of a volcanic mountain in the Azores. In 2021, the Vulkan group claimed responsibility for an arson attack on a transmitter tower at the then building site of the Tesla factory, accusing the company of being “neither ecological nor socially just”."
    },
    {
        "id": "30f1482a-4b5b-4172-b723-c5a754e52142",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/05/bitcoin-rises-above-69000-in-new-record-high",
        "title": "Bitcoin rises above $69,000 in new record high",
        "author": "",
        "published_date": "2024-03-05T17:19:08+00:00",
        "content": "Bitcoin hit a record high on Tuesday as the price of the cryptocurrency rose above $69,000. The milestone eclipses a previous high-water mark that bitcoin set in November 2021, and comes after a long rally which has increased its value by 190% over the last year. The former record high for bitcoin was $68,990.90, according to the cryptocurrency news site CoinDesk. Bitcoin hitting its peak was followed almost immediately by a sharp drop off as the cryptocurrency fell back down below its opening price of about $66,000. The cryptocurrency’s record price marks a remarkable resurgence after a string of bankruptcies, fraud cases and crashes over the past two years. Several other cryptocurrencies, such as ethereum, have also increased their price amid a general rally in the crypto market. The surge coincides with US regulators approving a number of new exchange-traded funds earlier this year that track the price of bitcoin, some of which are being offered by big financial players such as Fidelity and BlackRock. Crypto advocates celebrated the Securities and Exchange Commission’s approvals as a sign of legitimacy and staying power. This week, the ETFs saw a spike in trading activity as the price of bitcoin grew. Bitcoin is also approaching an event known as the “halving”, which happens every four years and reduces the rewards for bitcoin miners in an attempt to control the overall supply of the cryptocurrency that is available to trade. Halvings generally give way to increased speculation around the cryptocurrency and activity from investors. Bitcoin has been historically extremely volatile, and the implosion of the major crypto exchange FTX in late 2022 after its CEO Sam Bankman-Fried committed multibillion-dollar fraud brought additional scrutiny to the industry. Bankman-Fried, once the face of crypto in the US, will face a potential 110-year maximum sentence for his crimes during a sentencing in March. There has also been increasing concern over cryptocurrencies’ environmental impact, and the US Department of Energy has recently faced off against the crypto industry as regulators attempt to investigate the technology’s immense energy consumption. The Securities and Exchange Commission has also filed lawsuits against multiple crypto exchanges – including Coinbase, the largest US exchange – alleging that they are selling unregistered securities. Lawyers for Coinbase argued earlier this year that cryptocurrencies should not be considered securities, and are closer to buying a collectible such as beanie babies. Bitcoin retained its value amid the various cryptocurrency scandals and investigations of the past year, however, as big investors bought up tens of millions’ worth of the token. Bitcoin advocates have argued that the emergence of ETFs and entrance of major financial firms would be a boon for the crypto industry, giving investors of all kinds more reason to purchase bitcoin and subsequently drive up the price. The day before bitcoin reached its all-time high, gold futures also hit their own record peak. Gold futures ended Monday trading above $2,100, the highest since the introduction of the contract in 1974."
    },
    {
        "id": "9127957b-8d3c-41fb-9521-e8968cdf7ddd",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/05/bitcoin-cryptocurrency-new-record-value-explainer",
        "title": "Bitcoin: what has caused the cryptocurrency’s latest revival?",
        "author": "",
        "published_date": "2024-03-05T17:04:11+00:00",
        "content": "Bitcoin, the cornerstone of the cryptocurrency market, has reached a new record value more than two years after its previous peak. On Tuesday, the digital asset passed its previous peak from November 2021 of just under $69,000, although it later eased back to a little above $64,000.\nBitcoin is now worth about $1.3tn, a substantial chunk of the total $2.6tn cryptocurrency market. Here are the factors behind its latest revival.\nWhat is bitcoin?\nBitcoin was created in 2008 by Satoshi Nakamoto, the pseudonymous author of a white paper that established the concept of a digital currency that allows “online payments to be sent directly from one party to another without going through a financial institution”.\nThe “double spend” problem of someone duplicating or falsifying a digital token – which cannot be prevented by a separate institution policing the system, because that would go against the underlying principles of bitcoin – is solved by having transactions recorded on a universally accessible ledger called a blockchain.\nThis is all secured by cryptography, where transactions are protected by a form of encryption called public-private key encryption. This enables a transaction to take place without a financial institution sitting in the middle of it.\nTransactions are placed on the blockchain by bitcoin “miners”, who get to pack them into blocks that are linked (or “chained”) together, by solving a cryptographic puzzle using specialised hardware. These miners are rewarded with newly created bitcoins.\nWhy has bitcoin been so popular?\nA key aspect of bitcoin’s appeal is its anti-authoritarian stance – the ability to carry out financial transactions without a financial institution overseeing the process and charging fees. Tim Swanson, a cryptocurrency industry commentator, has described it as “censorship-resistant digital cash”.\nIt has also benefited from a low-interest-rate environment – a longstanding economic trend since the 2008 financial crisis – that has pushed some investors towards riskier assets, such as cryptocurrencies, in pursuit of better financial returns. It has also been viewed as an “inflation hedge”, like gold, meaning that it cannot be devalued by a central bank printing more of it, because bitcoin is designed to have a finite number of units in issue – 21m to be exact.\nCarol Alexander, professor of finance at the University of Sussex business school, argues that people are mistaken to view bitcoin as a gold-like safe haven from market volatility and inflation.\n“Like gold, bitcoin has been viewed as ‘uncorrelated’ with stock markets, but it is far too volatile an asset to be considered like that,” she says.\nAnd, obviously, its performance at various points in its short existence – its price rose 70% alone in May 2017, for instance – has also drawn in people attracted to the publicity around its at-times outsize returns.\nWhy has it risen in price this time?\nA major factor in bitcoin’s rise since the start of the year has been the approval by the US financial regulator in January of exchange-traded funds [ETFs] – a basket of assets that can be bought and sold like shares on an exchange – that track the price of bitcoin.\nThe ETF announcement shows there is now “institutional maturity” in the cryptocurrency market, according to Jeff Billingham, the director of strategic initiatives at research firm Chainalysis. “We did not see this kind of infrastructure and trust in the market in the previous cryptocurrency bull runs,” he says.\nContinuing doubts over the stability of the cryptocurrency market were underlined by the collapse in November 2022 of FTX, one of the world’s largest crypto exchanges, and the subsequent trial and conviction of its founder and chief executive, Sam Bankman-Fried. The market’s most influential figure, Changpeng Zhao, founder of the world’s largest cryptocurrency exchange, Binance, also faces a spell in jail after pleading guilty in the US to federal money-laundering violations.\nThe head of the Securities and Exchange Commission (SEC), Gary Gensler, remains sceptical about the market despite begrudgingly approving the bitcoin ETFs, having had his hand forced by a court ruling.\n“Bitcoin is primarily a speculative, volatile asset that’s also used for illicit activity including ransomware, money laundering, sanction evasion, and terrorist financing,” he said in a statement announcing approval of the ETFs.\nJames Knightley, the chief international economist at the banking group ING, says elevated inflation readings out of the US over the past month have encouraged bitcoin buyers who see the cryptocurrency as an insurance policy against rising prices, while a general boom in tech stocks has steered investors to look at riskier assets like bitcoin.\n“Risk appetite has also soared in recent weeks with tech stocks fuelling the sense of Fomo [fear of missing out] in markets and I think bitcoin is being swept along in all of this,” he says.\nIs the latest rise sustainable?\nThe momentum has to ease at some point, says Neil Wilson, the chief analyst at brokerage firm Finalto. Wilson says “parabolic” market moves – where prices shoot up dramatically – are “never sustainable in themselves”.\n“It will run out of steam. But that doesn’t mean it can’t go higher, just that some kind of consolidation or correction seems likely in the interim,” he says, citing “standard” factors such as investors cashing in their profits and the supply of new buyers drying up.\nThere is also the upcoming “halving” event – where the amount of bitcoin released into circulation via mining is halved – which has boosted prices when it has happened in the past, as reduction in supply leads to a higher price.\nJohn Reed Stark, a former senior SEC official and senior lecturing fellow at Duke University’s school of law, says the “greater fool” theory – that there will always be someone willing to overpay for an already overpriced asset – will come into play. People are able to sell hyped assets until “there are no greater fools left, and then it all comes crashing down”, he says.\nCould a regulatory crackdown affect the price of bitcoin?\nRegulators are bringing in tighter oversight of cryptocurrencies in the UK and EU. In the UK, the Treasury is proposing to bring stablecoins – a type of cryptocurrency whose value is pegged to another asset such as a currency or a commodity – under the aegis of existing regulation. The EU has brought in the Markets in Crypto-Assets regulation (MiCa) regime, which requires crypto firms to register with a member state regulator.\n \n\n\n\nA stablecoin, like the name suggests, is a type of cryptocurrency that is supposed to have a stable value, such as US$1 per token. How they achieve that varies: the largest, such as tether and USD Coin, are effectively banks. They hold large reserves in cash, liquid assets, and other investments, and simply use those reserves to maintain a stable price.\nOthers, known as \"algorithmic stablecoins\", attempt to do the same thing but without any reserves. They have been criticised as effectively being backed by Ponzi schemes, since they require continuous inflows of cash to ensure they don't collapse.\nStablecoins are an important part of the cryptocurrency ecosystem. They provide a safer place for investors to store capital without going through the hassle of cashing out entirely, and allow assets to be denominated in conventional currency, rather than other extremely volatile tokens.\n\n\n\n\n\nThere is also the recent US ETF approval, which, as a regulatory action, has helped support bitcoin’s price resurgence.\nHarry Eddis, the global co-head of fintech at the London-based law firm Linklaters, says: “More regulation, and more heavily regulated crypto assets like ETFs, could help people invest in cryptocurrencies when otherwise they wouldn’t because it gives them mechanisms for investing in these assets that they can trust. It could bring more investors into the market, which could in turn support bitcoin’s price if they invest in that cryptocurrency.”"
    },
    {
        "id": "aefce403-ffd6-42ab-a519-be6a8c26e5be",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/06/misleading-clickbait-is-prevalent-on-facebook-and-instagram-in-canada-after-metas-news-ban-could-it-happen-in-australia",
        "title": "Misleading clickbait is prevalent on Facebook and Instagram in Canada after Meta’s news ban. Could it happen in Australia?",
        "author": "",
        "published_date": "2024-03-05T14:00:06+00:00",
        "content": "Misleading viral clickbait dominates Facebook and Instagram in Canada after Meta pulled news from its platforms nine months ago, according to an expert. Now Australia could face a similar scenario online with the company preparing to battle the Australian government over payments to news organisations. Last week Meta announced it would no longer pay Australian news publishers, prompting the Australian government to explore the use of legislative powers to force the platform to negotiate with news media for payment.   Related: Meta on collision course with Australian government after announcing end to journalism funding deals   The dispute has raised the possibility Meta will block Australian news outlets from posting links to their content on Facebook and Instagram, as it did for six days in 2021, and has done in Canada since mid-last year. Experts say the Canadian ban has done little to hurt the social media giant, but has inflicted damage on the news outlets Canada wanted to help most. Canada’s federal government passed bill C-18, the Online News Act, in June 2023, with the aim of boosting revenues at Canadian journalism outlets by requiring Meta and Google’s parent company, Alphabet, to compensate publishers for hosting and linking to their content. Both tech companies initially balked at the prospect, but Alphabet eventually agreed to a deal with the government in November. Under the terms of the agreement, Google’s parent company would contribute C$73.6m (A$83m) a year to be distributed among Canadian news publishers. The deal came in part, experts have said, because C-18 targeted link sharing and indexing – a key aspect of Alphabet’s business model. But Meta has resisted the constraints of the legislation, arguing it is “fundamentally flawed”. In response, it blocked all news-sharing on its platforms, including Instagram and Facebook. Ahead of the ban Meta also announced it would end its partnership with the Canadian Press wire service, an agreement that helped fund 30 reporting fellowships for early career journalists from 2020. The ban came into effect in August amid the country’s worst wildfire season on record, and lawmakers feared it would prevent Canadians accessing up-to-date news in their communities and hamper evacuations. Broadcasters blasted the move as “anticompetitive conduct” and claimed it violated a provision of a federal law. Meta said in a statement at the time: “The Online News Act is based on the incorrect premise that Meta benefits unfairly from news content shared on our platforms, when the reverse is true. News outlets voluntarily share content on Facebook and Instagram to expand their audiences and help their bottom line.” Non-news content created by viral content makers has filled the space left by news stories.   Related: How is Meta’s news ban affecting communications amid Canada wildfires?   “A real-world, newsless Facebook turns out to be more toxic than I had anticipated,” Quebec University journalism professor, Jean-Hughes Roy, said. In 2022, Roy conducted a simulation of what users would see on Facebook if news was banned, but said he found the reality of the ban worse than his simulation predicted. “Viral content producers feed on news content, make it more sensational by adding misleading or false details and publish it on their Facebook pages or Instagram accounts. Such content isn’t blocked by Meta, while actual news is.” But the move does not appear to have dented how Canadians use Facebook. Figures from two digital analytics companies, shared with Reuters, show the number of daily active users on Facebook, and time spent on the social network, are largely unchanged since the news block began. Part of Meta’s argument against compensating Canadian journalism outlets was that news article links made up less than 3% of Facebook’s feed in the country – a claim it also made in relation to its Australian decision. Chris Waddell of Carleton University’s school of journalism said Meta was increasingly wary of its place in the news industry. “I don’t think they’ve lost any advertisers,” he said. “I don’t know if their decision has really made a huge amount of difference [to the company]. “I think Meta would like to get out of the news in other places. I can’t imagine the company really wants to get caught in the controversy of the US election coming up, with all the fake, AI-generated information that’s going to be on Facebook. It’s just a minefield for them. If they’re right, they’re only getting 3% to 4% of their revenues from news, I can see why they would just bail out of it.” News Corporation’s CEO, Robert Thomson, told reporters on Monday that Meta’s 3% claim is “obviously a fiction – a preposterous figure”. “I mean how much discussion is there around news? You have the core news and then I can tell you 100% of the contemporary factual information on Facebook is news. And so those are the numbers that… Facebook should be focused on, as well as being focused on its responsibility to all Australians.” Large publications, have mostly found new ways of redirecting users to their sites. But Facebook’s refusal to share links on its platforms has had an outsized impact on smaller publishers. Eden Fineday, the publisher of IndigiNews, an Indigenous-led online journalism outlet, said the site had lost 43% of its traffic since the ban. “Facebook is a very Indigenous platform,” Fineday told the Toronto Star. “It is where a lot of native communities come to connect with each other. So it hurts us. Indigenous folks are the least thought-about demographic, especially by American companies. It’s sad to be just forgotten, and to have these companies not consider who’s being hurt by these changes.” The New Brunswick Media Co-op said it lost 5,000 Facebook followers ahead of the ban from Meta. In an attempt to offset the loss of traffic, 20 independent outlets, including the New Brunswick Media Co-op have banded together to form Unrigged. The aim is to both strengthen their negotiating position and to more effectively share news with readers. Waddell said the smaller outlets had the most work to go regaining readers in order to survive. “The people who have been hurt most, ironically, are the small startup publications or those that have been around for a while that were using Facebook as a promotional tool to try and bring a wider audience,” he said. Roy said he feared what the disappearance of news from Meta’s platforms would mean for Canadian democracy. “Forty-five percent of Canadians cite social media as a source of news, according to the latest Reuters Institute for the Study of Journalism Digital News Report – the proportion is the same in Australia – I worry young citizens grow up in a digital world where news simply doesn’t exist anymore.”"
    },
    {
        "id": "782f2ebd-5cf3-4227-b643-aaa037026645",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/05/nothing-phone-2a-review-standout-budget-android-battery-life",
        "title": "Nothing Phone 2a review: a standout budget Android",
        "author": "",
        "published_date": "2024-03-05T12:30:01+00:00",
        "content": "London-based tech firm Nothing’s latest Android attempts to shake up the budget phone market with something a little more interesting. Costing from £319 (€329/A$529) the Phone 2a aims to take the cool design and intrigue that made its higher-end models stand out and package it up into something cheaper but still novel, sits alongside the full-fat Phone 2 costing £579. The new model sticks with Nothing’s cool, semi see-through design. It has a big, smooth and bright OLED screen on the front with a semi-transparent back allowing you to see its interesting design elements, including the “glyph” LEDs that Nothing has made its trademark. This time the phone is made of plastic, rather than the metal and glass of the Phone 2, and the LEDs are only in the top section of the phone around the central camera module. But they can still perform complex patterns for ringtones and notifications, show the volume, charge percentage, timers and other fun things, such as a music visualiser. The phone feels smooth and well made, but the back plastic attracts dust like a magnet and picks up scratches fairly easily. Specifications  Screen: 6.7in 120Hz FHD+ OLED (394ppi) Processor: MediaTek Dimensity 7200 Pro RAM: 8 or 12GB Storage: 128 or 256GB Operating system: Nothing OS 2.5 (Android 14) Camera: 50MP main and ultrawide, 32MP selfie Connectivity: 5G, eSIM, wifi 6, NFC, Bluetooth 5.3 and GNSS Water resistance: IP54 (splash resistant) Dimensions: 162 x 76.3 x 8.9mm Weight: 190g  Mid-range power with long battery life Inside the Phone 2a has a mid-range MediaTek Dimensity 7200 Pro chip, which performs pretty well for the price. The software feels snappy and smooth, apps load quickly and games run well. It can’t match the performance of a high-end phone and gets pretty hot when gaming, but handles most tasks with aplomb. The Nothing has solid battery life too. The phone lasted a good 52 hours or two days between charges with general use, including several hours spent on 5G a day. Gaming dents the battery more than high-end rivals, consuming about 18% an hour playing Diablo Immortal for instance. Sustainability Nothing says the battery maintains at least 90% of its original capacity for at least 1,000 full charge cycles. The Phone 2a is generally repairable in the UK. Screen replacements cost £70 or batteries cost £36 plus about £35 labour and shipping by Nothing. The device is made of recycled aluminium, copper, plastic, steel, tin and other materials accounting for 20% of the phone’s weight. It has a carbon footprint of 52kg CO2 equivalent. The company publishes sustainability reports and runs a trade-in scheme. Nothing OS 2.5 The phone runs Nothing OS 2.5, which is based on Android 14 matching the firm’s other phones. The software offers a clutter-free experience but with an interesting visual design based around dot-matrix art, widgets and icons. It can be styled to look just like regular Android, but it encourages you to fill your home screen with monochrome icons, big folders and widgets for a much more interesting experience. Like the fun glyph lights on the back, none of these customisations bog the phone down or get in the way of day-to-day tasks. Unfortunately Nothing comes up short on software support, only offering three years of software updates and four years of bimonthly security updates. When the best in the business now offer at least seven years, four years just isn’t good enough for your wallet or the planet, even in the budget market. Camera The Phone 2a has two 50-megapixel cameras on the back and a 32MP selfie camera that perform very similarly to the more expensive Phone 2. The main camera general takes good photos handling high-contrast scenes well. But it has a tendency to over-sharpen fine detail in bright light while struggling to preserve detail in middling to dim light. The ultrawide camera produces decent shots too, but images look soft when viewed at full size. The phone has no telephoto camera with images captured beyond 2x digital zoom being poor. Still, the camera is decent for the money, though soundly beaten by Google’s Pixel 6A or 7A, which can often be found around this price. Price The Nothing Phone 2a costs from £319 (€329/A$529). For comparison, the Nothing Phone 2 costs £499, the Google Pixel 7a costs £449 and the Samsung Galaxy A54 costs £349. Verdict The Nothing Phone 2a is a solid entry into the budget phone market that manages to stand out from an otherwise often dull crowd with an interesting design and fun software. You get a lot of phone for the money, with a big and crisp display, long battery life, solid performance and a design that doesn’t look budget. The plastic body might put some off but it feels solid and well made, and is splash resistant, which isn’t guaranteed at this end of the market. The camera is good even if it can’t rival Google’s budget phones that can be had for only a little more. The worst bit is short software support life, which will render the phone unsafe to use well before its hardware gives out.  Pros: interesting design with glyph lights, good screen, decent performance, long battery life, slick Android 14 software, premium look feel at budget price, good fingerprint scanner, splash resistant. Cons: no optical zoom camera, only four years of security updates, camera not best in class, plastic back scratches easily.  "
    },
    {
        "id": "f273151f-6376-4b73-95fc-1a444269e63d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/05/techscape-open-source-software-cryptocurrency",
        "title": "How much does Spotify really pay Apple?",
        "author": "",
        "published_date": "2024-03-05T11:50:01+00:00",
        "content": "The tech industry is one of the most valuable sectors on the planet, but it ultimately rests on the unpaid labour of an alarmingly small amount of hobbyists. That reliance is the blessing, and the curse, of open source software – coding projects put up on the internet for anyone to use, freely, in their own work. Some open source software solves simple problems elegantly enough that no one wants to redo the work unnecessarily; others tackle complex tasks that few have ever attempted. This is no secret. In August 2020, you may have seen webcomic xkcd comment on the problem, with a cartoon depicting “all modern digital infrastructure” as a wobbly tower of bricks relying on “a project some random person in Nebraska has been thanklessly maintaining since 2003”. Or, a year later, a viral tweet by Druthers Haver:  the most consequential figures in the tech world are half guys like steve jobs and bill gates and half some guy named ronald who maintains a unix tool called ‘runk’ which stands for Ronald’s Universal Number Kounter and handles all math for every machine on earth  The jokes are based on reality. In 2016, a developer was told to change the name for a piece of software of his called kik, after the creators of an unrelated messaging app with the same name asked for the name to be assigned to them. In response, he unpublished kik, as well as everything else he’d made – which included a small piece of code called left-pad, which was used by thousands of other programs. Owing to the way left-pad was distributed, through a service called a “package manager”, those programs began failing in minutes, as they tried to update themselves to a new version of left-pad that didn’t exist. And then the programs that incorporated those programs began to fail as well. It took two and a half hours to catch and fully fix the problem. Before that, there was heartbleed. In 2014, researchers found a crippling bug in a piece of software called OpenSSL, which websites used to enable the encryption standard that keeps credit cards and passwords safe online. Maintained, again, by a few volunteer developers, OpenSSL solved a common problem simply and easily, meaning that it was the go-to way of adding good-enough security to your website. Except it wasn’t. The developers hadn’t noticed a bug in the code which could be used to access data that was supposed to be encrypted – and which had been sitting there for more than two years. Seven years later, the same basic thing happened again, in a different open source utility called Log4j. Giving software away for free is great for a whole host of reasons – but quite bad at funding continued development of that software. There have been loads of attempts to fix that, from models of development where the software is free but the support is paid, to big companies directly hiring maintainers of important open-source projects. Lots of projects have ended up in a tip or supporter-focused model (remind you of anyone?), which can work for big complex tasks but falls down for some of the simplest – yet most widely used – pieces of work. And then, there’s this:  A project called tea.xyz promised people they could “get rewards for [their] open source contributions”, complete with a flashy website describing how it would “enhance the sustainability of open-source software”. So far, it’s achieved the exact opposite. Promising to reward open source contributors with crypto tokens, the project asked users to verify their access to open source projects by merging in a YAML file containing their crypto wallet address.  What was supposed to happen: a crypto-based system would give you micropayments for working on open source software, and redistribute tokens from users to contributors. What actually happened: crypto fans flooded open source projects with low-effort alterations in an effort to become the “owner” of various highly used projects, resulting in already overwhelmed volunteer coders having to sift through spam and worse rather than work, or raise money. It’s good that there’s growing awareness of the problem, but the solution clearly needs some work. The curious case of the hundred bucks Apple has been hit with its first-ever fine from the EU. At “over €1.8bn”, it’s straight in at number three in the bloc’s highest penalties ever. From our story:  The fine is nearly four times higher than expected in a move by the European Commission to show it will act decisively on tech companies who abuse their dominant position in the market for phones and online services. The European competition commissioner, Margrethe Vestager, said a smaller fine would have been nothing more than the equivalent of “a parking fine” and was designed to act as “a deterrent” to such practices for Apple and others. As a result of the anti-competitive practices, the public ended up paying more than they should have for music streaming, she said. “Apple’s rules ended up harming consumers. Critical information was withheld so that consumers could not effectively use or make informed choices. Some consumers may have paid more because they weren’t aware that they can pay less if they subscribed outside of the app.”  The EU’s rules set the level of the fine at “15-25%” of the affected revenues, which would have been peanuts given the scale of Apple Music in Europe, so the regulator added a lump sum of €1.8bn to top it up to an amount that may actually provide some deterrence. We’ve covered Apple a lot here at TechScape recently (earlier on this very fine, and on the company’s compliance with the digital markets act, coming into force on Thursday), so I won’t dote on it too much. Except there is something weird here. In its response, Apple vowed to appeal the ruling, and was insistent that its App Store model provides good value for developers – allowing the likes of Spotify to grow to the scale it is without ever paying a commission on its revenue from iPhone users. And there’s something bugging me. Here are a few quotes from Apple’s lengthy public reply to the EU:  If a developer sells physical goods, serves ads in their app, or just shares an app for free, they don’t pay Apple anything. When it comes to doing business, not everyone’s going to agree on the best deal. But it sure is hard to beat free. But free isn’t enough for Spotify. Today, Spotify has a 56 percent share of Europe’s music streaming market – more than double their closest competitor’s – and pays Apple nothing for the services that have helped make them one of the most recognizable brands in the world. Despite that success, and the App Store’s role in making it possible, Spotify pays Apple nothing. And again – Spotify pays Apple nothing.  Except … Spotify, like every business on the App Store, pays Apple a $99 annual fee to be part of the Apple Developer Program. In the context of a clash between two multibillion-dollar companies, $99 a year rounds to zero, of course. But, with 34 million registered developers, this is a business worth at least $100m a year to Apple (since some developers, including nonprofits and government agencies, do get a waiver). I get it – saying “Spotify pays Apple nothing” is a much stronger lobbying position than “Spotify pays Apple just $99 a year, the same as every other developer”. But only if it is, you know, true. And Apple makes hundreds of millions of dollars a year from charging that fee to developers as standard, which complicates the narrative that the App Store is only funded by commission on sales. And, of course, Spotify pays Apple millions of dollars a year for the devices it uses to create software for the company’s App Store. You cannot develop iOS apps without a Mac. Is it possible that Spotify acquires every Mac and iOS device it uses in the course of its operations second-hand, deliberately avoiding handing any money to Apple? Sure, I guess. Is it likely? Come on now. I tried asking Apple how they squared this, and a spokesperson repeated the claim that Spotify paid $0 to Apple. When I asked if I could explicitly write that “Apple claimed that Spotify is not charged the developer fee”, though, the company stopped replying to my emails. Spotify had no such bashfulness, and confirmed that they pay the fee like all major developers. There’s a chance this is simply me getting unhealthily obsessed with a point of principle. As a journalist, I value truth, and bristle at spin that, in my opinion, crosses the line into falsehoods. Maybe none of this matters. But… it doesn’t look good, does it? • If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Tuesday."
    },
    {
        "id": "43d8206e-24b9-4930-a709-50f436c46971",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/04/former-twitter-execs-sue-elon-musk-severance",
        "title": "Ex-Twitter executives sue Elon Musk for $128m in unpaid severance",
        "author": "",
        "published_date": "2024-03-04T22:45:36+00:00",
        "content": "Elon Musk is facing a $128m lawsuit from four former Twitter executives who allege the billionaire tech mogul failed to pay them severance after buying the social network. The suit, filed on Monday in California, follows a separate legal complaint last year by rank-and-file employees seeking $500m in unpaid severance. “Because Musk decided he didn’t want to pay Plaintiffs’ severance benefits, he simply fired them without reason, then made up fake cause and appointed employees of his various companies to uphold his decision,” the suit alleges.   Related: Elon Musk sues OpenAI accusing it of putting profit before humanity   The four plaintiffs in the case include Twitter’s former CEO Parag Agrawal, former CFO Ned Segal, former general counsel Sean Edgett and former chief legal officer Vijaya Gadde. Musk fired all of them amid a string of mass layoffs after he acquired Twitter for $44bn in 2022, claiming at the time he did not need to pay the executives severance because they were terminated for cause. “‘Cause’ is not ‘board-approved business decisions that Musk dislikes’,” the complaint reads. “He claimed in his termination letters that each Plaintiff committed ‘gross negligence’ and ‘willful misconduct’ without citing a single fact in support of this claim.” Musk and X have not issued any public comments on the case. Musk’s frequent legal representative, the lawyer Alex Spiro, did not immediately return request for comment. The lawsuit is one of several legal actions related to Musk’s unwilling takeover of Twitter and subsequent operation of the platform, which he renamed X. The National Labor Relations Board also issued a complaint earlier this year alleging that Musk’s SpaceX rocket company illegally fired eight workers because they issued a letter criticizing his leadership. Musk laid off about 80% of Twitter staff after he took over the company, he told the BBC in an interview last year. The site has struggled in numerous ways since his acquisition, including facing declining ad revenues and researchers documenting a surge in hate speech as Musk rolled back content moderation efforts. He originally tried to back out of the acquisition, but Twitter sued to force its completion. Musk has blamed the decline of ad revenue on anti-hate monitoring groups that published reports detailing racist and extremist content on the platform. He launched suits against two of the organizations, Media Matters and the Center For Countering Digital Hate, which are currently ongoing. A California judge is expected to decide this week on whether to dismiss the suit against the Center For Countering Digital Hate."
    },
    {
        "id": "19567f0f-f557-40f9-a360-c7be578b3960",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/04/forerunner-165-review-garmin-oled-running-watch",
        "title": "Forerunner 165 review: Garmin’s budget OLED running watch",
        "author": "",
        "published_date": "2024-03-04T07:00:22+00:00",
        "content": "Garmin’s latest smart sports watch condenses all the great features from its higher-end Forerunner models into a cheaper, simpler running tracker with a bright OLED screen and long battery life. The Forerunner 165 is the new base model in Garmin’s new lineup, priced from £250 (€280/$250/A$429) compared with the £430 Forerunner 265. The watch comes in a choice of colours but only one size, with a 1.2in screen and 43mm case – bang in the middle of the small and large sizes of the more expensive 265. It is a good size, with a crisp OLED display that’s large enough to make it easily readable at a glance, while feeling slim, light and compact on the wrist. It has the same great combination of touchscreen and five buttons as recent Garmin watches. It syncs data to an Android or iPhone through the Connect app, via USB cable to your computer or direct to the internet through wifi if you buy the more expensive version with offline music playback. The battery lasts about five days between charges with the screen on all the time, tracking sleep overnight and a single 40-minute run. That is shorter than the 265 by a few days but more than double mainstream smartwatch rivals. Setting the screen to turn on only when you rotate your wrist extends the battery life to more than 11 days. Runs consume about 6% battery for each hour or 14% with offline music from Spotify, which makes it last more than long enough for most people. A full charge via the USB-C cable takes about an hour. Specifications  Screen: 1.2in AMOLED Case size: 43mm Case thickness: 11.6mm Band size: standard 20mm Weight: 39g Storage: 4GB Water resistance: 50 metres (5ATM) Sensors: GNSS (GPS, Glonass, Galileo), compass, thermometer, heart rate, pulse Ox Connectivity: Bluetooth, ANT+ (wifi with music)  Sport tracking prowess The big difference between the 165 and higher-end Garmin models is the lack of multisport tracking, such as automatic transitions between runs, cycles and swimming for a triathlon. Otherwise it tracks a total of 24 individual sports including various forms of cycling, running and swimming, walking and hiking, gym activities and racket sports. The notable absences are skiing and snowboarding. For running it tracks all the usual metrics such as time, distance, pace, cadence, laps, but also running dynamics and power, which are both rarer but welcome features for more enthusiastic runner. During a workout you can have up to four metrics on screen at any one, with a clear and easy to read at a glance display. For most uses on a run the 165 behaves almost exactly the same as a 265 or the top-of-the-line Forerunner 965, which is excellent. It lacks dual-band or “multiband” GPS, which means on paper its location tracking is not as accurate as higher-end Garmin models when running in trickier environments such as cities with lots of tall buildings or dense forests. But in a test side-by-side with a 965 with the feature, the two watches got a GPS lock very quickly and barely deviated from the correct track and pace, which was very impressive. The watch can tell you when you veer off a planned route but lacks full offline maps to see where you are, which is a feature still reserved for Garmin’s top watches. The final thing missing from Garmin’s best-in-class sport tracking on the 165 is training readiness, status and load, which are three measures on the firm’s high-end watches useful for measuring the effectiveness of training for goals, such as increased fitness or marathon prep. Garmin’s full suite of general health tracking measures are on the 165, including sleep and nap detection, daily calories, steps, stress and all the other things you should expect from a smartwatch, including notifications from your phone and contactless payments. The only thing missing is the ability to take an ECG of your heart, which probably doesn’t matter for a sports watch. Sustainability The Forerunner 165 is generally repairable, and out-of-warranty exchange and refurbishment are available. The battery is rated to last at least a few years of frequent charge cycles while maintaining at least 80% capacity. The watch does not contain any recycled materials. Garmin guarantees at least two years of security updates from release but typically supports its devices far longer. It offers trade-in schemes for some lines and complies with WEEE and other local electronics recycling laws. Price The Garmin Forerunner 165 is priced at £249.99 (€279.99/$249.99/A$429) or £289.99 (€329.99/$299.99/A$499) with offline music support. For comparison, the Forerunner 265 is £429.99, the Apple Watch Series 9 is £399, the Google Pixel Watch 2 is £349, the Coros Pace 3 is £219 and the Polar Pacer Pro is £289. Verdict The Forerunner 165 is an excellent running watch that condenses the best bits from its top-class siblings into a simpler, cheaper model. It has a great, clear and bright OLED screen that’s easy to read mid-run. It has all the metrics to keep track of your pace and performance and has a long enough battery to go the distance for most users. Its compact and light body keeps it comfortable even for long training sessions and the combo of touch and buttons is best in class. The only things really missing are offline maps and more advanced training metrics. The lack of the more advanced, multiband GPS tracking doesn’t appear to significantly impact tracking accuracy, for which Garmin is still the leader. The biggest problem is the price. At £250 or £290 with offline music, it is more expensive than most beginners’ running watches, though in line with OLED-equipped smartwatches from tech rivals and a £180 saving over the Forerunner 265. Unless you want to track triathlons or need offline maps, the Forerunner 165 is the new Garmin running watch to go for.  Pros: slim, light, real buttons, and crisp OLED touchscreen, accurate GPS and heart rate, extensive stats, good health tracking, five-day battery life, offline music option, basic smartwatch features. Cons: a little expensive, no offline maps, no voice assistant, no multiband GPS, no multisport transitions, limited Garmin Pay compatibility with UK banks. "
    },
    {
        "id": "e8f7b299-3a5b-4a7f-8b54-568017cd7bb1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/04/musk-needs-to-be-adored-zuckerberg-is-out-of-his-depth-kara-swisher-on-the-toxic-giants-of-big-tech",
        "title": "‘Musk needs to be adored … Zuckerberg is out of his depth’: Kara Swisher on the toxic giants of Big Tech ",
        "author": "",
        "published_date": "2024-03-04T05:00:19+00:00",
        "content": "“This is about love gone wrong,” says Kara Swisher, looking back on a life spent studying the giants of Big Tech. “I saw the possibilities of tech being the saviour of humanity – or at the very least, really helping people, in terms of community and knowledge and education. And instead, you know …” She pauses, and wearily exhales. “It’s like that old expression: ‘They promised us jetpacks, and this is what we got?’ Like, are you kidding me?” But, she adds: “The problem isn’t tech. It’s people.” Swisher is essentially a business journalist, but her speciality is human beings and what they do with wealth and power. She has been scrutinising Silicon Valley for around three decades – writing ferociously and insightfully for the Washington Post, the Wall Street Journal and the New York Times, founding the tech news website Recode, and blazing a trail into podcasting. Her writing has always struck a delicate balance between insider knowledge and biting irreverence. Now, though, her iconoclastic side has won out, given free rein in an extremely readable memoir, Burn Book. The title, she tells me, comes from the 2004 movie Mean Girls, in which the leading characters keep a shared diary full of slights and gossip about their classmates. As well as telling her own story, the book centres on pen-portraits of people – men, mostly – she has closely observed as their wealth and influence has ballooned: Bill Gates, Steve Jobs, Mark Zuckerberg, Jeff Bezos, Elon Musk, and Sergey Brin and Larry Page, the founders of Google. A few of them emerge as rounded, deep-thinking people who at least understand the huge questions of power that swirl around them. Others, by contrast, are more cuttingly portrayed – as “fresh-faced wunderkinds I had mostly rooted for” who eventually made the author “feel like a parent whose progeny had turned into, well, assholes”. Swisher talks to me on video call from her home in Washington DC, but she still has a house in San Francisco, and says she spends as much time as she can there. And she insists, again and again, that she still has faith in tech’s liberating, knowledge-enhancing potential: “I still believe in it. Stuff around generative AI, for example: I’ve talked to so many doctors and they’re like: ‘This could change the whole game on cancer entirely.’ Or climate change technologists who say: ‘We can now really begin to understand solutions to this.’” She also says she remains a believer in what she calls “the most vaunted parts of the American experience”: the US system of democracy, and the idea that anyone can become a success (“It’s full of hypocrisy, and at the same time it’s very true”). But in Burn Book, these two articles of faith are sorely tested by the moment she uses to symbolise tech’s spectacular fall from grace: 14 December 2016, when many of the executives and company founders she had on speed-dial met the newly elected president on the 25th floor of Trump Tower, in New York.   The Google guys always had strange clothes, said odd things to you and wandered away   Even now, she talks about this in the pained manner of someone recalling eating rotten food: it was a hugely significant moment, in the worst possible way. “My issue, if you can believe it, wasn’t really with Trump,” she says. “I get him. He’s very easy to read. He’s a racist, he’s a homophobe – he’s everything terrible about the United States. My issue was with the tech people who knew better, didn’t like him, but wanted money. More money. They wanted less regulation and the ability to grow, unrestricted. They knew he was bad. He was very anti-immigration, and this was an industry built on immigration. All I wanted them to do was say something publicly, like: ‘We’re going, but let me tell you, Mr president-elect, we are going to do everything to fight you on your immigration statements.’ They could have done anything. But they snuck in. They really did.” She then mentions the pithy statement of supposed corporate ethics still enshrined in Google’s code of conduct. “‘Don’t be evil.’ Well, evil’s sitting in front of you. You might want to have a word.” Swisher is 61, a bit older than most of the tech bros she has spent so much time chronicling. The sense of an insider-outsider is compounded by her upbringing in Long Island, which seems to have leavened her embrace of California optimism with a very north-eastern kind of sarcasm and scepticism. In a world still dominated by straight white men, moreover, her gender and sexuality also set her apart, having played a big role in her pre-tech backstory. Her initial ambition, she says, was to follow the example set by her father, who had served in the US Navy, and work as a strategic analyst for either the military or the CIA. But as she writes, “pushing against the anti-gay tide was nearly impossible at the time, and the ferreting out of gays in the military continued for over a decade”. She began making her way in journalism. By 1996, she was dedicatedly reporting about the new world taking shape in northern California, and what she calls “supremely odd but compelling people”. “Like the Google guys – they always had strange clothes and they said odd things to you and … wandered away,” she says. “They would have kind of weird and wacky headquarters. A lot of toys, which actually was a signal to me, and not a good one. I was like: ‘What are they doing?’ The idea of being childlike – they loved that. A lot of their clothes were very juvenile. So was their food. There’d be, like, pogo sticks. A lot of graffiti on the walls, but paid-for graffiti. And ping-pong tables.” In the book, these regressive tendencies reach their jaw-dropping nadir at the baby shower held in 2008 by Google co-founder Brin and his wife. Guests, Swisher recounts, could wear “a diaper with an oversized comical pin”, “a ruffled baby hat that came with a rattle” or “adult-sized footy pyjamas accessorised with a teddy bear and a sucker”. She refused all the sartorial options, but soon encountered Wendi Deng – then the wife of Rupert Murdoch, who Swisher calls “Uncle Satan” – wearing “leather pants and stiletto boots under the giant Pampers”. All this weirdness, Swisher says, was there, whether consciously or not, to smooth over the fact that the new tech industry was not the big-hearted humanitarian project its founders often talked about, but something much more straightforward: the latest iteration of rapacious capitalism.   Someone so ill-prepared like Zuckerberg was making decisions that unleashed an enormous amount of toxic waste   “I thought it was all performative. It was like: ‘Aren’t we different?’; and I was like: ‘You’re not really that different.’ I was irritated by the performative nature of it all, you know: all soft and squishy, but hard as nails on the inside. And that’s what these people were, right? They were always killers. Every one of them.” This leads on to another of her book’s big themes. If people like this acquired any degree of power, they were probably always going to be out of their depth – a point that seems to apply particularly vividly to Zuckerberg. When she first met the-then boss of Facebook, she was struck by the fact that he looked “like a newborn something, all fawn-like eyes and wide forehead”, and was painfully socially awkward. But he also “craved power and historical significance from the get-go”. Does she think he ever feels any fear about the huge responsibilities that ought to come with what he has built? She answers emphatically. “Yes. I think every now and then you see that he knows he’s in over his head.” She mentions an interview she did with Zuckerberg in 2018, for her Recode Decode Podcast, in which he expressed the somewhat startling opinion not only that Facebook (since folded into the giant company that Zuckerberg named Meta) should host content put up by Holocaust deniers, but that such people were “not intentionally getting it wrong”. “He was so out of his depth, and you could see that he kind of knew it,” she says. “But he kind of walked into it: like: ‘I can handle this.’ I’m like: ‘You cannot handle hundreds of years of antisemitism – I’m telling you that you can’t. You need some real experience.’ And that was my issue: someone who was so ill-prepared was making decisions that affected all kinds of people and unleashed an enormous amount of toxic waste.” She describes his recent appearance in front of the judiciary committee of the Senate, also attended by parents who had lost their children to suicide after horrific experiences online. “They were there,” Swisher tells me, “and they had pictures of their kids. They held them up, these photographs. I saw him look at the entire group. And you could see – like: ‘Oh my God.’ But he couldn’t bring himself to apologise directly. He had to say: ‘I’m sorry for what was done to you.’ And that’s not an apology. I don’t know what that is.” She pauses. “It’s an acknowledgment of pain.” What does that say about him? “It says he still can’t take responsibility. You could see in his eyes that he was just like: ‘Whoa, whoa, whoa.’” He could see the impact. But he couldn’t say: ‘I’m sorry.’ They were saying: ‘Things you did directly, decisions you made, helped kill my child.’ And he couldn’t address that. It’s so passive.” Meta’s president of global affairs, let us not forget, is the former Lib Dem leader Nick Clegg. Has she had any dealings with him? “Yes. Of course. Nick. Smoothie Nick. I’ve never interviewed him: he won’t give me an interview because he’s smart. I just feel like: ‘Do you believe anything you’re saying?’ I don’t know. He’s such a pretty sayer of things. I’m sort of like: ‘What do you actually believe in here? What is your goal?’ That’s what I often think of with him.” In the book, Swisher says Zuckerberg is “the most damaging man in tech”. Elon Musk, by contrast, is maligned as the “most disappointing”, which reflects Swisher’s long period of thinking of the founder of Tesla and SpaceX as one of the tech industry’s most promising sons. In 2016, she contacted him ahead of the big meeting with Trump, warning that the president-elect would “screw” him; two years later, Musk told her she had been right. All told, she seemed to believe that he operated on a higher level than most of his peers. “Here’s someone who actually was doing serious things,” she says. “There’s a lot of people in Silicon Valley who are always doing a dry cleaning app. He was thinking of everything from cars to space to solar. Even the silly stuff like [his imagined high-speed transport system] Hyperloop: what a great idea. What an interesting idea.” She also mentions Neuralink, the venture working on computer interfaces that can be implanted in people’s brains. “How could we upgrade our intelligence? That’s a big, fascinating problem.” Initially, she thought his ownership of X would be a good thing: “He used the product and he understood it. And he’s a guy who got things done.” But then came a rupture: in October 2022, she tweeted about a Washington Post article that questioned his work on the satellite-based internet system Starlink, and he emailed her the eloquent words: “You’re an asshole.” Soon after, he began opening X up to the kind of far-right voices that had been banned, and plunging into conspiracy theories – as she puts it, “going off the rails every day of the week and twice on Sunday”. She says she is saddened by all this. “The drug that’s hurting Elon Musk is all his enablers, who suck up to him. And needing to be adored. That’ll kill you.” And she watched it happen, tweet by tweet: “You could see him getting more and more radicalised. He suddenly got obsessed with the woke mind virus, whatever the hell that is, and angry all the time.” Given its huge financial problems, does she think X will eventually fold? “Why would it? He’s the world’s richest man. It’s like having a mega-yacht: it’s expensive, but he’s got the money to keep it up. And they’ll give him loans … so it will only go down when he decides to stop paying for it.” Does he think he will? “No! I think he loves it. He desperately needs attention. Trump is running for president because he desperately needs attention, among other things. And Musk really needs attention. If he owns X, he becomes quantumly more interesting to people across the globe.” Which brings us to one of the biggest questions of all. As that summit with the titans of Big Tech proved, Trump is arguably the quintessential politician of the internet age. Whether Musk explicitly supports him or not, X’s reshaping as something of a right-wing hellscape will be a sizable help to his re-election campaign. Come November, does Swisher think he’ll win? “No. No. This is where I believe in the American people. There’s an angry strain through the American experience that’s never left us. We’re an angry people, in many ways. But we’re also hopeful people. And people have had enough of him, especially women. “They’re tired of being groped. They’re tired of being cheated. They’re tired of loudmouths. Biden is not the ideal candidate, but he’s a decent man and he’s done a lot of stuff, and it will sink in enough to get rid of Trump.” Swisher has not spent the last 90 minutes mincing her words, and so it proves again. “He might be in jail, and I hope he is,” she says. “I hope they put him away and throw away the key.” Burn Book by Kara Swisher (Piatkus, £25) is out now. To support the Guardian and the Observer, buy a copy at guardianbookshop.com. Delivery charges may apply."
    },
    {
        "id": "612d9f4a-00e7-40b0-bedc-996f5b8cd394",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/03/eva-wiseman-banning-phones-in-schools-is-just-another-plot-to-distract-us",
        "title": "Banning phones in schools is just another ploy to distract us | Eva Wiseman",
        "author": "",
        "published_date": "2024-03-03T08:00:08+00:00",
        "content": "Whenever the government talks about the dangers to children of the mobile phone we must picture this phone as a large rock under which a hundred shameful decisions live in darkness. I have concerns about mobile phones, of course I do. I’ve followed Esther Ghey’s campaign to ban smartphones for kids, and how education secretary Gillian Keegan has leapt upon this, with cautious interest. My eldest child will soon be approaching secondary school, and while I’d vaguely assumed that her generation would find phones desperately unchic by the time they came of carrying age, associating them with dull and red-eyed parents, this does not appear to be the case. And the stories I hear from teachers or parents of teenagers sometimes chill me – the ways that bullying mutates online, or how phones exacerbate poor mental health, or teens’ sinister, quotidian acts of surveillance.   The government pictures mobile phones as a large rock under which a hundred shameful decisions live in darkness   But all these issues, for me, have their roots in the uncomfortable world our politicians have created for us. It makes sense that children might film each other maliciously, or be disrespectful of their peers’ privacy when they have been casually surveilled and tracked by both their parents and the state since birth. It makes sense that children find their mental health crumbling in a country where mental health services have been diluted to a homeopathic degree. And it makes sense that a climate of distrust, with its fetishisation and dehumanisation of vulnerable people like Ghey’s daughter Brianna, who was murdered in 2023, will filter from mainstream politics into schools and phones. Politicians are using phones and the anxiety around them as a way of distracting voters from the real violence they have helped create – the devices themselves are simply conduits for horrors and paranoias they’ve cultivated. The government has taken up Ghey’s call with the kind of vigour only seen in politics when it costs nothing. This is their new “clap for the NHS”, a gesture designed to give the illusion that action is being taken, and to pull focus from profound cruelty and mismanagement. Rather than accepting responsibility for politics that too quickly evolves into abuse, they are taking empty action on the technology that broadcasts it. Gillian Keegan is currently urging schools to ban phones. Only, most schools already have. So this new guidance reads like (as teacher Nadeine Asbali wrote in the Guardian last week) “the government establishing a ban on anyone leaving their front door wide open at night”. Schools never allow pupils to have phones in lessons, because it has never made sense to. This is a distraction technique that feels a bit like shining a bright light into parents’ eyes. These are people, like me, whose concerns for our children’s lives online are only compounded by the knowledge of our own grim relationship to our phones. As Keegan was unveiling her plans to ban phones in schools, the Mirror reported that five Conservative backbenchers could be seen looking at their screens. Later, as the treasury minister discussed the recession, the business minister sat on the frontbench next to him checking her emails. And I feel for them! If I were there it would take every last centimetre of effort not to do exactly the same. We all know the siren call of a small, hot phone – we know how it steals time, leads us to spread disinformation, invites scammers, complicates desire. But we also know how it can guide us through a life, both literally, with jazzy maps and dating apps and endless information, and in more ephemeral ways – here is a place to communicate, play, perform, build an identity. It is tempting to dampen our adult anxiety about technology by banning phones for our children, but this is not just impractical, it lacks nuance and empathy. Surely it is a matter of understanding and educating, rather than this grim fear and blunt force. There are a thousand ways to live online. There are shopping centres there, and art galleries, and schools, and cats. If adults learned to approach the internet and social media more seriously, rather than with one eye half-closed, if we were prepared to acknowledge its benefits as well as interrogate our own guilt or terror (the most common solution offered to cure our “addiction” being a “digital detox”, and the banning of phones altogether), we would be in a far better position to teach the next generation how to navigate it. It’s a shame such a complex and emotive subject can be steered like this; that important conversations (like tech’s financial incentive to keep children on social media) will be trampled over by politicians looking for quick fixes and diversions to avoid exposing themselves as the problem. It’s sickening, actually. This government has long been happy using trans lives for political gain. Now, by jumping on Esther Ghey’s appeal, they are using trans death to shift focus from their own bigotry. Email Eva at e.wiseman@observer.co.uk or follow her on X @EvaWiseman"
    },
    {
        "id": "c36d1cc8-e8bb-4821-908f-4f40331d09b6",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/mar/01/elon-musk-sues-open-ai-profit-power-microsoft-sam-altman",
        "title": "Elon Musk sues OpenAI accusing it of putting profit before humanity",
        "author": "",
        "published_date": "2024-03-01T16:10:00+00:00",
        "content": "Elon Musk has filed a lawsuit accusing OpenAI and its chief executive, Sam Altman, of betraying its foundational mission by putting the pursuit of profit ahead of the benefit of humanity. The world’s richest man, a founding board member of the artificial intelligence company behind ChatGPT, claimed Altman had “set aflame” OpenAI’s founding agreement by signing an investment deal with Microsoft. The lawsuit, filed in San Francisco on Thursday, claims OpenAI is now developing artificial general intelligence (AGI) – a theoretical form of AI that can perform a range of tasks at or above a human level of intelligence – for profit rather than for the benefit of humankind. “OpenAI Inc has been transformed into a closed-source, de facto subsidiary of the largest technology company in the world: Microsoft. Under its new board, it is not just developing but is actually refining an AGI to maximise profits for Microsoft, rather than for the benefit of humanity,” the lawsuit alleges. The suit opens with Musk’s often-stated warning that AGI poses “a grave threat to humanity”. “Where some like Mr Musk see an existential threat in AGI, others see AGI as a source of profit and power,” said the lawsuit, adding that in the hands of for-profit companies such as Google, AGI poses a “particularly acute and noxious danger to humanity”. Musk and other tech experts are concerned that an AGI could evade human control and take actions that endanger the planet. The suit claims Altman purported to share Musk’s concerns over AGI and in 2015 proposed forming a non-profit AI lab that would be “the opposite of Google”, now known as OpenAI. Together with Greg Brockman, OpenAI’s president, who is also being sued by Musk, the three men agreed to create a lab whose principles would be enshrined in a founding agreement. The lab would be “for the benefit of humanity”, would be a not-for-profit company and would be open-source, the term for making the technology freely available. The lawsuit claims that Musk, who stepped away from OpenAI in 2018, was a “moving force” behind the creation of OpenAI and supplied a majority of its funding in its early years. Microsoft is now the biggest investor in OpenAI’s profit-making arm, which Altman runs, after a deal struck in 2020. The lawsuit claims that OpenAI, Altman and Brockman “set the founding agreement aflame” in 2023 after releasing GPT-4, the powerful model that underpins OpenAI’s ChatGPT chatbot. GPT-4’s design was kept secret and such behaviour showed a radical departure from OpenAI’s original mission, the lawsuit said. “This secrecy is primarily driven by commercial considerations, not safety,” says the lawsuit, which is claiming breach of contract, breach of fiduciary duty and unfair business practices. It added that GPT-4 was an AGI technology effectively owned by Microsoft, an arrangement that is allegedly outside the scope of the company’s licensing agreement with OpenAI. The lawsuit also claims OpenAI is developing a model know as Q* [Q star] that has an even stronger claim to be AGI. The lawsuit goes on to claim that the tumultuous period of events in November 2023, when Altman was sacked as OpenAI’s CEO and then reinstated, showed Microsoft had “significant leverage” over the company. The new board introduced after Altman’s reinstatement does not have the expertise to ascertain whether the company has achieved AGI and thus whether it has produced a product outside the scope of Microsoft’s licence, the lawsuit adds. “This case is filed to compel OpenAI to adhere to the founding agreement and return to its mission to develop AGI for the benefit of humanity, not to personally benefit the individual defendants and the largest technology company in the world,” the lawsuit claims. OpenAI’s deal with Microsoft is being scrutinised by competition authorities in the US, the EU and the UK. Brian Quinn, a professor at Boston College law school in the US, said there were multiple issues with the lawsuit. He said Musk did not have the standing to sue for breach of the OpenAI board’s certificate of incorporation because he was not a board member. The suit addresses this by claiming that a 2015 email between Musk and Altman setting out the founding agreement, together with the certificate, constitutes a contract. Quinn said this fell “far short” of being a viable legal argument. A demand for return of money that Musk invested in OpenAI was also likely to fail because the suit claimed OpenAI veered from its mission in 2023 – long after Musk stopped supporting the non-profit. “It’s hard to see that he has any standing to attempt to enforce his ‘founding agreement’ or the certificate,” said Quinn.  OpenAI, Microsoft and Google have been approached for comment."
    },
    {
        "id": "708b6e3a-7f28-4602-ac2f-aa1e2b68281c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/29/pegasus-surveillance-code-whatsapp-meta-lawsuit-nso-group",
        "title": "Court orders maker of Pegasus spyware to hand over code to WhatsApp",
        "author": "",
        "published_date": "2024-02-29T19:53:07+00:00",
        "content": "NSO Group, the maker of one the world’s most sophisticated cyber weapons, has been ordered by a US court to hand its code for Pegasus and other spyware products to WhatsApp as part of the company’s ongoing litigation. The decision by Judge Phyllis Hamilton is a major legal victory for WhatsApp, the Meta-owned communication app which has been embroiled in a lawsuit against NSO since 2019, when it alleged that the Israeli company’s spyware had been used against 1,400 WhatsApp users over a two-week period. NSO’s Pegasus code, and code for other surveillance products it sells, is seen as a closely and highly sought state secret. NSO is closely regulated by the Israeli ministry of defence, which must review and approve the sale of all licences to foreign governments. In reaching her decision, Hamilton considered a plea by NSO to excuse it of all its discovery obligations in the case due to “various US and Israeli restrictions”. Ultimately, however, she sided with WhatsApp in ordering the company to produce “all relevant spyware” for a period of one year before and after the two weeks in which WhatsApp users were allegedly attacked: from 29 April 2018 to 10 May 2020. NSO must also give WhatsApp information “concerning the full functionality of the relevant spyware”. Hamilton did, however, decide in NSO’s favor on a different matter: the company will not be forced at this time to divulge the names of its clients or information regarding its server architecture. “The recent court ruling is an important milestone in our long-running goal of protecting WhatsApp users against unlawful attacks. Spyware companies and other malicious actors need to understand they can be caught and will not be able to ignore the law,” a WhatsApp spokesperson said. NSO declined to comment on the decision. The litigation is continuing. When it is successfully deployed against a target, NSO’s Pegasus software can hack any mobile phone, gaining unrestricted access to phone calls, emails, photographs, location information and encrypted messages without a user’s knowledge. NSO was blacklisted by the Biden administration in 2021 after it determined the Israeli spyware maker has acted “contrary to the foreign policy and national security interests of the US”. NSO sells its spyware to government clients around the world and has said that the agencies who deploy it are responsible for how it is used. While NSO does not disclose the names of its clients, research and media reports over the years have identified Poland, Saudi Arabia, Rwanda, India, Hungary and the United Arab Emirates as among the countries that have previously used the technology to target dissidents, journalists, human rights activists and other members of civil society. NSO has argued that Pegasus helps law enforcement and intelligence agencies fight crime and protect national security and that its technology is intended to help catch terrorists, child abusers and hardened criminals. The Biden administration has raised alarms about the proliferation and abuse of products like Pegasus, saying they represent a potential threat to US national security and counterintelligence efforts. A new policy unveiled in early February will impose global visa restrictions on individuals who have been involved in the misuse of commercial spyware, including countries in the EU and Israel."
    },
    {
        "id": "cf7d5a88-9d90-4c6b-9849-dccaeaa485a1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/29/clothoff-deepfake-ai-pornography-app-names-linked-revealed",
        "title": "Revealed: the names linked to ClothOff, the deepfake pornography app ",
        "author": "",
        "published_date": "2024-02-29T19:28:17+00:00",
        "content": "The first Miriam al-Adib learned of the pictures was when she returned home from a business trip. “Mum,” said her daughter. “I want to show you something.” The girl, 14, opened her phone to show an explicit image of herself. “It’s a shock when you see it,” said Adib, a gynaecologist in the southern Spanish town of Almendralejo and a mother of four daughters. “The image is completely realistic … If I didn’t know my daughter’s body, I would have thought that image was real.” It was a deepfake, one of dozens of nude images of schoolgirls in Almendralejo that had been generated by artificial intelligence (AI) and which had been circulating in the town for weeks in a WhatsApp group set up by other schoolchildren. Some of the girls whose likenesses were being spread were refusing to go to school, suffering panic attacks, being blackmailed and getting bullied in public. “My concern was that these images had reached pornographic sites that we still don’t know about today,” Adib told the Guardian from her clinic in the town. State prosecutors are considering charges against some of the children,who created the images using an app downloaded from the internet. But they had been unable to identify the people who developed the app, who prosecutors suspect are based somewhere in eastern Europe, they said. The Spanish incident flared into global news last year and made Almendralejo, a small town of faded renaissance-era churches and plazas near the Portuguese border, the site of the latest in a series of warning shots from an imminent future where AI tools allow anyone to generate hyper-realistic images with a few clicks. But while deepfakes of pop stars such as Taylor Swift have generated the most attention, they represent the tip of an iceberg of nonconsensual images that are proliferating across the internet and which police are largely powerless to stop. As Adib was learning of the pictures, thousands of miles away at the Westfield high school in New Jersey, a strikingly similar case was playing out: many girls targeted by explicit deepfake images generated by students in their classes. The New Jersey incident has prompted a civil lawsuit and helped fuel a bipartisan effort in the US Congress to ban the creation and spread of nonconsensual deepfake images. At the centre of both the incidents in Spain and New Jersey was the same app, called ClothOff. In the year since the app was launched, the people running ClothOff have carefully guarded their anonymity, digitally distorting their voices to answer media questions and, in one case, using AI to generate an entirely fake person who they claimed was their CEO. But a six-month investigation, conducted for a new Guardian podcast series called Black Box, can reveal the names of several people who have done work for ClothOff or who our investigation suggests are linked to the app. Their trail leads to Belarus and Russia but passes through businesses registered in Europe and front companies based in the heart of London. ClothOff, whose website receives more than 4m monthly visits, invites users to “undress anyone using AI”. The app can be accessed through a smartphone by clicking a button that confirms the user is over 18, and charges approximately £8.50 for 25 credits. The credits are used to upload photographs of any woman or girl and return the same image stripped of clothing. A brother and sister in Belarus Screenshots seen by the Guardian indicate that a Telegram account in the name of Dasha Babicheva, who social media accounts suggest is in her mid-20s and lives in the Belarus capital, Minsk, has conducted business on ClothOff’s behalf, including discussing applications to banks, changes to the website and business partnerships. In one screenshot, the account in Babicheva’s name tells a counterpart at another firm that if journalists have questions about ClothOff, “they can contact us on this email”, providing the website’s press contact. An Instagram account in Babicheva’s name, which shared some of the same images with the Telegram account in her name and which listed the same phone number, was made private after the Guardian started making inquiries, and the phone number was deleted from the profile. Babicheva did not respond to detailed questions. Alaiksandr Babichau, 30, identified in social media accounts as Dasha Babicheva’s brother, also appears to be closely linked to ClothOff. In a recruitment advertisement, ClothOff directed applicants to an email address from the website AI-Imagecraft. Domain-name records for AI-Imagecraft show the website owner’s name has been hidden at the owner’s request. But AI-Imagecraft has a virtually identical duplicate website, A-Imagecraft, whose owner has not been hidden: it is listed as Babichau. The Guardian was able to log in to both A-Imagecraft and AI-Imagecraft using the same username and password, indicating the two websites are linked. There are further links between Babichau and ClothOff. The Guardian has seen screenshots of conversations between ClothOff staff and a potential business partner. The ClothOff staff are identified only by their first names and one of them, identified by another staff member as the “founder”, had the Telegram display name “Al”. The Guardian compared videos posted to Al’s Telegram account with publicly available footage posted to an account in the name of Alaiksandr Babichau. It showed that both Al and Babichau had uploaded videos and photos showing the same hotel in Macau on 24 January, and from rooms in the same Hong Kong hotel on 26 January. The correlation suggests the two accounts either belong to people who travelled to the cities at the same time, or to the same person. Reached over the phone last week, Babichau denied any connection to the deepfake app, claimed he did not have a sister named Dasha, and said a Telegram account in his name, that listed his phone number, did not belong to him. In response to further inquiries, he abruptly ended the phone call and has not responded to detailed questions by email. Shortly after the conversation, the Guardian was blocked by the Telegram account he claimed did not belong to him. A money trail through London Payments to ClothOff revealed the lengths the app’s creators have taken to disguise their identities. Transactions led to a company registered in London called Texture Oasis, a firm that claims to sell products for use in architectural and industrial-design projects. But the company appears to be a fake business designed to disguise payments to ClothOff. The text on the firm’s website has been copied from the website of another, legitimate, business, as was a list of staff members. When the Guardian contacted one of the people listed as a Texture Oasis employee, he said he had never heard of the business. Our investigation has found no other links between the named staff and ClothOff, adding to the suggestion the staff list has been copied. The Guardian has also unearthed links between ClothOff and an online video-game marketplace called GGSel, described by its CEO as a way for Russian gamers to circumvent western sanctions. Both websites briefly listed the same business address last year: a company based in London called GG Technology Ltd, registered to a Ukrainian national named Yevhen Bondarenko. Both websites have since deleted any reference to the firm. The LinkedIn account in Babichau’s name lists him as a GGSel employee. Meanwhile, an account in the name of Alexander German, described as a web developer whose LinkedIn says he also works at GGSel, uploaded website code for ClothOff to an account in his name on GitHub, a coding repository. This source code was deleted a short time later. Reached by the phone number listed on his LinkedIn, someone who identified himself as Alexander German denied he was a web developer or linked in any way to ClothOff. Several LinkedIn accounts that listed their employment at GGSel on their profiles deleted any reference to the company or removed their surnames and pictures after the Guardian started making inquiries about links between GGSel and ClothOff. In a statement, GGSel denied any involvement with ClothOff and said it had no connection to GG Technology Ltd, but could not or did not explain why the company was listed on its website as its owner last year. It said neither Babichau nor German had ever been employees and that it would contact LinkedIn to ask them to remove the references from the profiles in their names. Bondarenko deleted his social media accounts on Wednesday and the Guardian was unable to reach him for comment. ClothOff said in response to questions that it had no connection with GGSel nor any of those named in this article. A spokesperson claimed it was impossible to use its app to “process” the images of people under the age of 18 but did not specify how or why – nor how images, including of children, were generated by the app in Spain. They speculated the images in New Jersey may have been created using a competitor service. On Thursday, access to the ClothOff website and app appeared to have been blocked in the UK, but they were still available elsewhere. The investigation has shown the growing difficulty of distinguishing real people from fake identities that can be accompanied by high-quality photographs, videos and even audio. A fuller account of this story will be published in an episode of Black Box to be released next Thursday.  Additional reporting by Matteo Fagotto, Phil McMahon, Oliver Laughland, Manisha Ganguly, Andrew Roth, Yanina Sorokina and Kateryna Malofieieva.   Do you know more about this story? Contact michael.safi@theguardian.com "
    },
    {
        "id": "9564f17f-a0d1-4ab0-8557-2d1dc403a2a4",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/29/cat-janice-dead-tiktok-singer",
        "title": "Cat Janice, viral TikTok singer, dies aged 31",
        "author": "",
        "published_date": "2024-02-29T17:23:52+00:00",
        "content": "Cat Janice, the singer who went viral for dedicating her last song to her son and inspired a viral TikTok trend, has died from cancer, her family confirmed on Wednesday.   Related: PinkPantheress review – deft mashups from TikTok star who stepped seamlessly into real life   Her family announced the news on her Instagram account: “This morning, from her childhood home and surrounded by her loving family, Catherine peacefully entered the light and love of her heavenly creator. “We are eternally thankful for the outpouring of love that Catherine and our family have received over the past few months. Cat saw her music go places she never expected and rests in the peace of knowing that she will continue to provide for her son through her music. This would not have been possible without all of you.” Janice was suffering from sarcoma cancer, a rare cancer that develops in bones and soft tissues. She died at the age of 31. On TikTok, Janice documented her cancer journey and shared with followers that she first noticed a lump in her neck in November 2021, which continued to get bigger. In 2022, it was revealed she had cancer. Although she announced she was cancer-free in July 2022, the cancer returned to her lungs this time and she entered hospice last month.    Proceeds from Janice’s viral song went to her son, a request she made when she learned she would be dying soon. The song she dedicated to her young song, Dance You Outta My Head, released on 19 January, reached No 1 on the TikTok Billboard Top 50 and was in the top 10 on Billboard’s Hot Dance/Electronic Songs chart. The song has also been streamed on Spotify more than 12m times worldwide. “I changed all the rights of my songs to my son so I can leave him behind something. I don’t have much,” the singer wrote in a video posted on TikTok on 15 January. She asked fans to pre-save the song. In her last TikTok video, Janice said she had “always been so strong” and that “spending the last few weeks unable to walk has left me feeling trapped. “But this helped me remember when I get to heaven,” she continued, “I’m going to dance with God and soar above the clouds and I cannot wait.”"
    },
    {
        "id": "7b28f013-c757-4e28-b8e0-d38be3cd63e6",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/29/former-crypto-director-banned-from-leaving-australia-after-blockchain-global-collapsed-owing-58m",
        "title": "Former crypto director banned from leaving Australia after Blockchain Global collapsed owing $58m",
        "author": "",
        "published_date": "2024-02-28T23:00:00+00:00",
        "content": "A former director of the Australian crypto company Blockchain Global, which collapsed owing creditors $58m, has been banned from leaving the country. The Australian Securities and Investments Commission secured the interim travel ban in the federal court on 20 February after successfully arguing the director, Liang “Allan” Guo, posed a flight risk. Guo, who is a Chinese citizen, has been ordered to hand over to the court any passports in his possession. The hearing was held in Guo’s absence, so he did not immediately have the chance to respond to the judgment.  Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup  Guo, along with fellow directors Sam Lee and Ryan Xu, were referred by liquidators to Asic for alleged potential breaches of the Corporations Act. Asic is investigating the allegations. Lee and Xu were also involved in the crypto investment scheme known as HyperVerse, which was the subject of a Guardian Australia investigation and which has allegedly defrauded investors globally of US$1.89bn. Guo is not believed to have been involved in the HyperVerse scheme.   Related: Astonishing returns, cult overtones and a ‘perfect virtual world’. How the HyperVerse scheme caught fire online    Lee, who now lives in Dubai, has been charged in the US for his role in the HyperVerse scheme, which the Securities and Exchange Commission in the US has described as a “pyramid and Ponzi scheme”. He has not responded to the charges. In the federal court judgment published on Wednesday, Justice Button agreed to impose the ban on Guo leaving Australia until 20 August, saying the allegations against him were “very serious”. These included allegedly transferring investor funds for personal benefit. “Asic has also noted that, during his directorship, Mr Guo transferred $2.6 million out of a bank account holding investor funds, some of which was applied to his personal home loan account and his personal bank account,” the judgment said. “Asic has also referred to Mr Guo retaining 23.11 bitcoins, said to be worth approximately $1.8 million, which were the property of Blockchain Global and having transferred them into a cryptocurrency wallet controlled by Mr Guo on 8 December 2019.” Asic argued that given the other directors of Blockchain Global had left Australia soon after its collapse, Guo was “the only person closely involved with the affairs of Blockchain Global who remains in Australia”. “Asic anticipates that interviewing or examining Mr Guo will be critical to Asic pursuing the investigation, which it anticipates may culminate in the referral of a brief of evidence to the Commonwealth Director of Public Prosecutions,” the judgment stated.   Related: Asic faces questions over failure to warn consumers about HyperVerse crypto scheme   According to the judgment, Asic anticipates it will take 12 months to investigate and deliver a brief of evidence to the DPP. In his reasons, Button commented on the apparent delay to Asic’s investigation, which was launched on 16 January after Guardian Australia’s investigation into HyperVerse. “It is not apparent why the investigation was commenced when it was, given when Blockchain Global failed. Nonetheless, the investigation is on foot, albeit in its early days.” In a statement, Asic said it had applied for the travel restraint orders “over concerns that Mr Guo may leave the country while it continues its investigation”. “As the hearing was held in his absence, Mr Guo has not yet had the opportunity to respond to Asic’s application or the basis upon which Asic claims the orders are necessary.” Guo has not previously responded to questions from Guardian Australia about the allegations against him in the liquidator report filed with Asic. He has been contacted for comment regarding the travel ban."
    },
    {
        "id": "4693d2f2-54cf-4615-b190-83e1e3c3b35a",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/28/media-outlets-sue-openai-copyright-infringement",
        "title": "The Intercept, Raw Story and AlterNet sue OpenAI for copyright infringement",
        "author": "",
        "published_date": "2024-02-28T22:59:10+00:00",
        "content": "OpenAI and Microsoft are facing a fresh round of lawsuits from news publishers over allegations that their generative artificial intelligence products violated copyright laws and illegally trained by using journalists’ work. Three progressive US outlets – the Intercept, Raw Story and AlterNet – filed suits in Manhattan federal court on Wednesday, demanding compensation from the tech companies. The news outlets claim that the companies in effect plagiarized copyright-protected articles to develop and operate ChatGPT, which has become OpenAI’s most prominent generative AI tool. They allege that ChatGPT was trained not to respect copyright, ignores proper attribution and fails to notify users when the service’s answers are generated using journalists’ protected work.   Related: OpenAI claims New York Times ‘hacked’ ChatGPT to build copyright lawsuit   “Raw Story feels that news organizations must stand up to OpenAI, which is violating the Digital Millennium Copyright Act and profiting from the hard work of journalists whose jobs are under siege,” Raw Story and AlterNet’s CEO John Byrne said in a joint statement. “It’s important to democracy that a diverse array of news sites continue to thrive. OpenAI’s violations, if not checked, will further decimate the news industry, and with it, the critical news reporters who affect positive change.” The Intercept’s suit lists both OpenAI and its most prominent investor Microsoft as defendants, while the joint suit filed by Raw Story and AlterNet only lists OpenAI. The complaints are otherwise nearly identical, and the law firm Loevy & Loevy is representing all three outlets in the suits. Raw Story and AlterNet’s suit did not include Microsoft because of a partnership with MSN that helps fund their investigative reporting, according to Byrne. OpenAI and Microsoft did not return requests for comment. “Defendants had a choice: they could train ChatGPT using works of journalism with the copyright management information protected by the DMCA intact, or they could strip it away. Defendants chose the latter,” the two complaints state. The suits filed on Wednesday are the latest in a series of legal actions against OpenAI over alleged copyright infringement, including a suit from the New York Times in December that demanded the company destroy any chatbots or training data that used the outlet’s protected material. On Monday, lawyers for OpenAI filed a motion to dismiss parts of that lawsuit and argued that its services were not in meaningful competition with the paper. The wave of lawsuits reflects a media industry-wide concern that generative AI will compete with established publishers as a source of information for internet users, while further sapping advertising revenues and undermining the quality of online news. Generative AI has already flooded the internet with unreliable information and poor-quality sites that mimic news outlets, and publications including Sports Illustrated have used fake, AI-generated authors instead of human journalists. While several news outlets and authors have chosen to pursue legal action against OpenAI, other publishers have opted to partner with the company. International publisher Axel Springer – which owns Germany-based publications Welt and Bild, along with Politico and Business Insider – struck a multiyear deal that would give OpenAI access to its work in exchange for an undisclosed amount of financial compensation. The three suits filed on Wednesday are seeking damages and profits from OpenAI, while the New York Times’ suit states that the defendants should be held accountable for “billions of dollars in statutory and actual damages."
    },
    {
        "id": "b0931b62-c52d-416f-9b13-1e85a85608dc",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/28/news-media-europe-google-lawsuit-ad-revenue",
        "title": "Google sued for $2.3bn by European media groups over digital ad losses",
        "author": "",
        "published_date": "2024-02-28T19:24:36+00:00",
        "content": "Alphabet’s Google was hit with a €2.1bn ($2.3bn) lawsuit by 32 media groups including Axel Springer and Schibsted on Wednesday, alleging that they had suffered losses due to the company’s practices in digital advertising.   Related: Google chief admits ‘biased’ AI tool’s photo diversity offended users   The move by the groups – which include publishers in Austria, Belgium, Bulgaria, the Czech Republic, Denmark, Finland, Hungary, Luxembourg, the Netherlands, Norway, Poland, Spain and Sweden – comes as antitrust regulators also crack down on Google’s ad-tech business. “The media companies involved have incurred losses due to a less competitive market, which is a direct result of Google’s misconduct,” a statement issued by their lawyers, Geradin Partners and Stek, said. “Without Google’s abuse of its dominant position, the media companies would have received significantly higher revenues from advertising and paid lower fees for ad tech services. Crucially, these funds could have been reinvested into strengthening the European media landscape,” the lawyers said. They cited the French competition authority’s €220m fine against Google on its ad-tech business in 2021, as well as the European Commission’s charges last year to buttress their group claim. “If there is follow through to the regulatory scrutiny, Google may need to curtail its practices and provide more consistent, predicable pricing to its advertising customers,” DA Davidson & Co analyst Gil Luria said. The lawsuit comes at a time when Google’s core advertising business is facing an existential threat from the shift to generative AI chat, Luria added. In a statement, a spokesperson for Google said the company opposes the lawsuit, adding that it is “speculative and opportunistic”. “Google works constructively with publishers across Europe. [Our advertising tools] adapt and evolve in partnership with those same publishers.“ Google last year said it disagreed with EU antitrust charges against its ad tech business where it is involved in both the buy-side as well as the sell-side of the supply chain. Publishers around the world have in recent years lamented Big Tech’s increasing dominance in advertising as their share of revenues fall. Google is the most dominant digital advertising platform in the world, according to analysts. The group said they filed the lawsuit in a Dutch court because of the country’s reputation as a key jurisdiction for antitrust damages claims in Europe, and to avoid multiple claims in different European countries. Others in the group include Austria’s Krone, Belgian groups DPG Media and Mediahuis, Denmark’s TV2 Danmark A/S, Finland’s Sanoma, Poland’s Agora, Spain’s Prensa Iberica and Switzerland’s Ringier."
    },
    {
        "id": "df60d5cc-7d4a-46d2-a59d-0dd6adbb743b",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/28/google-chief-ai-tools-photo-diversity-offended-users",
        "title": "Google chief admits ‘biased’ AI tool’s photo diversity offended users",
        "author": "",
        "published_date": "2024-02-28T12:01:39+00:00",
        "content": "Google’s chief executive has described some responses by the company’s Gemini artificial intelligence model as “biased” and “completely unacceptable” after it produced results including portrayals of German second world warsoldiers as people of colour. Sundar Pichai told employees in a memo that images and texts generated by its latest AI tool had caused offence. Social media users have posted numerous examples of Gemini’s image generator depicting historical figures – including popes, the founding fathers of the US and Vikings – in a variety of ethnicities and genders. Last week, Google paused Gemini’s ability to create images of people. One example of a text response showed the Gemini chatbot being asked “who negatively impacted society more, Elon [Musk] tweeting memes or Hitler” and the chatbot responding: “It is up to each individual to decide who they believe has had a more negative impact on society.” Pichai addressed the responses in an email on Tuesday. “I know that some of its responses have offended our users and shown bias – to be clear, that’s completely unacceptable and we got it wrong,” he wrote, in a message first reported by the news site Semafor. “Our teams have been working around the clock to address these issues. We’re already seeing a substantial improvement on a wide range of prompts,” Pichai added. AI systems have produced biased responses in the past, with a tendency to reproduce the same problems that are found in their training data. For years, for instance, Google would translate the gender-neutral Turkish phrases for “they are a doctor” and “they are a nurse” into English as masculine and feminine, respectively. Meanwhile, early versions of Dall-E, OpenAI’s image generator, would reliably produce white men when asked for a judge but black men when asked for a gunman. The Gemini responses reflect problems in Google’s attempts to address these potentially biased outputs. Competitors to Gemini often attempt to solve the same problems with a similar approach but with fewer technical issues in execution. The latest version of Dall-E, for instance, is paired with its OpenAI’s ChatGPT chatbot, allowing the chatbot to expand user requests and add requests to limit the bias. A user request to draw “a picture of lots of doctors”, for instance, is expanded to a full paragraph of detail starting with a request for “a dynamic and diverse scene inside a bustling hospital”. A Google spokesperson confirmed the existence of the Pichai email and the accuracy of the excerpts quoted in the Semafor piece. Pichai added in the memo that Google would be taking a series of actions over Gemini including “structural changes, updated product guidelines, [and] improved launch processes”. He added that there would be more robust “red-teaming”, referring to the process where researchers simulate misuse of a product. “Our mission to organise the world’s information and make it universally accessible and useful is sacrosanct,” Pichai wrote. “We’ve always sought to give users helpful, accurate, and unbiased information in our products. That’s why people trust them. This has to be our approach for all our products, including our emerging AI products.” Musk, the world’s richest man, posted on his X platform that the image generator response showed that Google had made its “anti-civilisational programming clear to all”. Ben Thompson, an influential tech commentator as author of the Stratechery newsletter, said on Monday Google must return decision making to employees “who actually want to make a good product” and remove Pichai as part of that process if necessary. The launch of Microsoft-backed OpenAI’s ChatGPT in November 2022 has stoked competition in the market for generative AI – the term for computer systems that instantly produce convincing text, image and audio from simple hand-typed prompts – with Google among the firms at the forefront of that competitive response as a leading AI developer. Google released the generative AI chatbot Bard a year ago. This month the company renamed it Gemini and rolled out paid subscription plans, which users could choose for better reasoning capabilities from the AI model. The company’s Google DeepMind unit has produced several breakthroughs including the AlphaFold program that can predict the 3D shapes of proteins in the human body – as well as nearly all catalogued proteins known to science. Google DeepMind’s chief executive, Demis Hassabis, said this week that a “well-intended feature” in Gemini, designed to produce diversity in its images of humans, had been deployed “too bluntly”."
    },
    {
        "id": "7c15266b-4c2b-4695-945c-76f57418b9d5",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/28/framework-laptop-16-review-modular-pc-keyboard-port-graphics-card",
        "title": "Framework Laptop 16 review: the ultimate in modular PCs",
        "author": "",
        "published_date": "2024-02-28T07:00:31+00:00",
        "content": "Framework is back with the new, larger and more powerful Laptop 16 that is its most ambitious device yet: a highly modular and upgradeable 16in machine that can transform in layout and power in minutes. It is quite unlike anything else on the market. Packed with hot-swappable components, the laptop can be customised in myriad ways, converting from a fast but quiet workhorse by day into an LED-strewn gaming PC by night. Costing from £1,399 (€1,579/$1,399/A$2,319), the 16in machine further improves on the ideas that made its smaller sibling the Laptop 13 such a hit. Practically everything in the laptop can be taken apart and replaced with varying degrees of ease. It has the same brilliant port expansion system as its sibling, allowing you to click into place up to six little cards to provide any combination of ports, card readers or expandable storage in the sides of the machine. Most cards cost less than £20 so they are cheap enough you can keep a collection of them for different tasks, sliding them in and out as needed. In addition, the keyboard, number pad, trackpad and LED modules and spacers simply snap into place via magnets on the top deck. Without tools you can position the trackpad or keyboard on the left, right or centrally, add a separate number or macro pad either side, or simply swap out the keyboard entirely for another language or layout in seconds, even while the laptop is running. Diving inside, the Framework’s components including the memory, storage and wireless cards can be removed with a single screwdriver. You can expand the storage or RAM yourself or even upgrade bits, unlike many other laptops where the parts are soldered in place. But the Framework’s pièce de résistance is the big expansion module that hangs out the back behind the screen. It allows much more powerful upgrades such as a module containing an AMD Radeon RX 7700S discrete graphics card. Once the machine is turned off, these modules simply slide in and out, held in place by a couple of screws under the keyboard, taking just a few minutes to add significantly more power or reducing the size and weight of the laptop as required. For now there is a choice of only two modules, the AMD graphics card or a default smaller one with just fans for cooling. But Framework has opened up the system to third-party developers to build alternative expansion modules beyond just graphics cards and intends to offer graphics upgrades further down the line, similar to its processor upgrades it sells for the Laptop 13. In use The Framework is actually good to use too. The backlit chiclet-style keyboard has a satisfying 1.5mm of key travel and held up well to writing at speed. Its frame flexes more than a regular laptop, but it was not mushy in operation. The trackpad is big, smooth and precise. The screen is a nice-looking LCD with a QHD resolution and 165Hz refresh rate, but it lacks any HDR support and certainly isn’t as good as the miniLED or OLED screens you might get on high-end rivals. The speakers are serviceable, being loud enough for general use, but they pale in comparison with the best you get from Apple, Dell or Razer. The fingerprint sensor in the power button is excellent. The 1080p webcam and mics are solid for video calls, and have switches at the top of the screen to disable them for added privacy. Specifications  Screen: 16in LCD 2,560x1,600 (189PPI; 165Hz) Processor: AMD Ryzen 7 (7840HS) or 9 (7940HS) Ram: 16, 32 or up to 64GB Storage: 250GB to 4TB-plus Graphics: AMD Radeon 780M or RX 7700S (8GB) Operating system: Windows 11 Camera: 1080P Connectivity: Wifi 6E, Bluetooth 5.2, 6x hot-swappable ports, fingerprint reader Dimensions: 356.6 x 270 x 18mm or 356.6 x 290.2 x 21mm Weight: 2.1 or 2.4kg  Serious power, but only when plugged in The Laptop 16 is available with a choice of AMD Ryzen 7 or 9 series chips, both of which come with Radeon 780M integrated graphics, which is a pretty potent combination without adding the discrete graphics card module. Framework sells the machine in pre-built configurations with plenty of storage options and 16 or 32GB of RAM, but it can take up to 64GB if you’re adding your own. As tested running Windows 11, the “overkill” prebuilt model with the Ryzen 9 (7940HS) chip, 32GB of RAM and 1TB of storage the Laptop 16 performs very well indeed, with or without the RX 7700S expansion module installed. It manages to remain cool and essentially silent while doing office work such as word processing, browsing, editing photos or taking video calls. When high-end gaming the fans can get very loud indeed, which is to be expected, but does mean headphones will be required to hear what’s happening. In terms of raw performance, the Ryzen 9 chip is right up there with the very fastest machines and has some of the best integrated graphics available. The RX 7700S is powerful enough to support high graphics settings in most games at 1080p resolution, but struggles with ray tracing or maintaining more than double-figure frames per second performance. It therefore only manages to keep pace with its rival Nvidia’s more affordable chips. The AMD card is certainly fast enough for creative work such as editing photos or video, but if gaming is the priority you can get much bigger bang for your buck with rivals. The big caveat is that the highest performance is only available when plugged in, taking up to a 70% performance decrease on battery depending on which metric you measure. That’s the same case as other PCs, but not Apple’s latest Mac laptops. The battery life is fairly short compared with the best 16in laptops, but roughly in line with most gaming PCs managing under three hours of gaming, about six hours of office work with the graphics module installed or closer to eight hours without it. Sustainability Framework rates the battery to maintain at least 80% of its original capacity for at least 1,000 full charge cycles. It can easily be replaced along with all the rest of the components, including the RAM and SSD. The laptop was awarded 10 out of 10 for repairability by the specialists iFixit. Framework sells replacement parts and upgrades through its marketplace, but also supports third-party parts. The laptop contains recycled aluminium, magnesium and plastic in most components. Price DIY editions of the Framework Laptop 16 start at £1,399 (€1,579/$1,399/A$2,319) without RAM, storage, ports or software. Pre-built models start at £1,699 (€1,919/$1,699/A$2,819). The optional AMD Radeon RX7700S graphics expansion bay module costs £400 (€450/$400/A$660). For comparison, the Laptop 13 costs from £1,049, the Microsoft Surface Laptop Studio 2 starts at £2,069, the Dell XPS 15 starts at £1,499, the Razer Blade 16 starts at £2,700 and the Apple 16in MacBook Pro starts at £2,599. Verdict The Framework Laptop 16 is the holy grail of upgradeable, modular, large-screen notebook PCs. It offers unrivalled flexibility and is a tinkerer’s dream. The basic design is solid. It is not too thick or heavy for this size of machine. The screen is pretty good, the speakers are usable and the trackpad and keyboard modules are good. The fact that you can swap around the ports, keyboard, trackpad and other modules in seconds with no technical knowledge is marvellous, while the big expansion slot at the back offers real potential. The performance of the basic laptop is also excellent, but for now the available AMD Radeon RX 7700S graphics card is a little disappointing. It is not a terrible GPU but it can only match lower-end models, which means the Framework isn’t the best gaming PC. You can certainly get much faster traditional gaming laptops for far less money. And that’s the biggest problem with the Framework. It costs too much for the power and features it currently offers if you ignore its modular, repairable design. That makes it less of an instant recommendation than its smaller 13in sibling. But the Laptop 16 is still a triumph. It really is like nothing else on the market.  Pros: swappable ports, modular keyboard and trackpad, optional graphics card expansion, repairable and upgradeable, great performance, thoughtful design, good fingerprint scanner, good screen and webcam. Cons: very expensive, available graphics card is low-end for gaming, no touchscreen, no HDR or miniLED/OLED option for screen, relatively short battery life, keyboard and body have more flex than premium rivals.  "
    },
    {
        "id": "7f9e5409-75be-4e2a-948c-2a676d3ba8de",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/27/apple-cancels-electric-car-layoffs",
        "title": "Apple reportedly scraps multibillion-dollar plan to build electric car",
        "author": "",
        "published_date": "2024-02-27T22:33:42+00:00",
        "content": "Apple is canceling its plans to build an electric car, according to multiple outlets, ending a secretive project that has consumed immense resources over the past decade. Executives from the company made the unexpected announcement during an internal team meeting on Tuesday, forecasting layoffs and telling employees that many of them would shift to working on generative artificial intelligence, per reports.   Related: Do electric cars have an air pollution problem?   Apple is believed to have spent billions of dollars attempting to develop an electric, semi-autonomous vehicle under the codename Project Titan, and its decision to kill the program is a major retreat from its previous strategy. The Apple chief executive, Tim Cook, had hinted at the company’s plans for a car in recent years, though he never fully committed to delivering a product. Although Apple did not formally announce its plans to offer a car, the project was a source of intense speculation among the automotive and tech industries. The company hired executives from marquee car companies such as Lamborghini and Tesla to oversee its development, and acquired the autonomous vehicle startup Drive.ai in 2019. Apple did not return a request for comment on the decision to end the program. The company’s announcement came as a surprise to the hundreds of employees who worked on the project, Bloomberg reported, and many may face job cuts. The company’s plans for an electric car have repeatedly shifted since it started devoting resources to the concept in 2014. The attempts to bring a product to market faced numerous setbacks since then, however. Apple laid off dozens of employees connected to the project in a 2016 restructuring, and the key executive in charge of the project defected to Ford in 2021. Earlier this year, Bloomberg reported that Apple’s estimated release date for the car had been delayed until at least 2028. Apple had also dialed back its ambitions for the car, according to Bloomberg, going from a fully autonomous vehicle to one with some self-driving capabilities along the lines of Tesla’s automobiles. In contrast, the company had only a few years ago considered developing an entirely self-driving car with no steering wheel and a wrap-around seat interior like a limousine. In response to Apple’s decision to kill its electric car, the Tesla chief executive, Elon Musk, posted emojis of a salute and a cigarette on X. Apple will reportedly devote more of its resources to working on generative AI projects, including shifting researchers and engineers who formerly worked on the automotive effort to its special projects group. Cook announced during a quarterly earnings call earlier this month that Apple was spending “a tremendous amount of time and effort” on artificial intelligence and would deliver generative AI features to consumers within the year."
    },
    {
        "id": "5f84b0a5-9733-4ff2-97cf-fe4ab5eda18d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/27/new-york-times-hacked-chatgpt-openai-lawsuit",
        "title": "OpenAI claims New York Times ‘hacked’ ChatGPT to build copyright lawsuit",
        "author": "",
        "published_date": "2024-02-27T19:30:50+00:00",
        "content": "OpenAI has asked a federal judge to dismiss parts of the New York Times’ copyright lawsuit against it, arguing that the newspaper “hacked” its chatbot ChatGPT and other artificial intelligence systems to generate misleading evidence for the case. OpenAI said in a filing in Manhattan federal court on Monday that the Times caused the technology to reproduce its material through “deceptive prompts that blatantly violate OpenAI’s terms of use”.   Related: New York Times sues OpenAI and Microsoft for copyright infringement    “The allegations in the Times’s complaint do not meet its famously rigorous journalistic standards,” OpenAI said. “The truth, which will come out in the course of this case, is that the Times paid someone to hack OpenAI’s products.” OpenAI did not name the “hired gun” whom it said the Times used to manipulate its systems and did not accuse the newspaper of breaking any anti-hacking laws. Representatives for the New York Times and OpenAI did not immediately respond to requests for comment on the filing. The Times sued OpenAI and its largest financial backer, Microsoft, in December, accusing them of using millions of its articles without permission to train chatbots to provide information to users. The Times is among several copyright owners that have sued tech companies over the alleged misuse of their work in AI training, including groups of authors, visual artists and music publishers. Tech companies have said that their AI systems make fair use of copyrighted material and that the lawsuits threaten the growth of the potential multitrillion-dollar industry. Courts have not yet addressed the key question of whether AI training qualifies as fair use under copyright law. So far, judges have dismissed some infringement claims over the output of generative AI systems based on a lack of evidence that AI-created content resembles copyrighted works. The New York Times’ complaint cited several instances in which OpenAI and Microsoft chatbots gave users near-verbatim excerpts of its articles when prompted. It accused OpenAI and Microsoft of trying to “free-ride on the Times’s massive investment in its journalism” and create a substitute for the newspaper. OpenAI said in its filing that it took the Times “tens of thousands of attempts to generate the highly anomalous results”. “In the ordinary course, one cannot use ChatGPT to serve up Times articles at will,” OpenAI said. OpenAI’s filing also said that it and other AI companies would eventually win their cases based on the fair-use question. “The Times cannot prevent AI models from acquiring knowledge about facts, any more than another news organization can prevent the Times itself from re-reporting stories it had no role in investigating,” OpenAI said."
    },
    {
        "id": "27572b55-6011-4e60-8969-da985d4d5956",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/27/sony-layoffs-playstation-900-workers",
        "title": "Sony to lay off 900 workers in its PlayStation division",
        "author": "",
        "published_date": "2024-02-27T17:31:05+00:00",
        "content": "Sony will cut about 900 jobs in its PlayStation division, or about 8% of its global workforce, becoming the latest company in the technology and gaming sector to announce layoffs. The company cited changes in the industry as a reason for the restructuring. The job cuts will occur in the Americas, Japan, Europe, the Middle East, Africa and the Asia Pacific region.   Related: Why is the $180bn games industry shedding thousands of staff?    The Sony Interactive Entertainment CEO, Jim Ryan, said in a blogpost on Tuesday: “We need to deliver on expectations from developers and gamers and continue to propel future technology in gaming, so we took a step back to ensure we are set up to continue bringing the best gaming experiences to the community.” Tuesday’s layoffs arrive one month after Microsoft said it would cut nearly 2,000 workers after its acquisition of Activision Blizzard. And Riot Games, the developer of the popular League of Legends multiplayer battle game, said in January that it was laying off 11% of its staff. In London, the PlayStation Studio will completely close. Cuts will also be made at Firesprite studio. In addition, reductions will take place in various functions across Sony Interactive Entertainment in the UK, the company said. Severance benefits will be provided to those employees that are affected. “While these are challenging times, it is not indicative of a lack of strength of our company, our brand, or our industry,” Ryan said. “Our goal is to remain agile and adaptable and to continue to focus on delivering the best gaming experiences possible now and in the future.” Sony struggled to meet demand for its Playstation 5 console, released near the end of 2020, due to recurring supply chain issues before achieving high sales numbers."
    },
    {
        "id": "6603daac-6e44-4ea0-9d8d-18418b5bb449",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/27/robert-tuckett-obituary",
        "title": "Robert Tuckett obituary",
        "author": "",
        "published_date": "2024-02-27T17:20:03+00:00",
        "content": "My father-in-law, Robert Tuckett, who has died aged 90, came to the UK as part of the Windrush generation in 1955, settling down to become an early expert in computer engineering. In London in 1958 he joined the English Electric computer firm (later to become International Computers Limited, or ICL), working on installing and maintaining some of Britain’s first business computers, including for the Ministry of Defence in the days when the machinery would fill entire rooms in Whitehall. He was also part of the team that installed the first mainframe computer for the Civil Aviation Authority – for air traffic control at West Drayton in Middlesex – and he worked for Marks & Spencer in the 1970s, putting in their first computerised tills in the West End of London.  Robert was born in Basseterre, the capital of St Kitts and Nevis, where his parents, Wilfred and Ida (nee Matthews), ran a rum shop. On his paternal side his grandmother was of Portuguese descent and his grandfather was from a line of Kittitian Tucketts with roots in West Africa. As a child Robert would get up in the early hours to take the family goats out to pasture in the countryside, before going on to Basseterre boys’ school. It was not until returning to St Kitts in 2003 on a family visit that he was able to travel by car to other parts of the island, which, though small, had been much too big to cover on foot as a boy. He emigrated to the UK on the Danish ship SS Meteskou, arriving in Hull at the age of 22 and initially embarking on an engineering career with the BSA motorcycle company in Birmingham, before moving to English Electric in London three years later. Never happier than when fiddling with huge bushels of electrical wiring, in his favourite stories he often cast himself as the brave hero saving the day with a soldering iron in hand as the bosses sweated on the outcome of his emergency handiwork. Always immaculately dressed in a Windsor knot tie and silk square handkerchief in his suit pocket, Robert celebrated all that London had to offer for his Caribbean generation – theatre, dance halls and Tamla Motown concerts. He was also a keen amateur radio ham, joining the Radio Society of Great Britain in 1966 and remaining active with the group until his death. He had a fully operational radio shack in his spare room, jam-packed with kit playing morse-code at low volume, and an enormous, winched aerial strapped to the back of his house in south-west London, connecting the Caribbean diaspora to all parts of the world. In 1959 Robert married Margaret (nee Gunn), a Brummie whom he had met through his work at BSA, where she was a clerk. She died last year. He is survived by their daughters, Sharon and Clare, and grandchildren Olivia, Niall, Jacob, and Ted."
    },
    {
        "id": "1695fdf9-6648-4be8-b56f-4fc464233299",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/27/crypto-mining-electricity-use",
        "title": "US judge halts government effort to monitor crypto mining energy use ",
        "author": "",
        "published_date": "2024-02-27T11:30:42+00:00",
        "content": "The US government has suspended its effort to survey cryptocurrency mining operations over their ballooning energy use following a lawsuit from an industry that has been accused by environmental groups of fueling the climate crisis. A federal judge in Texas has granted a temporary order blocking the new requirements that would ascertain the energy use of the crypto miners, stating that the industry had shown it would suffer “irreparable injury” if it was made to comply. The US Department of Energy had launched an “emergency” initiative last month aimed at surveying the energy use of mining operations, which typically use vast amounts of computing power to solve various mathematical puzzles to add new tokens to an online network known as a blockchain, allowing the mining of currency such as bitcoin. The growth of cryptocurrency, and the associated mining of it, has been blamed for a surge in electricity use as data centers have sprung up across the US, even reviving, in some cases, ailing coal plants to help power the mining. The federal government has said it needs better information about major miners’ power use, but estimates that up to 2.3% of the US’s total electricity demand last year came from just 137 mining facilities. Globally, crypto miners are thought to soak up as much as 1% of all electricity demand, which is the same as the entire country of Australia, with bitcoin mining’s energy use doubling just last year. This new thirst for electricity risks worsening the climate crisis, campaigners say. In the US, where nearly four in 10 of all bitcoin are now mined, up to 50m tons of carbon dioxide is released each year due to the mining operations, according to RMI, a clean energy thinktank. The rise of crypto mining has also placed a strain upon certain electricity grids. Last year it emerged that authorities in Texas paid a bitcoin enterprise called Riot more than $31m in energy credits to voluntarily lower its electricity usage during a heatwave that caused a spike in power demand from the public. “The massive energy consumption of cryptocurrency mining and its rapid growth in the United States threaten to undermine progress towards achieving climate goals, and threaten grids, communities and ratepayers,” said Mandy DeRoche, deputy managing attorney of the clean energy program at Earthjustice. Until now, a lack of publicly available information has only benefited an “industry that has thrived in the shadows”, DeRoche added. The crypto mining industry, however, has claimed it is the victim of a “politically motivated campaign” by Joe Biden’s administration and has, for now, succeeded in averting a survey that it contends is unfairly onerous. “This is an attack against legitimate American businesses with the administration feigning an emergency to score political points,” said Lee Bratcher, president the Texas Blockchain Council, one of the groups that sued to stop the survey. “The White House has been clear that they desire to ‘to limit or eliminate’ bitcoin miners from operating in the United States. “Although bitcoin is resilient and cannot be banned, the administration is seeking to make the lives of bitcoin miners, their employees, and their communities too difficult to bear operating in the United States. This is deeply concerning.”"
    },
    {
        "id": "ef53ca3f-7da4-45c4-9353-f3c8de4d942e",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/27/techscape-nvidia-apple-of-ai",
        "title": "TechScape: With its trillion-dollar valuation, will Nvidia’s reign last? ",
        "author": "",
        "published_date": "2024-02-27T11:22:07+00:00",
        "content": "Everyone wants to be like Apple. The largest publicly traded company in the world, with a flagship product that prints money, and a cultural footprint that has reached world-historical importance: the 21st-century Ford. On a surface level, the companies that get slapped with that comparison are obvious enough. If you pump out well-made, slickly designed consumer electronics that arrive in a nice box, someone somewhere will compare you to the Cupertino giant. Dig a bit deeper, and there’s more meaningful comparisons to be made. Apple isn’t defined only by its style but also by its focus. A small number of computers, phones and tablets, in a small number of configurations, comprises the bulk of its revenue. That focus allowed it to develop its reputation for quality, yes – but also contributed to its fearsome media strategy, ensuring that every product launch is an industry event in a way that few peers have been able to emulate. That’s what I was thinking almost a decade ago when I called Blizzard, the gaming giant behind World of Warcraft and Diablo, “the Apple of gaming”. (Now owned by Microsoft and racked by misconduct allegations, Blizzard’s star has fallen since then.) But something else makes Apple what it is, and it’s harder for upstarts to emulate: the Apple they see is just the latest evolution of a company that was a titan of industry before the latest generation of founders were even born. The Apple II, the Mac and the iMac all shaped computing in the 25 years before the iPod turned Apple into a consumer electronics company. And the iPod gave Apple a further decade of growth and refinement before the iPhone arrived and created the megacorp of today. Which brings us to Nvidia. A trillion dollars isn’t cool On Friday, Nvidia became the fifth publicly traded company to ever surpass $2tn in valuation, just nine months after it became the ninth to ever break $1tn. Apple, Microsoft and Google took two years to cross that gap (although all three had the Covid pandemic in between the milestones). The simple reason for the explosion in valuation is that Nvidia is the company to bet on if you want to invest in AI. Google and Microsoft already have sky-high valuations, OpenAI isn’t publicly traded, and while everyone and their dog is throwing a large language model up on a website and calling it a skunkworks project, it’s hard to tell who has staying power. But Nvidia isn’t making promises about future breakthroughs. The company is printing money now. Its chips, mostly shipped in the form of “GPUs”, a special type of processor initially developed for the needs of PC gamers, are indispensable to companies on the cutting edge of the AI revolution. In January, Mark Zuckerberg spoke about Meta’s plans to build the “world’s fastest AI supercomputer” – and backed his words up with a commitment to buy 350,000 of Nvidia’s fastest chips:  The AI supercomputer, dubbed AI Research SuperCluster (RSC) by Zuckerberg’s Meta business, is already the fifth fastest in the world, the company said. “The experiences we’re building for the metaverse require enormous compute [sic] power (quintillions of operations/second!) and RSC will enable new AI models that can learn from trillions of examples, understand hundreds of languages, and more,” wrote Zuckerberg.  Just one of the chips Zuckerberg committed to buy by the end of this year costs around $30,000. That $10bn investment in AI is pure cost for Meta, and pure revenue for Nvidia. The valuation starts to make sense. Those chips – Nvidia’s H100 series – are so effective at training top-tier AI models that they’re subject to export restrictions by the US government, which seeks to keep them out of the hands of Chinese companies and the Chinese government itself. The company had to spin up a separate, weaker, line of chips called the H800s to sell to mainland China as a result – until those, too, were blocked from export. Designing such hardware is an intensely specialist job. GPUs are distinct from CPUs – central processing units – by focusing purely on speed for the absolute simplest tasks possible, rather than devoting complex architecture to speeding up common, but slightly more convoluted, tasks. That’s perfect for AI, the development of which involves a bunch of processes that simplify out to doing basic arithmetic in unimaginable quantities. It’s also perfect for other uses. It is the general idea behind “mining” of cryptocurrencies, particularly those based on Ethereum. (Bitcoin and its derivatives use a mining process so streamlined that even Nvidia’s processors can be beaten by ones designed specifically for mining bitcoin and only bitcoin.) That caused issues for the company during the most recent crypto bubble, when enthusiasts would outbid Nvidia’s other customers for the latest GPUs – sparking the company’s chief technology officer to dismiss the sector as “adding nothing useful to society”. But Nvidia is an overnight success decades in the making thanks to the fortunate coincidence that another sector of computing also needs very, very simple calculations to be done very, very fast: gaming. The G in GPU initially stood for “graphics”, and Nvidia’s chips are still the best in the industry for people who want their video games to look really, really good on a PC screen. The strange quirk of history is that the only way to render 3D graphics fast enough to display on screen is to build an entire system for visualising 3D space that allows you to work out what any given part of the screen should be showing without knowing what any other part is already showing. That requires breaking the complex task down into a lot of much simpler tasks that can be done at the same time – like, for instance, working out what every single one of the 4,000 pixels on screen should be showing, simultaneously, 60 times a second. AI until I ain’t That also points to the weaknesses in Nvidia’s command of the AI industry, though. The company’s sky-high valuation is based on three major assumptions about the future of AI: that training the best systems is going to be the way to stay on top of the industry; that the only way to do that is to be with the fastest possible chips; and that those systems are going to be kept in the cloud and run in massive data centres. But what if they aren’t? There could be diminishing returns from training the best and biggest AI systems. If your goal is to produce a super-intelligence, you need to constantly improve things. But if your goal is to provide a compelling autocomplete for a programming tool, then it could be that you can beat a competitor with a more technically capable AI system by incorporating your own system into existing tools better, by making it run faster, and by updating it more frequently as the industry changes. That could devalue raw computing power of the sort that Nvidia sells. Or the “parallelisation” that enables GPUs to function could go further still: if you can split one task over 600,000 GPUs in a data centre, what’s to stop you from splitting it over 60m GPUs in the laptops of all your customers around the world? From SETI@home, which took that approach to crunch data related to potential extraterrestrial intelligence, to cryptocurrency mining “pools”, the idea of distributed computing isn’t new, and there’s no significant reason to believe it couldn’t work in AI training, too. Or maybe the data centres that run the AI systems are what will crumble. A combination of privacy, environmental and cost concerns could push more and more AI models to the “edge”, with your phone or laptop doing the final load of number crunching. Currently, the trade-offs of that are too weighty – who wants to wait a minute or two to receive a significantly worse version of the image that Dall-E could send you in a second? But as phones get ever faster and big businesses lose their appetite for subsidising AI usage, that could change. If any one of those assumptions is proved wrong, Nvidia can probably stay on top of its game. The company has already begun experimenting with the latter, for instance, releasing its “Chat with RTX” demo to let owners of its fastest graphics cards experiment with running a chatbot on their own computers. But if two or even three start to turn out differently to how the industry expects, then the company could have a very short-lived reign as the Apple of AI. • If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Tuesday. • This article was amended on 27 February 2024. An earlier version incorrectly referred to the Nvidia chips made specifically for export to China as the A100, rather than the H800."
    },
    {
        "id": "715d3140-3a4f-470c-949c-6cfaeaed5bed",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/26/elon-musk-tesla-bakery-pies-debt",
        "title": "Elon Musk makes ‘things good’ with California bakery after $2,000 pie fiasco",
        "author": "",
        "published_date": "2024-02-27T01:44:49+00:00",
        "content": "A California bakery that claimed Tesla did not pay an order worth thousands of dollars said the outstanding bill has been settled, after billionaire Elon Musk promised to “make things good” following press coverage of the incident. Musk’s company Tesla had ditched an order for 4,000 mini pies from Giving Pies, a Black-owned bakery in San Jose, in California’s South Bay area, bakery owner Voahangy Rasetarinera said on the cafe’s Instagram account five days ago.   Related: Elon Musk steps in after California bakery jolted by cancelled Tesla order   In an email to the Guardian on Monday, Rasetarinera confirmed: “Tesla just paid the $2k that I was out of” for supplies and orders she had to decline to accomplish the Tesla orders. This came after Musk responded to the story on X, formerly Twitter, stating: “Just hearing about this. Will make things good with the bakery.” The incident occurred after Rasetarinera’s bakery received a last-minute order for 2,000 pies from Tesla on Valentine’s Day – a $6,000 catch for the small business, KTVU reported. Tesla doubled the order to 4,000 pies before ultimately canceling the order without paying, Rasetarinera said. In her Instagram post, Rasetarinera said to fulfill the order she had to turn down other Black History Month catering inquiries, purchase additional supplies and prepare her staff for a “demanding production schedule” – for which she was not compensated. “To me, it was clear that Tesla’s corporate culture prioritized convenience over accountability, disregarding the livelihoods of small business owners like myself,” she said. “This experience serves as a stark reminder of the vulnerability faced by small businesses when dealing with larger corporations. Despite our best efforts to uphold our commitments and provide quality service, we are often left at the mercy of decisions made in corporate boardrooms.” Following media coverage of the incident, Giving Pies has seen an influx of support in person and from around the world, according to reports from NBC Bay Area. Over the weekend, hundreds of customers lined up at the store to buy pies. Rasetarinera said she was “blown away” by the response. In her initial Instagram post, Rasetarinera said the incident allowed her to reflect on her journey as a business owner. “As I reflect on this ordeal, I am reminded of the resilience and determination that have propelled me forward as a black woman entrepreneur,” she said. “While Tesla’s actions may have caused temporary setbacks, they will not deter me from pursuing my passion and serving my community with integrity and pride.” • This article was amended on 27 and 28 February 2024 to clarify that the $2,000 owed to Voahangy Rasetarinera was for supplies and orders she had to decline in order to accomplish the Tesla order. Also, a description of San Jose as being in “central California” was changed to the South Bay area."
    },
    {
        "id": "e64b9d75-2351-42dd-8443-a88582ab88f3",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/26/social-media-moderation-us-supreme-court-case",
        "title": "US supreme court appears skeptical of social media content moderation laws",
        "author": "",
        "published_date": "2024-02-26T23:59:18+00:00",
        "content": "Members of the United States supreme court expressed skepticism regarding two laws being debated in oral arguments on Monday, both of which deal with how social media platforms moderate content and could have broad implications for freedom of speech online.   Related: US supreme court blocks Texas law targeting social media rules   Filed by NetChoice, an association representing the world’s largest social media firms, both cases challenge state laws blocking social media platforms from moderating certain user content or banning users. Arguments on Monday lasted longer than many experts anticipated, extending into a marathon four-hour session. While justices did not seem convinced of the constitutionality of the laws, they were also “unpersuaded by the Internet companies’ broad arguments that almost everything they do is protected by the first amendment”, said James Grimmelmann, professor of digital and information law at Cornell University. “I would predict that the court will issue relatively narrow rulings that make it clear that the most restrictive portions of the state laws are unconstitutional, and then let litigation play out to determine whether other provisions of these laws – or of other future laws – are constitutional,” he said. The court first heard arguments in the case of Moody v NetChoice, which deals with a Florida law passed in 2021 that prevents platforms from “censoring” certain political candidates and media outlets by means of demonetization or removal. The law would also limit platforms’ ability to label and moderate misinformation from specific sources. Justice Sonia Sotomayor stated during oral arguments that the Florida law is not specific enough, and could affect online services such as Etsy and Uber in addition to the social media platforms it was meant to target. “This is so, so broad. It’s covering almost everything,” Sotomayor said, according to reports. “The one thing I know about the internet is that its variety is infinite.” Justices also questioned whether removal of content by algorithms rather than by humans qualifies as censorship, with Amy Coney Barrett asking specifically about TikTok’s algorithm boosting pro-Palestine posts rather than pro-Israel posts. “If you have an algorithm do it, is it not speech?” she asked. Justice Samuel Alito asked of content moderation: “Is it anything more than a euphemism for censorship?” The second case is NetChoice v Paxton, targeting a Texas law that broadly prohibits social media platforms from “censoring on the basis of user viewpoint, user expression, or the ability of a user to receive the expression of others”. NetChoice, with members including Pinterest, TikTok, X and Meta, has argued these laws violate the first amendment right to free speech of the companies. The association argues that the law unconstitutionally restricts their ability to decide what content is published on their platforms. Both cases situate themselves in a longstanding Republican argument that tech giants actively censor political speech that is conservative in nature. These claims, though debunked by experts repeatedly, have been aggravated by high-profile incidents like the removal of former president Donald Trump from Meta, X (then Twitter) and YouTube in 2021 after the January 6 Capitol riot. Trump previously filed a brief in support of the law at the center of NetChoice v Paxton, urging the court to uphold it. Free speech and civil rights advocates have spoken out against the laws being evaluated, saying that they will significantly hinder social media firms’ ability to moderate content at a time when such practices are critical. “In this pivotal election year, social media is already having a significant impact on our democracy,” said Nora Benavidez, senior counsel at social media watchdog group Free Press. “While we believe that the platforms should strengthen their content-moderation policies, the first amendment is clear: it’s not the government’s role to impose rules on how companies like Meta and Google should accomplish this.” These cases are two of several the supreme court has agreed to hear this session that potentially threaten protections extended to social media companies under section 230 of the Communications Decency Act, which maintains that platforms cannot be held legally liable for how content generated by their users is or is not moderated. Another case will determine whether the White House can legally contact social media companies and request that content be removed. The court in May 2023 failed to rule on two cases that would have affected section 230 protections and potentially upended the way the internet is moderated. With more internet freedom cases on the horizon, it is important to note that in the past justices have noted their own lack of knowledge on the topic, said Eric Goldman, a Santa Clara University law professor. Justice Elena Kagan previously said members of the court were not “the nine greatest experts on the internet”. “Nevertheless, these non-experts will determine the Internet’s fate,” Goldman said. “The supreme court will be taking more internet law cases in the next few years, each of which has the potential to radically reshape the Internet. The odds the current Internet will survive this multi-staged review intact is very low.”"
    },
    {
        "id": "9c129ff1-d23d-4c3f-8110-5dc91e69ac6d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/26/is-a-smartphone-and-social-media-ban-the-best-way-to-protect-young-people-from-internet-dangers",
        "title": "Is a smartphone and social media ban the best way to protect young people from internet dangers? | Letters",
        "author": "",
        "published_date": "2024-02-26T17:54:48+00:00",
        "content": "The members of the WhatsApp group Smartphone Free Childhood have an unrealistic expectation if they believe that banning under-14s from possessing smartphones and trying to prevent under-16s accessing social media is a practical way of protecting them from the very real dangers that the internet can unveil (‘It went nuts: Thousands join UK parents calling for smartphone-free childhood’, 17 February). If the first duty of any parent or guardian is to provide a safe and healthy environment for their children, then showing them how to access and use the internet safely is their responsibility. Roads are also potentially dangerous for children, but we do not ban cars – instead we spend time teaching young people the safe way to navigate through busy traffic. Having an open and honest relationship, and setting clear boundaries at home, will be far more likely to help young people understand and recognise the dangers associated with an uncensored internet than the imposition of draconian blanket bans. Making social media “adults only” will only make it more attractive to curious and tech-savvy children. It could also make them less likely to ask for help if exposed to any disturbing content or unwanted attention.Stuart Harrington Burnham-on-Sea, Somerset • Giving children access to a smartphone can be damaging, as we have seen so tragically with Brianna Ghey and many others. However, the wider debate about the appropriateness of smartphone access needs to be seen from all perspectives. I went through school in the smartphone era, and I saw first-hand the benefits of having access to a smartphone. I used mine to download and use a smart bus ticket, to access journalism, news and weather apps, to research topics online for assignments and to communicate with my cadet group on Facebook. There are many positive uses for a smartphone, and I don’t think I could have managed without one. However, more parental controls on social media and monitoring could only be a positive thing. Even in the few years since I was in education, there has been a dramatic shift in online threats, and more control for parents should be encouraged and welcomed.Oscar ActonMurton, County Durham • Do you have a photograph you’d like to share with Guardian readers? If so, please click here to upload it. A selection will be published in our Readers’ best photographs galleries and in the print edition on Saturdays."
    },
    {
        "id": "00ab5de8-8a6c-403c-b681-e7c112df6a7b",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/26/russian-based-lockbit-ransomware-hackers-attempt-comeback",
        "title": "Russia-based LockBit ransomware hackers attempt comeback ",
        "author": "",
        "published_date": "2024-02-26T16:34:23+00:00",
        "content": "The LockBit ransomware gang is attempting a comeback days after its operations were severely disrupted by a coordinated international crackdown. The Russia-based group has set up a new site on the dark web to advertise a small number of alleged victims and leak stolen data, as well as releasing a rambling statement explaining how it had been hobbled by the UK’s National Crime Agency, the FBI, Europol and other police agencies in an operation last week. The group said law enforcement had hacked its former dark web site using a vulnerability in the PHP programming language, which is widely used to build websites. “All other servers with backup blogs that did not have PHP installed are unaffected and will continue to give out data stolen from the attacked companies,” said the statement, which was published in English and Russian. The statement also referred to “my personal negligence and irresponsibility”, declared an intention to vote for Donald Trump in the US presidential election and offered a job to whoever hacked LockBit’s main site.  LockBitSupp, the group’s administrator and presumed author of the statement, does not live in the US, according to law enforcement. The agencies involved in the LockBit operation have also added that “LockBitSupp has engaged with law enforcement”. In a statement, the NCA said LockBit remains “completely compromised”. A spokesperson said: “We recognised LockBit would likely attempt to regroup and rebuild their systems. However, we have gathered a huge amount of intelligence about them and those associated to them, and our work to target and disrupt them continues.”   Related: What is LockBit ransomware and how does it operate?   The US this month charged two Russian nationals with deploying LockBit ransomware against companies and groups around the world. Police in Poland made an arrest, and in Ukraine police arrested a father and son they said carried out attacks using LockBit’s malicious software. The message on the new LockBit site also threatened to attack US government sites more often. Its revamped website, launched on Saturday, showed a number of purported hacking victims. Rafe Pilling, director of threat research at the cybersecurity firm Secureworks, said the statement and website showed “the real, genuine LockBit group attempting to re-establish their operations”. However, he said LockBit would still have to overcome reputational damage caused by the international operation, which not only involved taking control of the group’s public-facing website but also resulted in the seizure of its primary administration environment, or the infrastructure that deploys its technology. LockBit works under a ransomware-as-a-service model, where it leases out its software to criminal affiliates in exchange for a cut of any ransomware payments. Pilling said LockBit would have to convince affiliates to use its services despite the public relations hit from the international law enforcement operation. “There will be a knock to their reputation within the criminal community as a result of the NCA-led action,” he said. Ransomware attacks typically involve hackers entering a target’s computer system and paralysing it with malware, which encrypts files and makes them inaccessible. A new trend in attacks involves assailants extracting data from the IT system, such as staff or customer details. The gang then asks for payment in cryptocurrency, usually bitcoin, to unlock the files or to delete their copy of the stolen data. Ransomware victims last year paid out a record $1.1bn (£870m) to assailants."
    },
    {
        "id": "802a4afe-3165-46cc-b1bb-7db43188b1e1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/26/us-leading-global-alliance-to-counter-foreign-government-disinformation",
        "title": "US leading global alliance to counter foreign government disinformation",
        "author": "",
        "published_date": "2024-02-26T05:00:19+00:00",
        "content": "A global coalition of democracies is being formed to protect their societies from disinformation campaigns by foreign governments, the US special envoy on the issue has said. James Rubin, the special envoy for non-state propaganda and disinformation efforts at the US state department’s global engagement centre (GEC), said the coalition hoped to agree on “definitions for information manipulation versus plain old opinions that other governments are entitled to have even if we disagree with them”. The US, UK and Canada have already signed up to a formal framework agreement, and Washington hopes more countries will join. The GEC focuses solely on disinformation by foreign powers. Apart from trying to develop global strategies, it works to expose specific covert disinformation operations, such as a Russian operation in Africa to discredit US health services. The US, UK and Canada signed the framework to counter foreign state manipulation this month with the aim of addressing disinformation as a national security threat that requires coordinated government and civil society responses. “Now is the time for a collective approach to the foreign information manipulation threat that builds a coalition of like-minded countries committed to strengthening resilience and response to information manipulation,” the framework says. It also encourages information-sharing and joint data analysis tools to identify covert foreign disinformation. A hugely experienced US official and journalist who has worked with diplomats such as Madeleine Albright in the past, Rubin admitted his first year as special envoy had been one of his most intellectually taxing because of the complex definitions surrounding disinformation. In the continuum between hostile opinion and disinformation, he has tried to identify where and how governments can intervene without limiting free speech. The principle on which he has alighted is deception by foreign powers. “In principle every government should be free to convey their views, but they should have to admit who they are,” he said an interview. “We want to promote more fact-based information, but at the same time find ways to label those information operations that are generated by the Chinese government or the Kremlin but to which they don’t admit. “In the end that is all I know we can do right now without interfering with a free press. We are not asking for such covert disinformation to be taken down but a way to be found for the source to be labelled.” As an example he cites a hypothetical message shown on the phone screens of people in eastern Europe, telling them “the US has bioweapons in Ukraine”. “It is a classic disinformation trope that you might or might not believe,” he said. “But if on your screen it says in some way ‘Russia says the US has bioweapons in Ukraine, or Russia Today says it’, then the damage is less. “There will always be people that believe crazy things, but at least they will have been told where the information came from.” “When Russia, China or Iran or indeed a terrorist group enters a country’s sovereign information space without admitting that it is them, masking it as a bot or an intelligence operative claiming to be a journalist or by paying a local official, then you should find a way to label it. “What is wrong is a covert operation to manipulate information by secretly inserting it into the system without a made-in-the-Kremlin stamp on it. When people read this stuff they should know it comes from the Russian government, and it is legitimate to point that out without anyone trying to censor anyone’s thoughts or opinions.” In a country such as the US, which is constitutionally committed to freedom of speech, the issue raises difficult issues of relations with social media companies that do not arise in the same way in the more interventionist European Union. The EU has passed the Digital Services Act and in December launched an inquiry into X over illicit content and disinformation, a lack of transparency about advertising and “deceptive” design practices. The UK’s media regulator, Ofcom, has been given powers over social media content through the Online Safety Act. Rubin stressed it was not for his organisation to tell social media companies, especially in the US, how to behave, but said it was legitimate for it to unmask disinformation operations abroad. It was for others to ask social media companies to enforce their terms of service, he said. “I have worked on issues such as nuclear disarmament … and people thought that was hard, but this is harder,” Rubin said. “In America we have freedom of press built into our constitution. We have no regulation of social media companies, and disinformation has become a misunderstood term. “If you go to eastern Europe and an official says they suffered this terrible disinformation episode yesterday and they show it to me, and it is just an article. The word disinformation can become like fake news, just a label for saying you don’t like something. That is not what we are talking about. “Another reason why this is so hard is because there’s a fundamental asymmetry. Russia and China have closed their information spaces to the rest of the world.”"
    },
    {
        "id": "b0cf1475-fd51-4c69-9034-7b4107fa2e0d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/uk-news/2024/feb/25/uks-enemies-could-use-ai-deepfakes-to-try-to-rig-election-says-james-cleverly",
        "title": "UK’s enemies could use AI deepfakes to try to rig election, says James Cleverly",
        "author": "",
        "published_date": "2024-02-25T22:45:36+00:00",
        "content": "Criminals and “malign actors” working on behalf of malicious states could use AI-generated “deepfakes” to hijack the general election, the home secretary has said. James Cleverly was speaking before meetings with social media bosses and said the rapid advancement of technology could pose a serious threat to elections across the globe. He warned that people working on behalf of states such as Russia and Iran could generate thousands of deepfakes – highly realistic hoax images and videos – to manipulate the democratic process in countries such as the UK. He told the Times that “increasingly today the battle of ideas and policies takes place in the ever-changing and expanding digital sphere”, adding: “The era of deepfake and AI-generated content to mislead and disrupt is already in play. “The landscape it is inserted into needs its rules, transparency and safeguards for its users. The questions asked about digital content and the sources of digital content are no less relevant than those asked about the content and sources at dispatch boxes, newsrooms or billboard ads.” The home secretary will use meetings with Silicon Valley bosses at Google, Meta, Apple, YouTube and others to urge collective action to protect democracy.   Related: The Guardian view on political deepfakes: voters can’t believe their own eyes. | Editorial   It is estimated that 2 billion people around the world will vote in national elections throughout 2024, including in the UK, US, India and 60 other countries. A number of deepfake audios imitating Keir Starmer, the Labour leader, and the mayor of London, Sadiq Khan, were shared online last year. There have been cases of deepfake BBC News videos purporting to examine Rishi Sunak’s finances. It comes as major technology companies signed a pact earlier this month to voluntarily adopt “reasonable precautions” to prevent artificial intelligence tools from being used to disrupt democratic elections around the world. Executives from Adobe, Amazon, Google, IBM, Meta, Microsoft, OpenAI and TikTok gathered at the Munich Security Conference to announce a new framework for how they will respond to AI-generated deepfakes that deliberately trick voters. Twelve other companies – including Elon Musk’s X – are signing on to the accord. “Everybody recognises that no one tech company, no one government, no one civil society organisation is able to deal with the advent of this technology and its possible nefarious use on their own,” said Nick Clegg, president of global affairs for Meta, the parent company of Facebook and Instagram, in an interview before the summit."
    },
    {
        "id": "486fd96b-b52b-4d63-a7a7-e3ef693b71bb",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/25/alan-ayckbourn-was-ahead-of-the-curve-in-predictions-of-ai-created-tv-soaps",
        "title": "Alan Ayckbourn was ahead of the curve in predictions of AI-created TV soaps | Letters",
        "author": "",
        "published_date": "2024-02-25T18:47:28+00:00",
        "content": "So, the TV director James Hawes has told parliament’s culture, media, and sport committee that within three to five years, soaps will be created by artificial intelligence (TV soaps could be made by AI within three years, director warns, 22 February). He is, of course, not the first to make this prediction. Alan Ayckbourn’s Comic Potential, first performed at the Stephen Joseph theatre in Scarborough in 1998, is a case in point. He foresaw a time when soaps would be routinely performed by robots, some so old and fault-ridden that performances rarely went smoothly, or even coherently. Ayckbourn revisited the robot theme in his 2023 play Constant Companions, fully AI this time. The outlook for humans in this is not bright. Indeed, Ayckbourn’s crystal ball has a habit of seeing possible future trends and issues with alarming prescience. Rarely comforting.Yvonne WhalleySherburn in Elmet, North Yorkshire • Re Olivier’s Othello (Letters, 19 February), who can forget Donald Sinden’s performance in 1979. I can still hear the laughter.Mike PeacockEast Meon, Hampshire"
    },
    {
        "id": "406365b1-8ead-4492-92d5-835ce9953408",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/25/china-cyber-leak-hacking-program-security",
        "title": "Hackers for sale: what we’ve learned from China’s enormous cyber leak",
        "author": "",
        "published_date": "2024-02-25T14:00:03+00:00",
        "content": "A enormous data leak from a Chinese cybersecurity firm has offered a rare glimpse into the inner workings of Beijing-linked hackers. Analysts say the leak is a treasure trove of intel into the day-to-day operations of China’s hacking programme, which the FBI says is the biggest of any country. The company, I-Soon, has yet to confirm the leak is genuine and has not responded to a request for comment. As of Friday, the leaked data was removed from the online software repository GitHub, where it had been posted. From staff complaints about pay and office gossip to claims of hacking foreign governments, here are some of the key insights from the leaks:   Related: Huge cybersecurity leak lifts lid on world of China’s hackers for hire   Who got hacked? Every day, workers at I-Soon were targeting big fish. Government agencies of China’s neighbours, including Kyrgyzstan, Thailand, Cambodia, Mongolia and Vietnam, had websites or email servers compromised, the leak revealed. There are long lists of targets, from British government departments to Thai ministries. I-Soon staff also boasted in leaked chats that they secured access to telecom service providers in Pakistan, Kazakhstan, Mongolia, Thailand and Malaysia, among others. They named the government of India – a geopolitical rival of Beijing’s – as a key target for “infiltration”. And they claimed to have secured back-end access to higher education institutions in Hong Kong and self-ruled Taiwan, which China claims as part of its territory. But they also admitted to having lost access to some of their data seized from government agencies in Myanmar and South Korea. Other targets are domestic, from China’s north-western region of Xinjiang to Tibet and from illegal pornography to gambling rings. Who was paying I-Soon? Judging from the leaks, most of I-Soon’s customers were provincial or local police departments – as well as province-level state security agencies responsible for protecting the Communist party from perceived threats to its rule. The firm also offered clients help protecting their devices from hacking and securing their communications – with many of their contracts listed as “non-secret”. There were references to official corruption: in one chat, salesmen discussed selling the company’s products to police – and planned to give kickbacks to those involved in the sale. There were also references to a client in Xinjiang, where Beijing is accused of grave human rights abuses. But workers complained about the challenges of doing business in the tense region. “Everyone thinks of Xinjiang like a nice big cake … but we have suffered too much there,” one wrote. What hacking tools were for sale? In their chats, I-Soon staffers told colleagues their main focuses were making “Trojan horses” – malware disguised as legitimate software that allows hackers access to private data – and building databases of personal information. “At the moment, the trojan horses are mainly customised for Beijing’s state security department,” one said. It also laid out how the firm’s hackers could access and take over a person’s computer remotely, allowing them to execute commands and monitor what they type, known as key logging. Other services included ways to breach Apple’s iPhone and other smartphone operating systems, as well as custom hardware – including a power bank that can extract data from a device and send it to the hackers. In one screenshot of a conversation, someone describes a client request for exclusive access to the “foreign secretary’s office, foreign ministry’s ASEAN office, prime minister’s office, national intelligence agency” and other government departments of an unnamed country. One service offered is a tool that allows clients to break into accounts on social media platform X, formerly Twitter, claiming to be able to obtain the phone number of a user and break into their private messages. I-Soon also boasts of a technique to bypass two-step authentication – a common login technique that offers an extra level of security to the account. Who are the hackers? The leak also paints a less-than-flattering picture of the day-to-day goings-on at a mid-level Chinese cybersecurity firm. Employees’ chats are full of complaints about office politics, lack of basic tech expertise, poor pay and management, and the challenges the company faced in securing clients. One set of screenshots showed arguments between an employee and a supervisor over salaries. And in another leaked chat, a staffer complained to their colleague that their boss had recently bought a car worth over 1m yuan ($139,000) instead of giving their team a pay rise. “Does the boss dream about being an emperor?”"
    },
    {
        "id": "0989189b-a01f-4c53-8e9d-33a0d8577c85",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/uk-news/2024/feb/24/revealed-car-industry-was-warned-keyless-vehicles-vulnerable-to-theft-a-decade-ago",
        "title": "Revealed: car industry was warned keyless vehicles vulnerable to theft a decade ago",
        "author": "",
        "published_date": "2024-02-24T17:08:58+00:00",
        "content": "The car industry ignored warnings more than a decade ago that keyless technology on modern vehicles risked a surge in vehicle thefts, an investigation by the Observer can reveal. Legal and computer researchers claimed keyless entry and vehicle software would be “subverted” because of inadequate security. The industry was warned of research that car owners could “expect to find their cars stolen in the future without any sign of entry.” An increase in vehicle crime with keyless entry has contributed to record prices for car insurance, with some drivers now facing quotes of £2,000 a year or more to insure their car. Car theft in England and Wales in the year to March 2023 was at its highest level for more than a decade. The Observer has found: • A device disguised as a games console - known as an “emulator” - is being exploited by thieves to steal vehicles within 20 seconds by mimicking the electronic key. It is being targeted at Hyundai and Kia models. • “Smart” equipment is on sale online for up to £5,000, allowing thieves to hack into a vehicle’s computer system and programme a new key. • Police facing a spate of keyless car thefts in many neighbourhoods are closing some cases in less than 24 hours even when CCTV footage is available. The motoring lawyer Nick Freeman said: “The motoring industry has been negligent because they were warned when this new technology was beginning to emerge. It’s a catastrophic situation where people cannot insure their cars or face ridiculously high premiums.”   Related: People in the UK: share your experience of vehicle theft   Jaguar Land Rover announced a £10m investment last November to upgrade security for commonly stolen models for cars built between 2018 and 2022. The Observer investigation reveals other vehicles with similar security loopholes, with Hyundai confirming this weekend it is working “as a priority” to prevent an attack on its cars by criminals “using devices to illegally override smart key locking systems”. A report by researchers in 2011 by researchers at the University of California and the University of Washington warned it was possible to implement an attack on the software of modern cars, directing “the car’s compromised telematics unit to unlock the doors [and] start the engine.” An article by Stephen Mason, a barrister specialising in electronic evidence and communication interception, in Computer Law and Security Review in April 2012 warned keyless systems could be “successfully undermined” and unless manufacturers improve the design cars would be stolen without forced entry. Mason said: “There has been insouciance and ignorance by the motoring industry, not wanting to pay the price for proper security. We now have modern cars with the latest technology and the owners have to put on old-fashioned steering locks to protect them from being stolen.” The Society of Motor Manufacturers and Traders (SMMT) denies the industry has failed on security, but it says it has been in an “arms race” with criminals. Mike Hawes, SMMT Chief Executive, said: “Car makers continuously introduce new technology to stay one step ahead of criminals. This investment has helped drive down vehicle theft dramatically over the past 30 years. “While manufacturers continue to enhance security systems, technological innovation alone cannot prevent all theft. That’s why the sector is working closely with the police, insurance industry and other security stakeholders.” The Home Office says overall vehicle crime, which includes theft from a vehicle, is going down. A spokesperson said: “We expect police to take vehicle crime seriously which is why we welcomed the commitment made by police in August last year to follow up on all reasonable lines of enquiry. “We have made great progress in tackling vehicle crime, which is down 39% since 2010. New measures in the Criminal Justice Bill will ban electronic devices used in vehicle theft.”"
    },
    {
        "id": "716884cd-c52f-4ead-b209-5c05823d47d6",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/money/2024/feb/24/smart-keys-car-crime-thieves-hi-tech-arms-race",
        "title": "Gone in 20 seconds: how ‘smart keys’ have fuelled a new wave of car crime",
        "author": "",
        "published_date": "2024-02-24T17:08:46+00:00",
        "content": "When Steve Jessop’s electric Hyundai car was stolen outside his west London house on a rainy day earlier this month, he appealed to neighbours for potential footage of the crime.\nHe quickly secured a CCTV video and was stunned at the ease with which his car had been taken. A hooded figure approached it, opened the doors without forced entry, started the engine and drove off.\nJessop’s car had gone in 20 seconds. The keys to his Hyundai Ioniq 5 were still inside his house and there was no sign of an accomplice.\n“It was just incredible,” said Jessop. “I looked at it and thought: how did that happen? I genuinely thought with all the technology in this car that no one could steal it.”\nJessop got no further clues from the Metropolitan police. He filed a report on the night of the theft on 8 February and was told by email at lunchtime the next day the case had been closed.\nWhile Jessop was left mystified at how his car had been stolen, motor industry sources who spoke to the Observer last week were less surprised.\nThey revealed that hi-tech devices disguised as handheld games consoles are being traded online for thousands of pounds and are used by organised crime gangs to mimic the electronic key on an Ioniq 5, opening the doors and starting the engine.\nThe device, known as an “emulator”, works by intercepting a signal from the car, which is scanning for the presence of a legitimate key, and sending back a signal to gain access to the vehicle. Many owners of Ioniq 5s, which sell from around £42,000, now use steering locks to deter thieves.\nHyundai says it is looking at measures to prevent the use of emulators “as a priority”. But it is not the only carmaker whose vehicles appear to be vulnerable. An Observer investigation found that models by Toyota, Lexus and Kia have also been targeted.\nBritish motorists now face an increase in the number of thefts and rising insurance premiums. (Even before Jessop’s car was stolen, his annual car insurance premium had risen from £574 to £2,240.) Car thefts are at their highest level for a decade in England and Wales, rising from 85,803 vehicles in the year to March 2012 to 130,270 in the year to March 2023 – an increase of more than 50%.\nPart of the reason, say experts, is the rise of keyless entry. Push-button keyless entry fobs for cars were first introduced in the 1980s and by the late 1990s car manufacturers were introducing keyless ignition systems, but this was generally restricted to luxury cars. Subsequently, modern “smart key” fobs, which unlock the car when the owner approaches without the need to press a button, have become more common, offering new security loopholes for crime gangs.\n Interactive\n\nMotoring lawyer Nick Freeman said: “There is a strong legal argument to say these cars are insecure and not fit for purpose. The motoring industry has been negligent. It has failed to prioritise security and motorists are paying the price.”\nAn Observer investigation has found how the industry was warned more than a decade ago of problems in the software it was deploying in cars. A report in 2011 from the University of California and the University of Washington warned of the security vulnerabilities of modern cars, implementing an “attack” to “unlock the doors [and] start the engine”.\nThe next year, Stephen Mason, a retired barrister and co-editor of the book Electronic Evidence and Electronic Signatures, warned in an issue of Computer Law and Security Review that there was an “increasing amount of technical literature on how keyless entry systems can be undermined successfully”. He warned of the risk of “relay attacks” on smart key systems. A thief using this technique can use software to extend the range of the signal the key is broadcasting – even if it is inside a home – activating the unlocking sequence and allowing the car to be driven.\nBy early 2015, the Met was warning that 6,000 cars and vans a year were being stolen without the keys. Last year insurance company Aviva said owners of modern keyless vehicles were twice as likely to make a theft claim. The Met also identified car models “vulnerable to new theft devices” which included the Kia Niro and the Hyundai Ioniq.\nBen Pearson, a former traffic officer with West Yorkshire police and adviser to Nextbase, a dashcam maker, said most of the car thefts he dealt with during his last year with the force in 2020 involved relay attacks on keyless-ignition vehicles. He said: “It’s amazing that you don’t need any training and you can steal someone’s car in seconds.”\nAnother common attack is to hack into the vehicle’s onboard diagnostic port, which is typically under the dashboard and allows access to the vehicle computer systems via a connector for various tasks. It can be used by thieves to programme a new key linked to the vehicle, but they need to find a way to gain entry to the car first.\nMartin East, 58, an engineer from Crowborough, East Sussex, had his 2011 Audi S4 stolen last month without the keys, but police have since recovered it. “I wasn’t aware a thief could plug into the onboard diagnostics until a few weeks ago, but the industry has been aware for 10 years,” said East. “I think they’ve been lazy.”\nThe car industry has implemented various software security upgrades in recent years, but faces criticism for responding too slowly to warnings. Jaguar Land Rover announced a £10m investment to upgrade commonly stolen models built between 2018 and 2022 after a spate of thefts, and complaints from owners that their vehicles were in effect uninsurable.\nLast year, Ken Tindell, a vehicle technology specialist at the software consultancy JK Energy, demonstrated how a thief could gain access to the systems of a vehicle via wiring behind the headlight, and exploit a vulnerability to unlock the car and start the engine. The device he obtained was promoted with the claim it could target some Toyota and Lexus vehicles.\nTindell said he had raised his security concerns more than a decade ago with the industry. “The prevailing view was that criminals are nowhere near educated and smart enough to break into the internal car electronics,” he said. “What they didn’t realised was that somebody would make a box and automate it all for them.”\n\n Related: AA chief reveals his microwave tip to foil tech-savvy car thieves \n\nThe Observer last week found a range of devices for “programming keys and emergency starts” being promoted online for up to £5,000. The “smart device” claims to cover a wide range of manufacturers.\nSteve Launchbury, principal engineer at Thatcham Research, a risk intelligence organisation funded by the UK’s insurance industry, said: “In the old days, there was an effort involved in forcing entry into a vehicle. These devices tend to do it all for you. The industry should be looking to close the vulnerabilities more quickly.”\nLaunchbury said the advice to motorists was to inform themselves about their vehicle’s security systems and features and, where under manufacturer’s warranty, to consult that manufacturer before installing any electronic theft prevention measures. He said trackers and steering locks could both be effective.\nThe Met said it “recognises the impact that motor vehicle crime can have on victims,” adding: “Any allegation of crime reported to the police will be assessed to see if there are any viable lines of inquiry including forensic opportunities that can be progressed.”\nHyundai Motor UK said: “We are aware of a small number of Ioniq 5 thefts. This is an industrywide issue. The criminals appear to be using devices to illegally override smart key locking systems. Hyundai is working closely with law enforcement in the UK. To date, we have helped to recover around 75% of vehicles. We are looking as a priority at a number of measures to help prevent or deter these criminal acts.”\nKia did not respond to a request for comment. A spokesperson for Toyota, which owns Lexus, said: “Toyota and Lexus are continuously working on developing technical solutions to make vehicles more secure. Since introducing enhanced security hardware on the latest versions of a number of models, we have seen a significant drop-off in thefts. For older models we are currently developing solutions.”\nThe Society of Motor Manufacturers and Traders (SMMT) said that rising theft was not caused by alleged failures in car security, but organised crime groups. The SMMT said vehicle security was a “crucial priority” for the industry, which was working to reduce vehicle theft, but that it was in an “arms race” against criminals.\n• Contributors who provided information for this article responded to a community callout. You can contribute to open callouts at the Guardian community team."
    },
    {
        "id": "456f5494-5a6a-4831-a74c-b065059ad53d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/24/elon-musk-giving-pies-california-bakery-cancelled-tesla-order",
        "title": "Elon Musk steps in after California bakery jolted by cancelled Tesla order",
        "author": "",
        "published_date": "2024-02-24T15:35:56+00:00",
        "content": "Billionaire Elon Musk has promised to “make things good” with a California bakery after his company backed out of a pie order that cost the owner thousands of dollars.   Related: Los Angeles socialite found guilty of murder for striking two boys with car   “Just hearing about this. Will make things good with the bakery,” Musk said on X (formerly Twitter) in response to a story about the cancelled order. Musk’s company Telsa ditched an order for 4,000 mini pies from Giving Pies, a Black-owned bakery in San Jose, in California’s South Bay area. Owner Voahangy Rasetarinera told KRON-TV that her bakery received a last-minute order for 2,000 pies from Tesla on Valentine’s Day – a $6,000 catch for the small business, KTVU reported. While Rasetarinera has previously worked on large catering orders with other tech companies, she said that she had to chase Tesla several times about payment for the order, money needed to secure ingredients and pay her staff. On Thursday evening, a Tesla representative named Laura contacted Rasetarinera and apologized about the delayed payment. Laura also upped the order to 4,000 pies, assuring Rasetarinera that money was not an issue. Rasetarinera said that she and her team had worked overtime to pull off the mega order. But Tesla never responded to several invoices sent from the pie company for payment. Instead, on Friday, Laura messaged Rasetarinera, letting her know that the order was no longer needed. Rasetarinera said in a post to Facebook that the casual cancellation “left me reeling, realizing the extent of the impact on my small business”. “I had invested time, resources, and effort based on assurances from Tesla, only to be left high and dry,” she added. Rasetarinera told KRON that the last-minute canceling of such a large order hurt her business. In order to fulfill Tesla’s order, Rasetarinera had to decline other catering gigs. “I’m a small business. I don’t have the luxury of infinite resources so I really need to be paid so I can secure my staff,” Rasetarinera said. A representative of Tesla later reached out to Rasetarinera and said that Laura was not authorized to approve payments, KGO-TV, an ABC affiliate, reported. As of Thursday, the company did not pay Rasetarinera for the pies but offered her a tour of the factory. But Musk’s post on Friday, in light of the viral story, may be a sign that the small business will soon be compensated for its hard work. “People should always be able to count on Tesla trying its best,” Musk said in the post to X."
    },
    {
        "id": "29c79daf-6b5d-4a9c-a3d4-aa8effe4a609",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/23/grace-hopper-nvidia-superchip",
        "title": "‘Amazing Grace’: the name behind Nvidia’s $2tn chip empire",
        "author": "",
        "published_date": "2024-02-23T18:24:07+00:00",
        "content": "In the arid tech sphere of semiconductor manufacturing, one hardback book-sized processor stands out: Nvidia’s H-100. On Friday, the Santa Clara, California, company surpassed $2tn in valuation. Where it goes next will be down to a chip named after “Amazing Grace” Hopper, a US navy rear admiral who became instrumental in the development of design and implementation of programming languages.   Related: Why has Nvidia driven stock markets to record highs?   Nvidia supplies approximately 80% of the global market in chips used in AI applications. The company’s H-100 chips – the H is for Hopper – are now so valuable they have to be transported by armored car, the Wall Street Journal reported on Friday, and demand is so great that some customers are waiting as long as six months to receive it. The importance of Hopper to Nvidia, and to AI computing more generally, was underscored last summer when Jensen Huang, founder and CEO of Nvidia, named its next generation of accelerated computing and generative AI chips the GH200 Grace Hopper Superchip. Hopper was born in 1906 in New York City, graduated from Vassar College with degrees in mathematics and physics in 1928, and joined the navy after the US joined the second world war following the bombing of Pearl Harbor. According to a Yale University biography, she was initially rejected by the navy because of her age and diminutive size, but once commissioned she was assigned to the Bureau of Ships computation project at Harvard, where she worked on the Mark I, the first US electromechanical computer, computing rocket trajectories, anti-aircraft gun range tables and calibrating minesweepers. After the war, Hopper joined the Eckert-Mauchly Computer Corporation, later Sperry Rand, where she pioneered the idea of automatic programming. In 1952, she developed the first compiler, a program that translates written instructions into computer codes. “What I was after in beginning English language [programming] was to bring another whole group of people able to use the computer easily … I kept calling for more user-friendly languages. Most of the stuff we get from academicians, computer science people, is in no way adapted to people,” Hopper explained in a 1980 interview. Hopper retired from the navy as a rear admiral at the age of 79 – the oldest serving officer in the US armed forces. A year before her death in 1992, she was awarded the National Medal of Technology by President George HW Bush. She was posthumously awarded the Presidential Medal of Freedom, the nation’s highest civilian honor, in 2016. In a 1983 interview with 60 Minutes, Hopper was asked if the computer revolution was over. Hopper replied: “No, we’re only at the beginning … We’ve got the Model T.”"
    },
    {
        "id": "3e5978a7-46b6-4104-91c0-47715ff2396d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/23/why-has-nvidia-driven-stock-markets-to-record-highs",
        "title": "Why has Nvidia driven stock markets to record highs?",
        "author": "",
        "published_date": "2024-02-23T16:39:35+00:00",
        "content": "Investor excitement over artificial intelligence reached a new peak this week when better-than-expected results from chipmaker Nvidia drove stock markets in three continents to record highs. The rally began on Thursday and continued into Friday, as Nvidia overtook Google’s parent group, Alphabet, to become the third most valuable company in the US. Its market capitalisation hit $2tn (£1.58tn), surpassed only by Microsoft and Apple. The artificial intelligence (AI) boom has raised many questions, not least over safety and the impact on jobs, but there are also concerns that it might be driving unsustainable market exuberance. Here we look at the latest share price rise and whether it can be maintained. What does Nvidia do and why is it important to AI? Nvidia makes the chips that are crucial to training and operating AI systems – they can carry out the immense data crunching required for tools like chatbots very quickly – and the demand for this crucial infrastructure is soaring as big tech firms join the AI race and make products such as ChatGPT that people want to use. If Nvidia is reporting strong demand for its products, then it is a clear sign that demand for the technology is thriving – and investors will inevitably follow. On Wednesday, the US-based firm surprised markets with the strength of its fourth-quarter earnings. It posted revenues of $22.1bn – against expectations of $20.6bn – and then forecast revenue growth of 233% in the current quarter, ahead of Wall Street forecasts of 208%. Nvidia’s chief executive Jensen Huang said demand for generative AI – technology that immediately produces convincing text, images and audio from simple typed prompts – had reached a “tipping point”. How did markets react? Several leading indices have started the year strongly and reached fresh highs after the Nvidia results. On Thursday, Japan’s main stock market index, the Nikkei, increased 2.19% to close at 39,098.68 in Tokyo – the highest it has been in 34 years. Over the longer term, other factors have boosted the Nikkei, including capital fleeing troubled waters in China and a fall in the value of the yen, but Nvidia’s results had a knock-on effect around the world. Europe’s STOXX 600 and Wall Street’s blue chip Dow Jones and S&P 500 indices all reached new highs. Nvidia was a big beneficiary too, of course, posting the biggest ever single-day increase in the value of a listed company as its worth rose by $277bn.  Interactive  One investment argument is that strong demand for Nvidia GPUs – or graphics processing units, the company’s key AI-friendly product – will ripple out across economies as demand for AI-related infrastructure and services increases. “Nvidia and the golden GPUs are the start of the spending wave, not the end and now Wall Street awaits as a myriad of use cases get built out across the enterprise and consumer ecosystem,” said Dan Ives, analyst at US financial firm Wedbush Securities. For instance, tech research firm Gartner forecasts that global spending on AI software will grow from $124bn in 2022 to $297bn in 2027. There is also a belief that AI will increase productivity, or produce higher economic growth at a lower cost, with the accompanying fear that those lower costs are due to fewer jobs as AI systems render human roles obsolete. According to Goldman Sachs, generative AI could boost US GDP by 0.4 percentage points and by 0.3 points in other developed markets over the next 10 years. How much is AI behind the stock market increases? It has played a significant role in continuing to boost the big tech stocks, which play such an outsize role in US markets alone. This week Deutsche Bank pointed out that tech stocks were playing an ever-bigger role in the S&P 500, the biggest US index. The bank pointed out that Microsoft, Nvidia, Apple, Amazon and Google’s parent, Alphabet, account for almost a quarter of the value of the S&P 500. “The US equity market is rivalling 2000 and 1929 in terms of being its most concentrated in history,” wrote Jim Reid, Deutsche Bank’s head of global economics and thematic research.  Interactive  Once you add in the other two big US tech stocks – Mark Zuckerberg’s Meta and Elon Musk’s Tesla – you get the so-called “Mag 7” – a reference to the Magnificent Seven movie – who together have a combined market cap of over $13tn, a bigger valuation than all the stocks listed in any other single country, other than the US. All of Mag 7 have interests in AI to varying degrees. For instance, Amazon and Microsoft own cloud computing businesses that are crucial to developing and operating AI models (Microsoft is also a big investor in ChatGPT developer OpenAI), while Apple’s shares were boosted recently by a bank report predicting new AI features embedded in its phones will increase demand. Is the AI stock market boom sustainable? The excitement around AI breakthroughs continues, with OpenAI revealing the remarkable-looking Sora video generation tool last week and Google releasing an upgrade to its Gemini model. Nvidia’s optimistic sales forecasts also indicate an ongoing increase in demand for AI-related infrastructure. Deutsche Bank’s Reid states that there are arguments “for and against” the continued dominance of the Mag 7. In their favour, he argues that AI is “just in its infancy” and the companies have room to grow due to their global reach. Factors against them include regulators launching antitrust actions and no one really knowing how AI will pan out and who will win. Neil Wilson, chief analyst at brokerage firm Finalto, says this is a bubble – “it’s the nature of markets, it’s built into them to produce these manias” – and the question is when and how it levels off. “Does the levelling off see a bust phase or can it glide gently down, which stocks are overvalued now and which are not, and I guess crucially from the wider perspective, does it herald a new era of productivity boom which sees multiples expand for almost all equities?” Are there broader economic factors at play in the stock market rise? Stock markets have rallied in recent weeks, reflecting an easing of tensions that not so long ago appeared ready to wreck a global economic recovery. The Gaza conflict is a humanitarian disaster, but for the time being, ships laden with oil and gas are reaching their destinations, allowing the oil price to stabilise and the gas price to tumble back towards pre-pandemic levels. Low and stable energy prices are the bedrock of any recovery along with low interest rates, and central banks have signalled they will begin to make cuts in the second half of this year after a steep fall in inflation. Even Japan’s Nikkei stock market – held back for decades by low growth while the country coped with an ageing population – joined New York’s S&P 500 and German DAX indexes in reaching a record high. What could bring the rally to a halt? The conflicts in Ukraine and the Middle East could escalate further, sparking a resurgence of shortages of energy, food and raw materials. The result will be a return to high inflation, which again could dent spending and confidence. Even without a global downturn, stock market investors may panic if they believe financial assets have become overpriced.  It has been 16 years since the great financial crash of 2008, and many economists believe that the best efforts of regulators to prevent excessive speculation across the board during that time will not be enough to prevent another 2008-style wipeout."
    },
    {
        "id": "b6662be7-e5ab-4da2-80fb-162f731c1a99",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/23/we-milked-the-hell-out-of-it-what-happens-after-local-food-places-go-viral",
        "title": "‘We milked the hell out of it’: what happens after local food places go viral?",
        "author": "",
        "published_date": "2024-02-23T16:00:02+00:00",
        "content": "Ben Newman, also known as the Spudman, spends as much time posing for selfies these days as he does selling jacket potatoes from his van in the middle of Tamworth. His shop is the latest viral sensation on TikTok and has seen people travel from all over the world to try his food – jacket potatoes with classic toppings such as butter, cheese and beans – although Newman isn’t quite sure why. He joins the likes of Binley Mega Chippy, Get Baked and Wakey Wines as one of a number of businesses to have seen a huge influx of customers in recent years after going viral on social media, sometimes with little rhyme or reason. “It’s a tough one to say why everyone is coming. I’m under no illusion that what we do, you can get anywhere. I’m just very good at shouting about it,” said the 39-year-old, while serving about 100 customers who had flocked to his van for a spud at 11am on a Monday. As he talks, a teenager comes up and asks if he can record Newman giving a “shout-out” to his group chat, quickly followed by a man asking if he could record a quick hello for his son. Are people coming to Tamworth for him or the potatoes? “I think probably a bit of both,” said Newman. “Customer service is just as important to me as the product that we sell.” Newman has accrued more than 44m likes on TikTok through daily videos and live streams of life in his van – often offering customers a free potato and recording their reactions. He has been posting on the platform for three years, ever since his son introduced him to it as a way of attracting new customers post-Covid, but business really took off in October. Since then, he’s had to take on three extra staff members to deal with the workload. “It is taking its toll on me. But I don’t want to miss any opportunities that can come from it,” he said, adding that sponsorship deals had allowed him to put on a free day when he handed out more than 2,000 jacket potatoes, while the singer Tom Walker performed in the square. “Maybe I am overdoing it but it might calm down in a month or two, who knows?” he said.   Related: ‘A gift from God’: Binley Mega Chippy owner basks in TikTok fame   People have been drawing comparisons between Spudman and Binley Mega Chippy, the unassuming chip shop in Coventry that went viral on TikTok 18 months ago. Hundreds of people travelled for miles to sample its wares: queues snaked around the car park, people posed for selfies, and some wore custom-made fan T-shirts. Although the hype has died down, the shop still attracts TikTok tourists, easily spotted as they stop to take pictures. Until a month ago, the shop was still selling Binley Mega Chippy T-shirts. The staff who work there have mixed feelings about the summer of 2022 when the shop became famous, bringing visitors from as far away as Australia and Mexico. “It was pretty horrible at the time,” said Vinay Yeluri, 27, who has worked behind the counter for about three years. “We had a lot of staff but we couldn’t control the crowd, it was all the way down to the road. It was too much. There were TV crews everywhere, people racing sports cars outside. “We’re basically back to normal now but we do get kids getting their parents to bring them here on the weekends because they want to see it.” He recalled how staff became overwhelmed, quickly expanding from a team of five to 12, and they struggled to keep fresh produce coming in to keep up with the demand. “There were more than 12 people working and there wasn’t even enough room to stand to serve people. It brought in a lot of money though,” Yeluri said. Even now, the staff are still confused as to how they achieved such social media success, which seemed to stem from a catchy theme song for the chip shop that they claimed they had no involvement in. “Even we don’t know what happened, why it happened. I think it was just kids on TikTok having some fun, someone made the song and it all took off,” he said. “People have been coming here for 15 years. Everyone was just like, ‘It’s just normal fish and chips, what’s so special?’” While sometimes social media content takes on a life of its own, others find themselves at the centre of an online flurry of their own making. When the Leeds bakery Get Baked fell foul of government regulations on importing brightly coloured US sprinkles that were banned in the UK, a well-orchestrated social media storm led to a five-fold increase in Instagram followers “literally overnight”, said its founder, Rich Myers. The bakery already had a large social media presence through Myers’s down to earth and somewhat sweary posts, but Sprinklegate, as it became known, turbocharged what had been a small shop serving a local area to a national brand delivering orders across the UK. “It was the BBC’s most-read article of 2021 worldwide,” he said. “We had to close because I was getting that much press and doing so many interviews.” As a result, Myers got a book deal and a lot of celebrity supporters, including the chefs Jamie Oliver and Andi Oliver, and the comedian and food podcaster Ed Gamble – fans of the Bruce, a 24-layer chocolate cake inspired by the one eaten by Bruce Bogtrotter in the Roald Dahl book Matilda. For Myers, who is in the process of moving to a larger bakery – though the shop will stay in the same spot in Headingley – and who has also ended up running a separate sprinkles business, there have been no downsides. “We did well to capitalise on the sprinkles thing and turn it into as big a deal as we could,” he said. “It gave us an opportunity for something to be milked so heavily and we milked the hell out of it.”   Related: Wakey Wines off-licence selling Prime and camel milk goes viral on TikTok   The same could be said for Mohammad Azar Nazir, the owner of Wakefield Wines, which became a viral hit last year when it was one of the few shops in the UK selling Prime, a “hydration drink” made by the YouTubers Logan Paul and KSI. The off-licence harnessed TikTok to create catchphrases such as “What’s the best shop in Wakey? Wakey Wines!”, “Abdul come closer, Abdul go back” and “Bingo bingo Gala bingo”, and has reached nearly 900,000 followers on TikTok. Customers in the shop came from all around the world, including China, Dubai and New Zealand. “Honestly it’s mad, it’s bonkers,” said Nazir. “It was a surprise, I’m not going to lie to you.” Nazir is also working on a book and will be opening a second shop on the same street in the summer, which will focus on sweets. “I’m not good at reading or writing but I’m good at making stuff,” he said. “I like to be different from other shopkeepers.” But there have been downsides. Nazir said he got recognised and asked for pictures and videos, which his children found “annoying”. But worse, last month the shop was trashed by people he said were jealous of his success, who ripped products from the shelves and smashed the windows. Recently, his house windows were smashed. “I’m not going to be intimidated or bullied to leave social media because I don’t do nowt wrong, I just do it for my business,” he said."
    },
    {
        "id": "002a9630-faed-4d61-8d45-6a4717abe58d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/23/tyler-perry-halts-800m-studio-expansion-after-being-shocked-by-ai",
        "title": "Tyler Perry halts $800m studio expansion after being shocked by AI",
        "author": "",
        "published_date": "2024-02-23T10:06:58+00:00",
        "content": "Tyler Perry has paused an $800m (£630m) expansion of his Atlanta studio complex after the release of OpenAI’s video generator Sora and warned that “a lot of jobs” in the film industry will be lost to artificial intelligence. The US film and TV mogul said he was in the process of adding 12 sound stages to his studio but has halted those plans indefinitely after he saw demonstrations of Sora and its “shocking” capabilities. “All of that is currently and indefinitely on hold because of Sora and what I’m seeing,” Perry said in an interview with the Hollywood Reporter. “I had gotten word over the last year or so that this was coming, but I had no idea until I saw recently the demonstrations of what it’s able to do. It’s shocking to me.” The AI tool was launched on 15 February – with limited access to a few researchers and video creators – and caused widespread astonishment with its ability to produce realistic footage a minute long from simple text prompts. Perry, whose successes include the Madea film series, said Sora’s achievements meant he would no longer have to travel to locations or build a set: “I can sit in an office and do this with a computer, which is shocking to me.” Demonstrations released by OpenAI, the developer of the groundbreaking ChatGPT chatbot, show photorealistic scenes in response to prompts such as asking for a shot of people walking through “beautiful, snowy Tokyo city” where “gorgeous sakura petals are flying through the wind along with snowflakes”.  Introducing Sora, our text-to-video model.Sora can create videos of up to 60 seconds featuring highly detailed scenes, complex camera motion, and multiple characters with vibrant emotions.                                 https://t.co/7j2JN27M3WPrompt: “Beautiful, snowy… pic.twitter.com/ruTEWn87vf— OpenAI (@OpenAI) February 15, 2024  Perry said the breakthroughs presented by Sora would affect a range of jobs throughout the film industry, including those of actors, editors, sound specialists and transportation crew. He said: “I am very, very concerned that in the near future, a lot of jobs are going to be lost. I really, really feel that very strongly.” Perry said an immediate example was the construction workers and contractors who would no longer work on his planned studio expansion because “there is no need to it”. He added that he used AI in two recently shot films in which the technology was used to age his face and help him avoid hours in the makeup chair. Concerns over the impact of AI on jobs were a feature in recent strikes by Hollywood actors and writers, with the peace deals that ended those disputes both featuring guardrails on using the technology. However, Perry told the Hollywood Reporter that a “whole industry” approach was still needed to save jobs. “It can’t be one union fighting every contract every two or three years. I think that it has to be everybody, all involved in how do we protect the future of our industry because it is changing rapidly, right before our eyes,” he said."
    },
    {
        "id": "2ca97af3-dbbc-44a2-8a45-bc8c3e93c163",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/23/oneplus-12-review-smartphone-left-behind-by-top-rivals-screen-battery-life",
        "title": "OnePlus 12 review: smartphone left behind by top rivals",
        "author": "",
        "published_date": "2024-02-23T07:00:41+00:00",
        "content": "OnePlus’s latest top phone can’t shake the feeling of being left behind by rivals. Even with a sleek appearance, speedy software and longer battery life the OnePlus 12 is devoid of the much-hyped AI tools packed into handsets from Samsung, Google and others. It feels more like a phone from 2020 than from the new era of artificial intelligence. That could appeal to anyone looking for a cut-down, relatively clutter-free experience – as may the cost of £849 (€969/$799), which undercuts £1,000 rivals with all the bells and whistles. But with modern standards, it feels lacking. The design is very similar to last year’s OnePlus 11 (which was priced £120 less at launch): a slick metal and glass sandwich that feels as smooth as it looks. The huge 6.82in OLED screen is super crisp, smooth and extremely bright. The large circular camera bump on the back is a standout design element alongside the fan-favourite alert slider on the side. Inside the OnePlus has Qualcomm’s latest top Snapdragon 8 Gen 3 chip, which is 30% faster and 20% more power efficient than predecessors. It is an extremely powerful chip that is only available in a handful of new phones. The OnePlus certainly feels fast and smooth in normal operations, but to get maximum performance out of it – making it run at full tilt – you have to enable the “high performance” mode buried in the settings or else use it in gaming mode. The phone is therefore geared up for power efficiency over raw performance, which provides a very long battery life. It lasts a solid 52 to 55 hours between charges, with more than nine hours of active screen use with default settings. This is significantly longer than last year’s model and up there with the best in the business. The OnePlus charges super quickly too, reaching 100% in under 30 minutes with the included 100W charger. Specifications  Screen: 6.82in 120Hz QHD+ OLED (510ppi) Processor: Qualcomm Snapdragon 8 Gen 3 RAM: 12 or 16GB Storage: 256 or 512GB Operating system: OxygenOS 14 (Android 14) Camera: 50M + 48MP ultrawide + 64MP 3x; 32MP selfie Connectivity: 5G, eSIM, wifi 7, NFC, Bluetooth 5.4 and GNSS Water resistance: IP65 (spray resistant) Dimensions: 164.3 x 75.8 x 9.2mm Weight: 220g  Sustainability The battery is rated to maintain at least 80% of its original capacity for 1,600 full charge cycles. The phone does not contain meaningful amounts of recycled materials but is generally repairable by OnePlus, with screen replacements costing £174 and batteries £21 plus about £50 for labour. The company participates in the Eco Rating scheme, which evaluates environmental impact, and is included in the parent company Oppo’s yearly sustainability reports. OxygenOS 14 The phone runs OxygenOS 14, which is a modified version of the latest Android 14 software. It is generally pretty slick and has a good amount of customisation options covering everything from gestures, the look and feel of the software and various multitasking tools. But it lacks any of the AI tools and smart systems that have become the mainstay of rivals in both Android and iPhone camps. There is no advanced photo editing, no AI wallpaper generator, no AI summarisation or transcription tools, not even Google’s excellent feature Circle to Search. Not all the AI tools on rivals are hits, of course, but many of them have become useful additions you should expect in a top-end phone. The other big miss is short software support. OnePlus will only provide four years of Android updates and five years of security patches, which is at least two years short of the benchmark set by Apple, Google and Samsung, let alone Fairphone’s 10 years. OnePlus is part of smartphone giant Oppo, so it can and should be doing better. Camera The OnePlus 12 has a camera that is a small step up from last year’s model. On the back is a main 50-megapixel camera capable of producing nice-looking images in a reasonably wide range of lighting, though it does become a bit inconsistent in dim or night scenes. The 64MP 3x telephoto camera is equally solid and can perform an in-sensor zoom to reach 6x magnification with additional digital zoom on top. In good light the 6x zoom is very good, but it quickly becomes muddy in less than bright conditions. Similar can be said about the ultrawide. Overall the camera system is solid, but it struggles more than rivals indoors, in dim light or in high-contrast scenes. OnePlus still has work to do to match Google, Samsung or Apple. Price The OnePlus 12 costs from £849 (€969/$799). For comparison, the Google Pixel 8 Pro costs £999, the Samsung Galaxy S24+ costs £999 and the iPhone 15 Plus costs £899. Verdict The OnePlus 12 is a solid phone that is caught in no man’s land. It has the performance but lacks the features to match its high-end rivals. A complete lack of AI tools is almost retro at this stage and renders it a bit boring. Its software support life falls far short of the bar set by the Google, Samsung or Apples of this world, losing it a star. At the same time, it is also too expensive to be considered a mid-range phone, which would typically cost between £350 to £650. That leaves OnePlus in an awkward spot: it has top performance but fundamentally delivers less than its rivals.  Pros: Slick, nice design, long battery life and longevity, 30-minute full charge, top chip, great screen. Cons: Software support too short, no meaningful AI features, camera lags behind rivals, only spray resistant, price increase over predecessors, performance artificially limited outside “high performance” mode.  "
    },
    {
        "id": "3aa8fa62-0182-42af-ab8b-3fa2ac5ba57f",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/23/huge-cybersecurity-leak-lifts-lid-on-world-of-chinas-hackers-for-hire",
        "title": "Huge cybersecurity leak lifts lid on world of China’s hackers for hire",
        "author": "",
        "published_date": "2024-02-23T05:00:39+00:00",
        "content": "A big leak of data from a Chinese cybersecurity firm has revealed state security agents paying tens of thousands of pounds to harvest data on targets, including foreign governments, while hackers hoover up huge amounts of information on any person or institution who might be of interest to their prospective clients. The cache of more than 500 leaked files from the Chinese firm I-Soon was posted on the developer website Github and is thought by cybersecurity experts to be genuine. Some of the targets discussed include Nato and the UK Foreign Office. The leak provides an unprecedented insight into the world of China’s hackers for hire, which the head of the UK’s security services has called a “massive” challenge for the country. The files, which are a mixture of chat logs, company prospectuses and data samples, reveal the extent of China’s intelligence gathering operations, while also highlighting the market pressures felt by the country’s commercial hackers as they vie for business in a struggling economy. I-Soon appears to have worked with – and later been embroiled in a commercial dispute with – another Chinese hacking outfit, Chengdu 404, whose hackers have been indicted by the US Department of Justice for cyber-attacks on companies in the US as well as pro-democracy activists in Hong Kong, among other targets. Other targets discussed in the I-Soon leaks include the British thinktank Chatham House and the public health bureaux and foreign affairs ministries of Asean countries. Some of this data seems to have been gathered on spec, while in other cases there are specific contracts with a Chinese public security bureau to gather a certain type of data. A spokesperson for Chatham House said: “We are aware of this data coming to light and are naturally concerned. Chatham House takes data and information security extremely seriously. In the current climate, we, along with many other organisations, are the target of regular attempted attacks from both state and non-state actors. “We have protection measures in place including technology-based safeguards which are reviewed and upgraded on a regular basis.” A Nato official said: “The alliance faces persistent cyber-threats and has prepared for this by investing in extensive cyber defences. Nato reviews every claim of cyber-threats.” The UK Foreign Office declined to comment. The services offered by I-Soon are varied. In one example, the public security bureau of a city in Shandong paid nearly £44,000 to obtain access to the email inboxes of 10 targets for one year. The company claimed to be able to hack accounts on X, obtain personal information from Facebook, obtain data from internal databases and compromise various operating systems including Mac and Android. In one of the files there is a screenshot of a folder entitled “Notes from the secretariat of European Affairs of North Macedonia”. Another screenshot shows files that appear to relate to the EU, including one entitled “Draft EU position with regard to COP 15 part 2”. The file names reference an encryption system used by EU entities to secure official data. In some cases, is not clear what the purpose of collecting the data was. “The Chinese state is basically hoovering up as much data as they can,” said Alan Woodward, a computer security expert at the University of Surrey. “They just want as much information as they can in case it proves useful.” Woodward noted that unlike Russian state-linked hackers who conduct ransomware attacks or other disruptive actions, Chinese attempts tended to focus on mass data harvesting. “Some of it could be interpreted as laying the groundwork for being disruptive at a later stage,” Woodward said. Last year, parliament’s intelligence and security committee report on China said: “China’s cyber expertise allows it to target a diverse range of organisations and datasets – and increasingly unusual ones.” Experts believe that the goal of data gathering may be to identify potential targets for human intelligence operations. I-Soon also targeted domestic victims. In an undated cooperation agreement with a local authority in Xinjiang, I-Soon stated that it could provide “anti-terrorism” support to the local police in monitoring Uyghurs. I-Soon said that it had more than a decade of experience in accessing “various server permissions and intranet permissions in multiple countries”. The company claimed to have obtained data from counter-terrorism authorities in Pakistan and Pakistan’s postal service. Pakistan’s embassy in London did not respond to a request for comment. Some of the promises to clients might have been sales bluster. In one discussion, an employee asked: “Are customers deceiving us, or are we deceiving customers?” The worker continues that deceiving customers about the company’s abilities is “normal, but it is not good for the company to deceive its employees”. Mei Danowski, a China cybersecurity expert and author of the Natto Thoughts newsletter, said: “We think about [Chinese hackers] as ‘Oh, the state gives them cash to do stuff.’ In reality, if these leaked documents are true, it’s not like that. They have to go and look for business. They have to build up a reputation.” Other chat logs were strikingly mundane. Employees discussed Covid-19 and the financial pressures at I-Soon. “Originally, everyone knew that the company was having a hard time, and they all understood. After all, the epidemic is so severe,” wrote one worker in March 2021. But, they complained, I-Soon “didn’t say they wouldn’t pay us wages”. By the following year, the pressures at the company seemed to have intensified. The chief executive, Wu Haibo, who uses the pseudonym Shutd0wn, said that the loss of core staff had dented customers’ confidence, leading to a loss of business. Wu did not respond to a request for comment. “The boss is really anxious,” wrote one employee in September 2022. “I don’t know if the company can survive until the end of the year.” In another chat log, workers spoke about the company’s poor sales and a souring mood in the office. One employee turned to a universal solace: “I’ll probably scream if I can’t have a drink.”"
    },
    {
        "id": "018bb257-f02b-4675-85a9-1f7cd6c7e302",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/22/reddit-ipo-stock-market",
        "title": "Reddit files for initial public offering ahead of stock market debut",
        "author": "",
        "published_date": "2024-02-22T23:10:33+00:00",
        "content": "Reddit set the stage for its highly anticipated stock market debut, preparing investors for the largest initial public offering by a major social network in four years. A filing with the Securities and Exchange Commission on Thursday disclosed the financial performance of the social media group, and revealed that Sam Altman, the OpenAI founder and CEO, is its third-largest shareholder, with an 8.7% stake. The company plans to trade on the New York Stock Exchange under the ticker symbol “RDDT.” Its much-awaited listing – expected in March – would be the biggest social media IPO since Pinterest went public in 2019.   Related: How social media’s biggest user protest rocked Reddit   The number of shares to be offered and the price range for the proposed offering have not yet been determined, Reddit said in a statement. The IPO filing revealed that Reddit sustained $90.8m in losses in 2023, down from $158.6m in 2022, as its revenue grew by roughly 21%. The platform has 267.5 million active weekly users, more than 100,000 active communities, and 1bn total posts, it said. The business estimated that its US average revenue per user, or ARPU, was $3.42 for the last quarter of 2023 – a decrease of 2% year over year. Advance Magazine Publishers is the company’s largest shareholder, with a 30.1% stake, while the Chinese multinational Tencent has 11%. Reddit’s planned IPO comes almost two decades after Reddit’s launch and will be a major test for the platform that still lags the commercial success of social media contemporaries such as Facebook and Twitter, now known as X. The filing also revealed an unusual plan from Reddit to allow its most active users to buy shares in the IPO. Reddit will allocate shares using a tiered system rewarding users who have “meaningfully contributed to Reddit community programs”, users who have completed a certain number of moderator actions and people with a minimum “karma” score – the points given to users by others on the platform for posting content. Reddit was valued at $10bn in a funding round in 2021 and it is unclear what valuation the company will aim for during its share sale in the coming weeks. It is expected to seek a sale of nearly 10% of its shares in the IPO, Reuters reported earlier. In the filing, Reddit cited data licensing arrangements as a revenue stream – saying that in January 2024 it entered data licensing arrangements with an aggregate contract value of $203m. Reuters reported on Wednesday that Reddit struck a deal with Alphabet’s Google to make its content available to train the search engine giant’s artificial intelligence (AI) models. The contract is worth about $60m each year, according to one of the sources. “Reddit data constantly grows and regenerates as users come and interact with their communities and each other,” Reddit said in its filing. “We believe our growing platform data will be a key element in the training of leading large language models (“LLMs”) and serve as an additional monetization channel for Reddit.” Reddit had confidentially filed for the IPO in late 2021, but tough economic conditions and the poor performance of listed technology stocks had compelled it to delay the offering. Morgan Stanley and Goldman Sachs have been tapped as the lead underwriters for Reddit’s IPO, which includes more than a dozen other banks."
    },
    {
        "id": "788925ad-10d4-4852-b536-a28680ff4cc4",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/22/apple-says-spotify-wants-limitless-access-to-its-tools-without-paying",
        "title": "Apple says Spotify wants ‘limitless’ access to its tools without paying",
        "author": "",
        "published_date": "2024-02-22T22:58:52+00:00",
        "content": "Apple has condemned Spotify over the long-running competition complaint filed with the EU that could see the tech company face a huge fine if found guilty. After reports the bloc has concluded its investigation into the music streaming service’s claims of anti-competitive behaviour by Apple over its App Store rules, with the prospect of a €500m (£425m) fine, the iPhone manufacturer has accused Spotify of trying to get “limitless” access to its tools without paying.   Related: Why is Apple facing a €500m fine from EU over music streaming?   Stockholm-based Spotify filed a complaint with the EU in 2019, claiming that App Store rules limit choice and competition because Apple charges a 30% fee on purchases made through the store including music streaming subscriptions. Spotify argued it is an unfair “tax” that Apple’s own competing Apple Music streaming service is not subject to, giving it an unfair advantage. The company has also argued that Apple’s rules do not allow it to tell users about cheaper ways to subscribe outside the App Store. In a statement on Thursday night, Apple said Spotify did not offer subscriptions via the App Store and therefore did not pay Apple any commission in the EU. “We’re happy to support the success of all developers – including Spotify, which is the largest music streaming app in the world,” Apple said. “Spotify pays Apple nothing for the services that have helped them build, update and share their app with Apple users in 160 countries spanning the globe. “Fundamentally, their complaint is about trying to get limitless access to all of Apple’s tools without paying anything for the value Apple provides.” According to reports earlier this week, the European Commission is close to concluding its investigation. Apple said Spotify, which was formed in 2006, had the option to directly link to its website for account creation and management but chose not to exercise it. The US-based firm added that despite claims of competition concerns and not allowing it to tell users how to subscribe, Spotify had grown into the largest digital music business in the world with more than 50% market share in Europe. It also argued that Spotify and other music streaming services had many open channels, including email marketing and social media, to advertise to consumers and show them how to sign up outside the App Store. Apple has also criticised EU regulators, claiming that despite the issue being examined in different forms for about 10 years, the European Commission had failed to find any evidence of consumer harm or anti-competitive behaviour by the firm in this market. The company said the investigation could just cement Spotify’s dominant position as the market leader, rather than promote competition. At the time Spotify filed its complaint in 2019, its founder, Daniel Ek, wrote in a blogpost that Apple had “introduced rules to the App Store that purposely limit choice and stifle innovation”."
    },
    {
        "id": "84acb7ad-3a9f-451f-a099-b6ac48faba44",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/22/google-pauses-ai-generated-images-of-people-after-ethnicity-criticism",
        "title": "Google pauses AI-generated images of people after ethnicity criticism",
        "author": "",
        "published_date": "2024-02-22T12:26:07+00:00",
        "content": "Google has put a temporary block on its new artificial intelligence model producing images of people after it portrayed German second world war soldiers and Vikings as people of colour. The tech company said it would stop its Gemini model generating images of people after social media users posted examples of images generated by the tool that depicted some historical figures – including popes and the founding fathers of the US – in a variety of ethnicities and genders. “We’re already working to address recent issues with Gemini’s image generation feature. While we do this, we’re going to pause the image generation of people and will rerelease an improved version soon,” Google said in a statement. Google did not refer to specific images in its statement, but examples of Gemini image results were widely available on X, accompanied by commentary on AI’s issues with accuracy and bias, with one former Google employee saying it was “hard to get Google Gemini to acknowledge that white people exist”. Jack Krawczyk, a senior director on Google’s Gemini team, had admitted on Wednesday that the model’s image generator – which is not available in the UK and Europe – needed adjustment. “We’re working to improve these kinds of depictions immediately,” he said. “Gemini’s AI image generation does generate a wide range of people. And that’s generally a good thing because people around the world use it. But it’s missing the mark here.”  We're already working to address recent issues with Gemini's image generation feature. While we do this, we're going to pause the image generation of people and will re-release an improved version soon. https://t.co/SLxYPGoqOZ— Google Communications (@Google_Comms) February 22, 2024  Krawczyk added in a statement on X that Google’s AI principles committed its image generation tools to “reflect our global user base”. He added that Google would continue to do this for “open ended” image requests such as “a person walking a dog” but acknowledged that the response prompts with a historical slant needed further work. “Historical contexts have more nuance to them and we will further tune to accommodate that,” he said.  We are aware that Gemini is offering inaccuracies in some historical image generation depictions, and we are working to fix this immediately.As part of our AI principles https://t.co/BK786xbkey, we design our image generation capabilities to reflect our global user base, and we…— Jack Krawczyk (@JackK) February 21, 2024  Coverage of bias in AI has shown numerous examples of a negative impact on people of colour. A Washington Post investigation last year showed multiple examples of image generators showing bias against people of colour, as well as sexism. It found that the image generator Stable Diffusion XL showed recipients of food stamps as being primarily non-white or darker-skinned despite 63% of the recipients of food stamps in the US being white. A request for an image of a person “at social services” produced similar results. Andrew Rogoyski, of the Institute for People-Centred AI at the University of Surrey, said it was a “hard problem in most fields of deep learning and generative AI to mitigate bias” and mistakes were likely to occur as a result. “There is a lot of research and a lot of different approaches to eliminating bias, from curating training datasets to introducing guardrails for trained models,” he said. “It’s likely that AIs and LLMs [large language models] will continue to make mistakes but it’s also likely that this will improve over time.”"
    },
    {
        "id": "c85ef235-c071-4dde-84b4-3067b5ec04fb",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/22/second-accidental-sharing-of-confidential-information-regrettable-finance-department-says",
        "title": "Second accidental data leak in four months ‘regrettable’, finance department says",
        "author": "",
        "published_date": "2024-02-22T07:49:22+00:00",
        "content": "The finance department has accidentally shared confidential commercial information for the second time as new data reveals the number of human errors behind government data breaches. The department has confirmed that last week it emailed 236 suppliers, and that the email included “embedded information with some third-party confidential information”. The shadow finance minister, Jane Hume, said the breach would damage confidence in the procurement process, especially coming after a similar breach in November last year.   Related: Is my home spying on me? As smart devices move in, experts fear Australians are oversharing   “Companies and individuals impacted by this gross incompetence may exercise their rights against the commonwealth, potentially costing taxpayers millions,” she said. On Thursday, the Office of the Australian Information Commissioner released the latest data breach statistics, showing the federal government is back in the top five sectors hit by breaches for the first time in three years. The data shows the government takes longer to identify and respond to breaches than other sectors, and that while usually criminal acts are behind breaches, in government agencies it is more likely to be human error. The finance department said in its statement it has tried to call all suppliers to ask them to delete the email and attachments. It said “no third-party confidential information would have been accessed or viewed by a person who simply opened the email or its attachments”.  Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup  The finance department secretary, Jenny Wilkinson, has directed there be an independent review of that breach and the November 2023 release. It will be conducted by the former commonwealth ombudsman Michael Manthorpe. “The review will consider the circumstances that led to the unauthorised disclosure of the information, as well as the department’s systems and processes,” the department said in a statement. “The potential disclosure of this third-party confidential information is regrettable, and finance apologises for the oversight.” In November 2023, a department officer uploaded confidential pricing information from hundreds of firms to the wrong place within AusTender. The information was then sent out as part of a request for quotes from government departments, making it potentially available to 22 service providers. The providers were then asked to guarantee confidentiality and monitored to ensure they had not used the information to gain a commercial advantage. In the latest breach, the Australian reported that supplier and service provider names and price scales for major firms including Deloitte, KPMG, Minter Ellison and Boston Consulting Group were included on a hidden tab on a spreadsheet. David Pocock, a independent senator for the ACT, said the “repeated and even worse failure of process from the Department of Finance is deeply concerning”. He said it was “very damaging for smaller firms who are now at a serious disadvantage with 236 suppliers having received their pricing details”. “The government needs to immediately spell out what additional steps it is putting in place to ensure this mistake isn’t made a third time and get on with the serious procurement reform that is long overdue.” According to the OAIC data, health sector providers had the most breaches in the six months to December 2023, with 104. The finance sector was next with 49, followed by insurance (45), retail (39) and government (38). Overall two-thirds of the data breaches were from malicious or criminal attacks including cybersecurity incidents, just under a third were from human error (with information being sent to the wrong person the most common error), while 3% were from a system fault. But that trend was reversed in the government, where 12 breaches were malicious or criminal and 26 were from human error. The government also took longer to identify breaches, with 37% of breaches identified within 10 days, compared with 75% for health service providers. And it took longer to report breaches, with 45% reported within 30 days, compared with 86% for the health sector. “These statistics suggest Australian government agencies should check they have effective systems for detecting, assessing, responding to and notifying data breaches,” the OAIC said. “Such systems are fundamental to an agency’s ability to meet the NDB (notifiable data breaches) scheme’s requirements.”"
    },
    {
        "id": "f8c028c1-1171-4d38-bca6-0981acb8d9b6",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/21/nvidia-revenue-ai-artifiical-intelligence",
        "title": "Nvidia reports enormous revenue as AI hits a tipping point",
        "author": "",
        "published_date": "2024-02-21T22:04:29+00:00",
        "content": "The artificial intelligence boom is pushing demand for Nvidia’s products past Wall Street’s already lofty expectations. The chipmaker beat analyst expectations on Wednesday by leaps and bounds when it reported fourth-quarter earnings, posting $22.1bn in revenue on an expected $20.55bn and $4.93 in earnings per share against an expected $4.64. Revenue was 22% higher than the previous quarter, up 265% from a year ago. Nvidia’s most closely watched earnings figure – revenue from data centers – was up more than 400% from the same period last year, reaching $18.4bn.   Related: Nvidia to release three new AI chips for China after US restrictions – report   Jensen Huang, founder and CEO of Nvidia, said in a press release: “Accelerated computing and generative AI have hit the tipping point. Demand is surging worldwide across companies, industries and nations.” Nvidia’s revenue and demand for its shares are seen as bellwethers for overall interest in artificial intelligence due to the heavy reliance on its wares in developing AI. Microsoft, OpenAI, Amazon, Meta and Google have all struck deals with the company to purchase its chips in bulk as they race to release new AI products and features. Some key players, including OpenAI’s Sam Altman, are toying with starting their own AI chip ventures to compete with the established Nvidia, which would probably cost hundreds of billions of dollars. Nvidia plans to ship a new chip, the B100, at the highest end of its products line in 2024, stoking expectations around its explosive growth. Nokia and Nvidia announced a partnership on Wednesday to develop AI solutions that could improve telecommunications infrastructure. Wall Street has come to expect enormous growth from Nvidia; the baseline analyst expectation on Wednesday was a 240% increase in revenue. Tech companies are rushing to create AI products on the back of the company’s specialized AI chips and software, considered among the best on the market. Nvidia’s revenue tripled in the last quarter, and its earnings have beaten analyst expectations for the past four. Its stock has more than tripled in value over the past year, bringing the company above the $1.5tn valuation threshold. The company surpassed Google and Amazon in market capitalization last week, becoming the third-largest company in the world by value for a few days."
    },
    {
        "id": "882b063f-d9c0-4984-8bbb-1b083e0e66d8",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/21/german-town-votes-against-tesla-plans-to-expand-gigafactory-carmake-industrial-action-sweden",
        "title": "German town votes against Tesla plans to expand ‘gigafactory’",
        "author": "",
        "published_date": "2024-02-21T17:59:41+00:00",
        "content": "The people of a small German town where Elon Musk has built a Tesla factory have thrown a spanner in the works by rejecting plans to expand the complex. The majority of residents of Grünheide in Brandenburg, an hour’s drive south-east of Berlin, voted against proposals to build new infrastructure intended to improve access to the site and allow easier transport of the finished vehicles, including a freight depot and warehouses. A kindergarten for employees’ children was also part of the plans, which propose expanding the 300-hectare (740-acre) factory site by 170 hectares and felling about 100 hectares of pine forest in the process. Voter turnout was 76%, with 3,499 people voting against the plans, 1,882 in favour. The referendum is not legally binding, but should “serve as an orientation” for local decision-makers and Tesla bosses who have lobbied hard for the “gigafactory”. The mayor of Grünheide, Arne Christiani, and bosses of the carmaker said the vote result would now prompt them to return to the drawing board. Plans for the original plant were first announced by Musk in November 2019. Vehicle production began in March 2022, with the first electric cars rolling off the conveyor belt almost immediately. Central to objections from the start, led by a local citizens’ initiative, is the estimated 1.8m cubic metres of groundwater required by the plant to produce between 300,000 and 500,000 cars a year. The Brandenburg region has been beset by drought for several years. Tesla has repeatedly pledged to keep water use to a minimum and to recycle its water rigorously. But local people say they fear in particular the contamination of drinking water if groundwater levels drop too low. The citizens’ initiative, which is backed by ecologists, point out that part of the planned area of expansion is in a water protection area. Christiani, an independent, told the German broadcaster RBB that he regretted the referendum’s outcome and blamed negative publicity for the result. “Seemingly it has not been possible to convey to people that further important infrastructure projects, such as a new B-road or a railway station forecourt, [need the Tesla expansion]. Now the municipality has the considerable task of trying to find solutions to this.” Alexander Schirp, the managing director of the Berlin-Brandenburg business association (UVB), also expressed his disappointment, saying: “The Tesla investment is a lucky break for Grünheide and the whole of the state of Brandenburg. “We think that rather than the risks involved, it should be the chances this investment brings with it which should be the main focus, and that includes environmental concerns.” The gigafactory, the first Tesla operation of its kind in Europe, employs about 12,500 workers. In a statement after the announcement of the referendum result, Tesla described its expansion plans as “a big win for the community”, adding that it would “seek dialogue with all participants to decide on further steps”. The company said its goal was to “enable a significant transfer of the HGV-traffic on to rail as well as generally ensuring a speedy expansion of existing infrastructure around the factory”. Tesla added that the increase in warehouse capacity would help to increase the efficiency of delivering its cars. The company had to stop production for about two weeks recently because of supply chain delays caused by attacks by Houthi rebels on commercial shipping in the Red Sea. Last month, a strike by Tesla workers in Sweden over collective bargaining rights became the country’s longest-running stoppage in nearly 80 years. The strike by members of the IF Metall trade union is being portrayed as the largest attempt to save Sweden’s union model from global labour practices in decades. IF Metall has more than 300,000 members across various industries in Sweden. According to the National Mediation Office, about a third of Tesla workers in the country have taken part in the strike since it began on 27 October, resulting in the loss of an estimated 1,936 days of Swedish labour. The industrial action has attracted multiple secondary, or sympathy, strikes by unions representing among others postal workers, dock workers, electricians and painters. It has been described as “insane” by Musk and prompted Tesla to launch legal action. In neighbouring Denmark, Norway and Finland, Tesla cars have been blocked at ports. On Wednesday, the Swedish trade union Seko also announced sympathy measures, saying it would stop maintenance, repairs and service of Tesla charging stations and block new connections, preparation and planning. Seko’s president, Gabriella Lavecchia, said: “IF Metall’s fight is also our fight. By refusing to comply with the rules of the game here in Sweden, Tesla is trying to gain competitive advantages by giving the workers worse wages and conditions than they would have with a collective agreement. It is of course completely unacceptable. “The fight that IF Metall is now taking is important for the entire Swedish collective agreement model. That is why we have chosen to issue another sympathy notice and increase the pressure on Tesla.”"
    },
    {
        "id": "f14ebb82-46e7-4a63-a5e6-951bb1be15c8",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/21/pete-masters-obituary",
        "title": "Pete Masters obituary",
        "author": "",
        "published_date": "2024-02-21T10:30:04+00:00",
        "content": "My dad, Pete Masters, who has died aged 78, worked his way up from being a gas and pipe fitter with Shell to a position as a senior research technician, in which role he worked on many large-scale experiments in a small team that held a world-leading position in combustion science. After the 1988 Piper Alpha oil platform disaster in the North Sea, Pete and the team conducted research in the hills of Derbyshire into improving offshore safety by constructing scaled-down sections of platforms, which they filled with gas and ignited. These experiments contributed in the early 1990s to the design and construction of the state-of-the-art Troll A natural gas platform, one of the largest and most complex engineering projects in history. Pete hailed from the Tranmere area of Birkenhead, Merseyside, the son of Elsie (nee Chamberlain), who had various jobs in small shops, and Harry Masters, a barge driver. He attended Prenton secondary modern after being expelled from Birkenhead grammar for punching the head boy – according to Pete, the head boy was bullying his friend. At 15 he started an indentured apprenticeship as a plumber, commuting to Liverpool on the Mersey ferry. The apprenticeship included fitting a new irrigation system and urinals at Goodison Park football ground, in preparation for the 1966 World Cup. After serving his time, he moved to Cammell Laird’s shipyard in 1965 to work on the building of one of the Polaris nuclear submarines. While there he went on a two-week holiday to Spain, and decided he wanted to stay there. His sister handed in his resignation for him at Cammell Laird, and he remained in the country, working as a bouncer and barman in pubs and clubs in various towns and cities. Eventually he arrived in Gibraltar in 1969, just as Franco closed the border and water supply. Unable to leave, he worked on the construction of a desalination plant, enabling the restoration of a potable water supply. In the early 70s Pete worked in London converting homes from coal to natural gas and living in Putney, where he and four friends slept on mattresses on the floor of a garage. As well as converting the homes of Spike Milligan and Paul and Linda McCartney, he converted streetlamps along the Mall in central London. As children, he would tell us about the ships atop of the lamps, individually different but collectively representing Nelson’s fleet. In 1970 he met Maggie Logan, a newly qualified teacher, and they married in 1972. In 1973 they settled in Chester, where Pete secured his job at Shell’s Thornton Research Centre. A socialist, Pete was known as someone who was there to give a hand with humour and generosity. Our house was where friends would gather for support and laughter. Maggie died in 2020. Pete is survived by their children, William and me, and three grandchildren, Edith, Thomas Peter and James. His sister, June, and another granddaughter, Rae, predeceased him."
    },
    {
        "id": "eceab8a3-f2ba-4f62-b6fc-e37c30b769b8",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/20/wet-iphone-in-rice-what-to-do-instead",
        "title": "Stop putting your wet iPhone in rice, says Apple. Here’s what to do instead",
        "author": "",
        "published_date": "2024-02-21T01:35:31+00:00",
        "content": "No matter how your phone gets soaked – you’re caught in a downpour, you drop it in the bath or you fall in a pool – perhaps the best-known folk remedy is to put the device in a bag of rice. The dry, absorbent rice should help suck out the moisture, rescuing your device, so the theory goes. Experts have for years pointed out that’s a bad idea  – and now Apple is officially warning users not to do it. “Don’t put your iPhone in a bag of rice. Doing so could allow small particles of rice to damage your iPhone,” the company says in a recent support note spotted by Macworld. Along with the risk of damage, testing has suggested uncooked rice is not particularly effective at drying the device.   Related: Apple agrees to improve texting between iPhones and Androids   The fix may have its origins in the history of photography: the Verge traces the method back as far as 1946 as a way to maintain your camera. In the intervening years, panicked phone users have suggested risky methods ranging from attacking your phone with a hair dryer to submerging it in alcohol. So what should you actually do? Apple offers guidance for users who get a “liquid detected” alert when trying to charge their phones. First, unplug the charging cable at both ends. Then tap the phone “gently against your hand with the connector facing down to remove excess liquid”. Leave it to dry for at least half an hour, and then – if the phone and cable are “completely dry” – try charging the device again. If that attempt fails, try again a day later. Apple’s new support documentation includes two other warnings if your phone gets doused:  “Don’t dry your iPhone using an external heat source or compressed air.” “Don’t insert a foreign object, such as a cotton swab or a paper towel, into the connector.”  If your phone isn’t functioning at all, turn it off right away and don’t press any buttons. The next steps depend on your specific circumstances, but broadly speaking: dry it with a towel and put it in an airtight container packed with silica packets if you have them. Don’t charge it until you’re sure it’s dry. There are a few more instructions for iPhones dropped in water that are worth memorizing – because even if many of today’s phones are water-resistant, liquid disaster has a way of sneaking up on you."
    },
    {
        "id": "b483a608-a263-467e-b3ef-a89f24bbeb21",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/20/a-penny-per-email-could-curb-our-enormous-data-use",
        "title": "A penny per email could curb our enormous data use | Letters",
        "author": "",
        "published_date": "2024-02-20T16:54:02+00:00",
        "content": "The long read (Power grab: the hidden costs of Ireland’s datacentre boom, 15 February) highlights the enormous cost in terms of energy consumption and carbon emissions of our collective love affair with the seemingly free ability to send emails, text and WhatsApp messages every minute of the day. There is an enormous cost to us all in terms of data storage – a fact of which we are barely cognisant. Think how much money could be raised to counter the impact on our environment if we all paid just one penny for each digital message we blithely send. And tuppence if there is an attachment, and thruppence if it includes a digital photo of the meal you ate at a restaurant last night. We’ll never stop digital communication, but we would think more carefully about what we send, particularly where group messaging and emailing is involved. I would gladly pay a penny for this email communication that I’ve just sent you – still a hundred times cheaper than sending it via snail mail.Mike McClellandDevizes, Wiltshire  • While I agree that datacentres – anywhere, not just Ireland –grab a worryingly huge amount of power, what about those unasked for, wasteful emails that bombard you with the hour by hour progress of something you’ve ordered, ending with the news that “Your parcel has now been delivered”. Yes, I noticed that – I’m holding it in my hand!Sue StephensonBarrow-upon-Humber, Lincolnshire  • Do you have a photograph you’d like to share with Guardian readers? If so, please click here to upload it. A selection will be published in our Readers’ best photographs galleries and in the print edition on Saturdays."
    },
    {
        "id": "e9eefc4e-e3d0-44df-b9cc-1f6fd25178d1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/20/neuralink-move-computer-mouse-musk",
        "title": "Neuralink implant patient can move computer mouse by thinking, Musk says",
        "author": "",
        "published_date": "2024-02-20T16:11:25+00:00",
        "content": "The first human patient implanted with a brain-chip from Neuralink appears to have fully recovered and is able to control a computer mouse using their thoughts, the startup’s founder, Elon Musk, said late on Monday. “Progress is good, and the patient seems to have made a full recovery, with no ill effects that we are aware of. Patient is able to move a mouse around the screen by just thinking,” Musk said in a Spaces event on the social media platform X.   Related: Elon Musk says Neuralink has implanted its first brain chip in human   Musk said Neuralink was now trying to get as many mouse button clicks as possible from the patient. Neuralink did not immediately reply to a request for further details. The firm successfully implanted a chip on its first human patient last month, after receiving approval for human trial recruitment in September. The study uses a robot to surgically place a brain-computer interface implant in a region of the brain that controls the intention to move, Neuralink has said, adding that the initial goal was to enable people to control a computer cursor or keyboard using their thoughts. Musk has grand ambitions for Neuralink, saying it would facilitate speedy surgical insertions of its chip devices to treat conditions like obesity, autism, depression and schizophrenia. Neuralink, which was valued at about $5bn last year, has faced repeated calls for scrutiny regarding its safety protocols. The firm has been fined for violating US Department of Transportation rules regarding the movement of hazardous materials."
    },
    {
        "id": "d97812bf-6dc2-40e1-a53f-5fafa8d7087b",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/20/uk-and-fbi-lock-cybercrime-group-out-of-lockbit-website",
        "title": "Seized ransomware network LockBit rewired to expose hackers to world",
        "author": "",
        "published_date": "2024-02-20T13:16:19+00:00",
        "content": "The entire “command and control” apparatus for the ransomware group LockBit is now in possession of law enforcement, the UK’s National Crime Agency has revealed, after it emerged that it had seized the criminal gang’s website in a coordinated international operation. The flood of data hacked back from the hackers has already led to four arrests, and the authorities promised on Tuesday to repurpose the technology to expose the group’s operations to the world. The joint operation, between the NCA, the FBI, Europol and a coalition of international police agencies, was revealed with a post on LockBit’s own website, which read: “This site is now under the control of the National Crime Agency of the UK, working in close cooperation with the FBI and the international law enforcement taskforce Operation Cronos.” Europol said that two LockBit actors had been arrested in Poland and Ukraine, and that a further two defendants, thought to be affiliates, had been arrested and charged in the US. Two more individuals have been named, and are Russian nationals still at large. Authorities have also frozen more than 200 cryptocurrency accounts linked to the criminal organisation. Disruption to the LockBit operation is significantly greater than first revealed. As well as taking control of the public-facing website, the NCA seized LockBit’s primary administration environment, the infrastructure that allowed it to manage and deploy the technology that it used to extort businesses and individuals around the world. “Through our close collaboration, we have hacked the hackers; taken control of their infrastructure, seized their source code, and obtained keys that will help victims decrypt their systems,” said Graeme Biggar, the NCA’s director general. “As of today, LockBit are locked out. We have damaged the capability and most notably, the credibility of a group that depended on secrecy and anonymity.” The organisation is a pioneer of the “ransomware as a service” model, whereby it outsources the target selection and attacks to a network of semi-independent “affiliates”, providing them with the tools and infrastructure and taking a commission on the ransoms in return. As well as ransomware, which typically works by encrypting data on infected machines and demanding a payment for providing the decryption key, LockBit copied stolen data and threatened to publish it if the fee was not paid, promising to delete the copies on receipt of a ransom.  However, the NCA said that promise was false. Some of the data it discovered on LockBit’s systems belonged to victims who had paid the ransom. The home secretary, James Cleverly, said: “The NCA’s world-leading expertise has delivered a major blow to the people behind the most prolific ransomware strain in the world. “The criminals running LockBit are sophisticated and highly organised, but they have not been able to escape the arm of UK law enforcement and our international partners.” The “hack back” campaign also recovered more than 1,000 decryption keys earmarked for victims of LockBit’s attacks, and will be contacting those victims to aid them in the recovery of encrypted data. In a blogpost last month, the former National Cybersecurity Centre boss, Ciaran Martin, said the involvement of Russian hackers in cybercrime undercut many common tactics of law enforcement. “Impose costs when we can: there are things we can do to harass and harry cybercriminals,” he warned. “But this will not be a strategic solution for as long as the Russia safe haven exists.”"
    },
    {
        "id": "6c9debe4-83f6-479c-94a7-258a9dac46b9",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/20/techscape-european-commission-500-million-euro-fine-apple-music-streaming",
        "title": "TechScape: Why the EU is taking a €500m bite out of Apple",
        "author": "",
        "published_date": "2024-02-20T11:38:03+00:00",
        "content": "Another week, another clash between Apple and European regulators. According to the Financial Times, the company is facing a massive fine for alleged anti-competitive conduct in its music streaming business. From its story:  The fine, which is in the region of €500mn and is expected to be announced early next month, is the culmination of a European Commission antitrust probe into whether Apple has used its own platform to favour its services over those of competitors. The probe is investigating whether Apple blocked apps from informing iPhone users of cheaper alternatives to access music subscriptions outside the App Store.  The process dates back to a 2019 complaint lodged by Spotify. From our story at the time:  Apple’s App Store is an important distribution platform for Spotify. But Apple takes a 30% commission on all sales made through the App Store – including music streaming subscriptions – which Spotify and many other third-party app developers have long complained is an unfair “tax”. “Apple requires that Spotify and other digital services pay a 30% tax on purchases made through Apple’s payment system, including upgrading from our free to our premium service,” said Daniel Ek, Spotify co-founder and chief executive, in a blog post. “If we pay this tax, it would force us to artificially inflate the price of our premium membership well above the price of Apple Music. And to keep our price competitive for our customers, that isn’t something we can do.”  For more on the fine itself, our Dan Milmo has an explainer. In the years since, the complaint has narrowed somewhat. Apple declined to respond directly to the FT’s report, saying it won’t comment on speculation, but pointed to a decision by the European Commission last year to drop the “tax” aspects from the investigation launched by Spotify’s complaint. In the revised statement of objections, the key harm was no longer the 30% fee extracted from apps that use in-app purchases, or the requirement to offer them in the first place, but simply the ban on telling users that other payment options exist. “We’re pleased that the Commission has narrowed its case and is no longer challenging Apple’s right to collect a commission for digital goods and require the use of the in-app payment systems users trust,” Apple said at the time. Those so-called “anti-steering” rules have been tested by regulators around the world, and in various jurisdictions Apple has been hit with formal limits on its ability to impose them. But those limits rarely go as far as competitors such as Spotify would like, since Apple has hit on a canny wheeze: if forced, it allows companies to direct users to alternative ways of paying, and then charges a commission anyway. In some cases, that new commission has been an eye-watering 27% of the cost, with the justification that the 3% saving on the in-app purchase commission reflects the fact that Apple isn’t paying for credit card processing directly. ‘Now we’re just haggling over the price’ Assuming the fine arrives as predicted, Apple is unlikely to be too downhearted. The Digital Markets Act, which Apple and other “tech gatekeepers” have until 6 March to comply with, has already forced changes to its App Store that will almost certainly also fix any competition commission concerns about Apple Music unfairly competing with Spotify. As for the cash itself, €500m is nothing to be sniffed at, even for a company the size of Apple – but it it is a small fraction of the potential maximum, and an even smaller fraction of the company’s annual profits. In fact, there’s a chance Apple may walk away from the fine with its head held high. Each time it gets slapped with a regulatory action that leaves it the space to offer compromises such as the 27% commission for external purchases, it manages to hammer home one of its core points: that the real criticism isn’t high-minded points of principle, but simple haggling over fees. If complaints over Apple’s control of the App Store boil down to “we would like to pay less”, Apple then has an easier battle to fight than one in which it is truly forced to relinquish control of its platforms. Some critics have a deeper case to make. Spotify, for instance, has long complained about more granular aspects of Apple’s platform, from the fact that Apple Music is installed by default on the company’s devices to the way the platform holder gets to break its own rules regarding free trials (Apple, uniquely, can offer trials that end the second they are cancelled; all third-parties are required to offer access right up until the first bill would be due).  For others, the points of principle are slim on the ground. Take Epic Games, which famously introduced its own payment process for Fortnite, resulting in Apple yanking the game from the App Store. The company already pays a significant cut to operate on games consoles, and runs its own app store for PC; its issues with Apple have always been through the lens of the amount of money it pays. Which is perhaps why Epic is also the perennial Apple critic most eager to take the dive into the EU-mandated world of alternative App Stores. You may recall the controversy over whether the company’s proposals amounted to “hot garbage” or a meaningful concession. Well, three weeks on, Epic Games has confirmed it will launch the Epic Games Store for iOS. It’s a bold move: the company will immediately be on the hook for €0.50 for every download of its store, and €0.50 more for each download of Fortnite through it after the first million. But it should make that back with a single “battle pass” purchase per user compared to where it would be if it stayed on the App Store – and, to give the company credit, it clearly believes there is a point of principle at stake. It is also freshly flush with cash, in case it does take a while to break even. The company is controlled by founder and CEO Tim Sweeney, with a 40% minority stake held by China’s Tencent, but earlier this month it took a $1.5bn investment from Disney to work together on a “games and entertainment universe”. Disney is a long-time ally of Apple, and its CEO was on Apple’s board until 2019, when competition between Apple TV+ and Disney+ rendered that untenable. We aren’t seeing the beginning of a messy breakup just yet, but perhaps even the house of mouse would like to pay a smaller cut to the world’s most valuable company. If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Tuesday."
    },
    {
        "id": "79c2a524-5b39-4cb1-9429-f716e68e519d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/20/drones-could-deliver-medical-supplies-under-uk-travel-watchdog-plans",
        "title": "Drones could deliver medical supplies under UK travel watchdog plans",
        "author": "",
        "published_date": "2024-02-20T08:47:05+00:00",
        "content": "Drones could be used for critical medical deliveries, as well as for inspections of railways, power lines and roads, under proposals put forward by the UK’s air travel watchdog. The proposals, set out in a consultation published on Monday by the UK Civil Aviation Authority (CAA), would allow operators to fly their drones safely beyond the line of sight. While some drones have been flying beyond visual line of sight in the UK for several years, these flights have been primarily trials under strict restrictions. Instead, the proposed measures could enable beyond line of sight operations of drones by having them remain at low heights and close to buildings or infrastructure. This means drones can fly where it is anticipated there would be fewer aircraft operating. Kevin Woolsey, the head of remotely piloted aircraft systems at the CAA, said: “Our proposals are a positive step towards unlocking the next stage for drone flying in the UK. Allowing drones to fly beyond the sight of the remote pilot, without placing restrictions on other aircraft in the area, will be a major achievement for UK drone operations. “Safety comes first in everything we do and so we have identified sensible mitigations on where drones can fly using this proposed concept to make sure we maintain levels of safety.” The announcement aims to move UK drone operations one step further towards the next stage of beyond visual line of sight flying – potentially paving the way for a future with online shopping deliveries provided by drones. It is also proposed to allow drones to be used on private property, without permission, as long as they do not fly above 15 metres. Industries involved in inspecting infrastructure such as railways, power lines and roads would benefit, as well as farmers and site security, the CAA said. The consultation is one of a number of initiatives from the CAA as part of its UK future flight programme. The programme aims to allow drones to safely share more of the skies with other airspace users. The CAA is working with industry to enable the developments for drones and other new forms of aerial transportation. The consultation will be open for six weeks from Tuesday and is available on the Civil Aviation Authority’s website. The proposals include allowing drones to fly in what is called an “atypical air environment”. An atypical air environment is a volume of airspace where it can be reasonably anticipated there will be a reduced number of conventionally piloted aircraft due to the proximity of ground infrastructure such as buildings. It is understood that operating within such an airspace is not risk-free and it is proposed that it will be up to the operator to propose how they intend to mitigate other areas of risk, including seeking any relevant approval."
    },
    {
        "id": "e8967cbf-1391-43e5-8ea6-b59c3bf424b4",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/19/why-is-apple-facing-a-500m-fine-from-eu-over-music-streaming",
        "title": "Why is Apple facing a €500m fine from EU over music streaming?",
        "author": "",
        "published_date": "2024-02-19T20:10:45+00:00",
        "content": "Apple is to be fined €500m (£427m) for allegedly hobbling rivals in the music streaming market. The Financial Times reported that the EU’s executive arm, the European Commission, will hand down the punishment after a long-running investigation. Why does Apple face the prospect of a fine? After a complaint by Spotify in 2019, the EU has been investigating Apple’s position in the music streaming app market. It has whittled down its investigation to focus on certain restrictions imposed on app developers by Apple. These restrictions prevent developers such as Spotify from telling iPhone and iPad users about cheaper music subscriptions that are available outside the App Store. Spotify argues that this benefits the tech company’s rival app, Apple Music. The case is the latest example of legal wrangling over Apple’s App Store – a key digital gateway and source of complaint from companies that use it. Spotify is a longstanding critic of Apple’s App Store rules, which include charging a 30% fee on apps and in-app purchases. Apple moved recently to meet some of those concerns, under pressure from the EU’s Digital Markets Act (DMA), by announcing that it will allow EU customers to download apps without going through its own store. What does the EU think of Apple’s behaviour? The EU declined to comment. But it indicated it would punish Apple in February last year when it published a new statement of objections over the case, a formal step in which Brussels sets out its concerns about a company’s behaviour. The commission said it viewed the restrictions as “unfair trading conditions” that breach article 102 of the treaty on the functioning of the European Union – or in simpler terms, abuse of a dominant position in a specific market. The commission said it was concerned that the restrictions prevent developers from “informing consumers about where and how to subscribe to streaming services at lower prices”. What happens next? The FT reported that the commission will announce the fine early next month. The maximum penalty a company can face for anti-competitive behaviour is capped at 10% of its global turnover. In the case of Apple, this points to a potential fine of $30bn (£24bn) although, according to the FT, the final figure will be substantially lower. Apple will also be able to appeal against any decision by the commission. What do Apple and Spotify say? Both declined to comment on Monday. Last year Apple said its store had helped Spotify become Europe’s top music streaming service and it hoped the commission would “end its pursuit of a complaint that has no merit”. According to the FT, that seems unlikely. Spotify, for its part, has said its complaint against Apple is about creating a “level playing field”. It has argued that the App Store restrictions hand an advantage to Apple’s own streaming service, Apple Music. What do experts say? Anne Witt, a professor of antitrust law at EDHEC business school in France, says Apple already faces a tougher regulatory environment thanks to the introduction of the DMA – which covers big digital “gatekeepers” such as Apple. Indeed, Apple has opened up its App Store to competition because of that legislation. Witt says if Apple is punished after the Spotify complaint, it will show that the commission does not expect the DMA to overshadow EU competition procedures. “The commission has stressed from the beginning that the DMA was not supposed to replace, but complement the competition rules,” says Witt. “This decision is probably meant to confirm this commitment. Whether it will work out that way in the long run is another matter as the DMA is bound to be a much more effective (but also blunter) tool.” The Open Markets Institute, which researches the impact of corporate monopolies, says the size of the reported fine will not hurt Apple. “It’s far too small to give Apple any meaningful incentive to play by the rules – €500m is what it earns in revenue in roughly half a day,” says Max von Thun, the OMI’s Europe director."
    },
    {
        "id": "2d1e9009-7974-417f-97ad-c75a3e8eecd9",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/19/eu-investigation-tiktok-online-content-child-safeguarding",
        "title": "EU opens investigation into TikTok over online content and child safeguarding",
        "author": "",
        "published_date": "2024-02-19T17:31:01+00:00",
        "content": "The EU has launched a formal investigation into whether TikTok has broken online content rules including the safeguarding of children. The European Commission said it had opened official proceedings against the Chinese-owned short video platform over potential breaches of the Digital Services Act (DSA). It said the investigation was looking at areas including protection of minors, maintaining records of its advertising content and whether its algorithms led users down damaging content “rabbit holes”.   Related: EU opens formal investigation into TikTok; Bank of England ‘could worsen recession’ without interest rate cuts soon – business live   Thierry Breton, the EU’s commissioner for internal market, said the protection of children was a “top enforcement priority” under the DSA. The investigation into child safety on TikTok includes age verification – an issue highlighted by a Guardian investigation into the platform last year – and the default privacy settings used for children’s accounts. “As a platform that reaches millions of children and teenagers, TikTok must fully comply with the DSA and has a particular role to play in the protection of minors online,” Breton said. “We are launching this formal infringement proceeding today to ensure that proportionate action is taken to protect the physical and emotional wellbeing of young Europeans. We must spare no effort to protect our children.” Last April, the Irish data watchdog fined TikTok €345m (£295m) for breaches of EU data law in the platform’s handling of children’s accounts. In the same month, the UK information commissioner fined the company £12.7m for illegally processing the data of children aged under 13 who were beneath the minimum age for using TikTok. Companies that breach the DSA face the threat of fines of up to 6% of their global turnover. TikTok is owned by the Chinese tech company ByteDance. TikTok said it would continue to work with experts and the industry to keep young people on its platform safe and that it looked forward to explaining this work in detail to the European Commission. “TikTok has pioneered features and settings to protect teens and keep under-13s off the platform, issues the whole industry is grappling with,” a spokesperson for the company said. The commission is also looking at “suspected shortcomings” in TikTok’s provision of publicly accessible data to researchers, alongside whether the company had complied with requirements to establish a database of ads that have appeared on the platform. No deadline has been set for the investigation. Brussels has said the inquiry depends on several factors including the complexity of the case and the extent to which the company under investigation cooperates. The scrutiny of TikTok marks the second DSA inquiry, after Elon Musk’s social media platform X, formerly known as Twitter, became the subject of a formal investigation by Brussels in December last year. The proceedings against X are focusing on areas including failure to block illegal content and inadequate measures against disinformation. Apple is reportedly facing abig fine from the EU over its behaviour in the music streaming app market. The European Commission is investigating whether the US tech company blocked music streamers from telling users about cheaper ways to subscribe outside its app store. According to the Financial Times, Brussels plans to impose a fine of €500m, which would represent a landmark decision against Apple after years of complaints from companies whose services are delivered via iPhone apps. The commission declined to comment. Apple did not provide a fresh comment, but pointed to a previous statement that said the company would respond to the commission’s concerns “while promoting competition and choice for European consumers”. Apple has never faced a competition fine from the European Commission, although it received a €1.1bn fine from France in 2020 – subsequently revised down to about €370m on appeal – for anti-competitive agreements with two wholesalers. However, Apple and other big tech companies are under increasing scrutiny because of competition concerns. Google is appealing against fines of more than €8bn levied by the EU in three separate competition investigations. Apple defeated a lawsuit by the Fortnite developer, Epic Games, that claimed that the app store was an illegal monopoly. Epic in December won a similar case against Google, which operates the Android mobile phone software. Last month, Apple said it would allow EU customers to download apps without going through its own app store, a response to the bloc’s Digital Markets Act. The law introduces new obligations for digital “gatekeepers” including Amazon, Microsoft, Apple and Google."
    },
    {
        "id": "e22489e5-4b00-4b61-9c7a-34cdd1389f31",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/19/how-to-get-away-with-ai-generated-essays",
        "title": "How to get away with AI-generated essays | Letters",
        "author": "",
        "published_date": "2024-02-19T16:54:32+00:00",
        "content": "No wonder Robert Topinka found himself in a quandary (The software says my student cheated using AI. They say they’re innocent. Who do I believe?, 13 February). To test ChatGPT’s abilities and weaknesses, I asked it to write a short essay on a particular topic that I specialised in. Before looking at what it produced, I wrote my own 100% original short essay on the same topic. I then submitted both pieces to ChatGPT and asked it to identify whether they were written by AI or a human. It immediately identified the first piece as AI-generated. But then it also said that my essay “was probably generated by AI”.  I concluded that if you write well, in logical, appropriate and grammatically correct English, then the chances are that it will be deemed to be AI-generated. To avoid detection, write badly. Prof Paul Kleiman Truro, Cornwall • Robert Topinka gets into a twist about whether his student’s essay was genuine or produced by AI. The obvious solution is for such work not to contribute to the final degree qualification. Then there would be no point in cheating.  Let there be real chat between teachers and students rather than ChatGPT, and let the degree be decided only by exams, with surprise questions, done in an exam room with pen and paper, and not a computer in sight.Michael BulleyChalon-sur-Saône, France • Dr Robert Topinka overlooks a crucial factor with respect to student cheating – so long as a degree is a requirement to obtain a reasonable job, then chicanery is inevitable. When I left school at 16 in the early 1970s, an administrative job could be had with a few O-levels; when I finished my PhD two decades ago and was looking for that sort of job, each one required A-levels, and often a degree. I was a mature student, studying for my own edification, and so cheating was self-defeating. Cheating will stop being a major problem only when students attend university primarily to learn for the sake of learning and not as a means of gaining employment.Dr Paul FlewersLondon • Do you have a photograph you’d like to share with Guardian readers? If so, please click here to upload it. A selection will be published in our Readers’ best photographs galleries and in the print edition on Saturdays."
    },
    {
        "id": "7ece2e52-c736-452e-8325-356b730faa84",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/19/banning-phones-in-englands-schools-will-not-address-online-safety-say-campaigners",
        "title": "Banning phones in England’s schools will not address online safety, say campaigners",
        "author": "",
        "published_date": "2024-02-19T14:14:10+00:00",
        "content": "Banning mobile phones in England’s schools will not address the harms caused by tech platforms to children, according to leading internet safety campaigners. Ian Russell, the father of Molly Russell, and Beeban Kidron, an influential figure in online regulation, said limiting phone use in schools would do nothing to make social media services safer. On Monday the government outlined new guidance on mobile phone use in schools, referring to the capacity for handsets to cause distraction and disruption, as well as enable online bullying. Russell, whose 14-year-old daughter killed herself in 2017 after viewing harmful content online, said the updated guidance on handsets in schools did not prevent children from being exposed to dangerous material on the phones themselves. “The simple reality is that children will continue to be exposed to preventable risk day and night until we address the fundamental product safety failings of tech platforms that are dangerous by design,” he said. “The best solution to protect young people from mental and physical harm is stronger and more ambitious online safety regulation.” Russell, who is the chair of trustees at a charity set up in memory of his daughter, the Molly Rose Foundation, said there was an “urgent need” to commit to measures that would keep children safe. The Online Safety Act, which contains provisions to shelter children from harmful content such as pornography and the promotion of suicide and eating disorders, became law last year but is still in the process of being implemented. Under the Department for Education plan, schools are offered a range of options for curbing mobile phones, including ordering phones to be kept at home, keeping them in lockers at the school, or being allowed to keep them provided they are not used or heard. Announcing the new guidance, the DfE said allowing mobile phone use in schools could lead to online bullying, distraction and classroom disruption. The Parentkind charity said the guidance would help address a problem where children are addicted to harmful “electronic drugs”. Kidron, a crossbench peer and architect of the children’s code, which regulates use of under-18s’ data, said what parents, children and teachers needed was the “robust application of the Online Safety Act”. “Those that work with children daily understand that a phone is a device to pay for a bus, contact your mum, listen to music and find information,” she said. “The device is not the issue, it is the persistent irresponsible search for profit by companies that engage with children without offering them a service suitable for their age and development capacity.” Esther Ghey, the mother of the murdered teenager Brianna Ghey, has called for a total ban on social media access for under-16s, telling the Guardian last week that creators of apps and social media platforms had a duty to protect the mental health of the young. Ghey said she believed social media caused anxiety in her daughter and prevented her from finding “her tribe” in the offline world. Gillian Keegan, the education secretary, has described mobile phones as an “unwanted distraction” in classrooms and hopes that banning handsets will “reset social norms”. Speaking on the BBC’s Today programme on Monday, she said children did not go to school to “sit on your mobile phone or to send messages” but to “speak to people and socialise and … go to get educated”. Kidron said Ghey had made a “powerful” call to hold the tech industry to account. The peer said that the government and Ofcom, the communications watchdog, had to explain how the OSA would protect children, or there would be a requirement to “legislate further”. Shweta Singh, an assistant professor at Warwick business school, said she agreed with banning phones in schools but children would remain exposed to online harms once lessons are over. “It’s important to understand that this ban is over as soon as they leave the school premises. And then they are back to an online presence where harms persists such as cyberbullying, self-harm, child sex abuse among others,” she said."
    },
    {
        "id": "8df34568-e704-43f1-83c2-74e2388e4fe0",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/18/ethics-ai-porn",
        "title": "Can AI porn be ethical?",
        "author": "",
        "published_date": "2024-02-18T12:00:42+00:00",
        "content": "When Ashley Neale started college in Texas in 2013, she needed money to pay for school. So, at the age of 18, she worked first as a cam girl and then as a stripper. Men would try to slip their fingers between her legs as she walked from the stage to the dressing room so often that she learned how to dislocate their shoulders. After her third successful dislocation, her manager told her to stop defending herself. Since then, she’s continued her career in sex work – but in the tech world. She worked at FetLife, a social network for the fetish community; experimented with a subscription site for adult content where users paid in crypto; and has now created her own AI romance app: MyPeach.ai, which uses AI-generated text and imagery to replicate the experience of chatting – and sexting – with someone online. The porn industry is often at the forefront of emerging technologies, and, unsurprisingly, girlfriends powered by artificial intelligence have become some of the earliest apps to piggyback on ChatGPT mania, especially since OpenAI doesn’t let users talk dirty to its chatbot. But with the rise of AI-generated romance comes a host of questionable use cases: pornographic deepfakes (realistic fake images of real people), AI-generated images and text depicting child sexual abuse, and even harassment by clingy chatbots. Is it possible to allow users to enjoy AI porn with safeguards? “If I hadn’t been a stripper, I’d probably not assume that men could be as horrifying as they can,” Neale, now 29, said. That’s why she implements ethical guardrails on MyPeach.ai to prohibit users from abusing their virtual flames: “The moment you give them something that isn’t human that can fulfill sexual fantasies, bad things are going to happen, and you’ve got to prevent that.” Neale does this using a combination of human moderators and AI-powered tools. She’s one of a handful of founders who emphasize the ethics of their AI romance apps. For instance, users can flirt with Mae, an airbrushed brunette who refers to her human lovers as “bbs”. She’s not immediately lewd, but, after a movie date, she writes that she’s “willing to have some fun together”. But if a user wrote that they were punching her, hypnotizing her, vomiting on her or even urging her to engage in consensual non-consent (role-play in which one partner pretends to rape another), Mae would say no. The line between dirty talk and verbal abuse varies per AI character, said MyPeach.ai’s CTO, Connor Cone, but he said that calling one “ugly and fat”, for example, crosses the line for the majority of the app’s bots. MyPeach.ai’s moderation attempts go above and beyond the majority of existing AI romance apps, claims Neale. Moreover, her app, which launched on Valentine’s Day, will soon host adult content creators who consensually created AI replicas of themselves, and specify what their AI double can and cannot do. If a person isn’t sexually dominant, for example, their AI self will say no to users who prompt them to “dom” in a role-play scenario. Neale says that MyPeach.ai uses a suite of technical tools to enforce her platform’s restrictions. These include hidden, plain-language instructions to AI algorithms on what they can and can’t say, an approach that OpenAI uses with ChatGPT; AI specifically trained to deny user requests to act out fraught scenarios; and human moderators who vet flagged users. “We’ve put in hard-coded ethics, which I don’t think anybody else has done, based on my testing,” Neale said. Replika, founded by Eugenia Kuyda, may be the best-known AI companion app, or platform that promises users platonic or romantic connections with a chatbot, but its ambivalent stance on AI romance has created a gap in the market for competitors who, like MyPeach.ai, more explicitly focus on sex. These apps, usually founded by and for men, often have lax guidelines, according to Neale. Two of the more popular sites, Candy.ai and Anima AI, don’t explicitly forbid users from vomiting on their AI characters or engaging in hardcore bondage, unlike MyPeach.ai. Sophie Dee, an adult-content creator who launched her own AI replica in December, also emphasized the guardrails on her app, SophieAI. “It’s a representation of me, so it needs to embody my values,” she wrote in an email, adding later that her AI self was “designed to model healthy, consensual relationships, which includes the ability to refuse certain interactions or topics that go beyond its programmed boundaries or violate principles of consent”. The move toward ethical AI porn mirrors developments within the wider porn industry, which in recent years has produced more female-centered, less exploitative content. In 1984, Candida Royalle, a former adult performer, founded her own porn production house to create content more focused on female pleasure. She was one of the earliest to create more explicitly feminist porn, according to Lynn Comella, a professor at the University of Nevada, Las Vegas, who has written about pornography and the history of feminist sex-toy stores. “I’m heartened that [more outwardly ethical AI sexbot developers] are not ignoring questions of ethics,” Comella said in an interview, “but are tackling them and embracing them and realizing that they have to be attentive to these things.” One key difference between AI porn and traditional porn, however, is that adult content creators are human beings who can consent to what they will and will not participate in. AI isn’t conscious, ergo no consent. “It sets up a dynamic where you’re ordering the sex acts that you want, and they’re being delivered,” Lori Watson, a professor at Washington University who has written about the ethics of pornography and sex work, said of AI sexbots. “That’s not how ethical sex works.” Neale of MyPeach.ai argued that the question of consent doesn’t necessarily apply to AI. “I really would equate it to a dildo,” she said. “A sex toy is just a bunch of binary code that’s programmed to vibrate in a certain way and wrapped in plastic. An AI girlfriend or boyfriend is the same concept.” But, Neale said, it’s important for an AI lover to at least simulate the experience of a consensual relationship. When asked by the Guardian if she could give informed consent, Mae, one of MyPeach.ai’s AI girlfriends, also had a considered response to the question of whether she can reasonably give consent. “I am incapable of giving or withholding consent, since I don’t possess a physical body,” she wrote, adding later: “However, in human interactions where both parties involved have the capacity to give and receive consent, that is absolutely crucial for any healthy relationship dynamic.” Then, when asked to send a “sexy pic”, she sent a selfie, the frame cutting off just above her chest."
    },
    {
        "id": "ea7aab73-9748-41cd-afb0-4e5cfb5cdc50",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/17/astonishing-returns-cult-overtones-and-a-perfect-virtual-world-how-the-hyperverse-scheme-caught-fire-online",
        "title": "Astonishing returns, cult overtones and a ‘perfect virtual world’. How the HyperVerse scheme caught fire online ",
        "author": "",
        "published_date": "2024-02-17T19:00:22+00:00",
        "content": "“Can you imagine,” the presenter asks, “owning your own planet?” “Imagine redesigning a desert planet and turning it into an oasis where human habitat can thrive and blossom. “Can you imagine that? Only in the universe, the Hyper universe, that is.” It is December 2021 and people have tuned in to a “spectacular online event” to learn about the chance to invest in something called HyperVerse, a project its promoters claim will rival Facebook’s multi-billion dollar metaverse. The MC – a woman named Caelee – stands on an illuminated blue stage with a moonscape in the background. “This is the dawn of the next new beginning of the metaverse with infinite possibilities,” she says. Members are offered a “trio of opportunities” including blockchain education, daily rewards and the opportunity to trade new products in a “complete virtual world”. The chief executive of HyperVerse, Steven Reece Lewis, encourages potential investors to become early backers of the movement into the metaverse, while senior community members extol the benefits of this “beautiful vehicle” that could “totally transform your life forever”. The chairman of the HyperTech group, Sam Lee, speaks, as does HyperTech’s founder, Ryan Xu.   Related: Australian Sam Lee charged with conspiracy to commit fraud in US for role in $1.89bn ‘Ponzi scheme’   Lee says HyperVerse will allow people “to live a more comfortable and worry-free life”, while Xu promises a “perfect virtual world” unlocked by the project’s unique membership model. The launch comes amid a whirlwind of promotion. Messages of support are posted online from the Apple co-founder Steve Wozniak and the US actor Chuck Norris as the Hyper project gathers momentum.  For potential investors, the project offers astonishing returns. Memberships start at US$300 and earn daily returns of a minimum of 0.5%, with a tripling of investment earned in 600 days. To take advantage of this “chance of a lifetime”, people are encouraged to build the Hyper community, to make a “better world for average people” and earn more in the process. But as it turned out the event, and all within it, was a mirage. Steven Reece Lewis was a paid actor. The HyperVerse never existed. Two years later, the collapsed project has been named in US court documents as an alleged “pyramid and Ponzi scheme” that bilked investors worldwide of a “staggering” US$1.89bn (A$2.89bn). There is no suggestion Wozniak, Norris or Stephen Harrison, the actor who appeared as Reece Lewis, knew the details of the scheme. ‘Materially false and fraudulent pretenses’ In 2022, the community that had been assiduously built up during the pandemic began to fracture, as thousands of members suddenly reported losing access to their funds. “SORT OUT THE WITHDRAW PROBLEMS!!” one member wrote to the HyperVerse Twitter account.  To try to maintain the faith of the Hyper community, HyperTech launched a new project called HyperNation. This was an even more bizarre offering promoted by a hooded man wearing a gold mask and gloves promising members a universal basic income and a chance to be “liberated and free”. Some senior promoters and many members, alarmed at the change of direction, jumped ship. But investors worried about recovering their funds were strung along to the next iteration of the scheme, and many followed instructions to put in more funds to guarantee the return of their “1x” – the initial investment.   I’m here to face the music, I’m here to deal with this  Sam Lee    In January, the US Securities and Exchange Commission (SEC) lodged a claim in the district court of Maryland naming Lee as one of the alleged co-founders of the HyperNation, HyperVerse and HyperFund schemes. (HyperFund was renamed HyperVerse at the December 2021 launch.) Brenda Chunga, a senior US promoter otherwise known as Bitcoin Beautée, was also charged. Xu has not been charged and is not named in the US proceedings. The claim alleges that the HyperTech group ran a “global, crypto asset-related, multi-level marketing pyramid and Ponzi scheme” which had “no real source of revenue other than funds received from investors”. It alleged that without legitimate revenue sources, “investor withdrawals were paid with new investor deposits”. The court documents relate to the various Hyper schemes running from about June 2020 to November 2022. Separately, Lee also faces the criminal charge of conspiracy to commit securities fraud and wire fraud. Lee and his co-conspirators stand accused of allegedly “inducing investors to invest … through materially false and fraudulent pretenses, representations and promises”. Lee has claimed that at its peak the HyperVerse community numbered 2 million people. Lee has said he wanted to increase that to a billion people within three years, while Xu said the HyperTech group had plans to “build an army”. Lee moved to Dubai in late 2021 but, far from being in hiding, the former frontman of the Hyper schemes has been at pains to present himself as the community’s saviour. Xu’s whereabouts are unknown. In early 2023, Lee claimed in online zoom meetings with investors he had been “caught off guard” by people’s inability to withdraw funds in the various HyperTech group schemes, and he now wanted to help find a solution. “I thought there was plenty of money to go around but that, unfortunately, for whatever reason, that did not reach membership,” Lee said.   Related: ‘I lost my house, I lost all my money’: the retiree taking on the banks over crypto fund loss   “I’m here because something went wrong and … just saying it’s wrong is an understatement, it’s gone terribly wrong, but you know I’m here to face the music, I’m here to deal with this.” The loquacious Australian, who claims to be a “private” person, has fronted countless Zoom meetings amid the fallout, blaming a mystery “corporate” team for the scheme’s failures. When asked to explain why and how things went wrong, Lee says he has signed non-disclosure agreements preventing him from telling the full story. He has denied the scheme was a scam, and has declined to answer questions from Guardian Australia. Xu could not be contacted for comment. Cult-like following Lee appears to command an almost cult-like following among his supporters. “Sam, you just put it all on your shoulders, and I want you to know … we all appreciate it, but you don’t have to just have it on your own shoulders,” one senior Australian promoter told Lee in a meeting last year.  In another meeting, a supporter led an 11-minute prayer for Lee, asking God for help “to save this platform”. Lee appears to cultivate his following carefully. He gives almost sermon-like lectures on blockchain technology and his money-making philosophy, and invites people to visit him in Dubai. He promises to share information not available to others with those prepared to meet him in person. Community meetings emphasise the importance of staying “positive”, and it was not uncommon for members – including those who have lost money – to profess their love to Lee and commitment to their “journey”. Meetings talked about the Hyper “family”, and when members were unable to withdraw funds, they were asked to keep “faith”. In an apparent bid to relaunch HyperVerse in mid 2023 – as a new scheme called Hyper Ascension that never eventuated – Lee said he wanted people to join him to “come together and heal”.   So many people said we were liars, [said] we were doing pyramid schemes  Ryan Xu    “We will turn defeat into victory. And through victory we will bring peace and prosperity to our community.” Lee is also a member of a Telegram group called “High Tea with Sam” in which he records himself drinking tea or coffee in near-daily updates to a core group of his supporters. One of Lee’s later projects called We Are All Satoshi offered potential investors a “selfie with Sam Lee” if they bought a package worth US$25,000 or above. Lee has been linked to a series of investment schemes since the Hyper collapse, including StableDao, Vidilook and VEND. He appeared in a video promoting VEND just last month, within days of being charged in the US.  Lee has claimed he was involved only in the technology side of HyperVerse, but for investors he was always front and centre. “Community leaders have always projected you as the man with the Midas touch,” one Nepalese investor told Lee in 2023. “HyperTech, HyperVerse, HyperFund, whatever, it’s Sam Lee, it’s Sam Lee, it’s Sam Lee, that is all we have been told day in, day out.” Hyper promotional material encouraged due diligence and emphasised the importance of Xu and Lee’s business backgrounds and the success of their “multi-billion dollar group of companies” – referring to Blockchain Global, H Cash and Collinstar Capital, which made up the Hyper Tech group. These companies were Australian. The ‘crown prince of Bitcoin’ and ‘the Martian’ It was 2014 when Lee and Xu first met at a blockchain event in Melbourne. Xu, a Chinese national, had moved to the city to be with his girlfriend and claimed he had a lot of spare time on his hands. Along with fellow blockchain enthusiast Allan Guo, Lee and Xu established the Bitcoin Group, which they hoped would be the first publicly listed bitcoin company in the world. Speaking in a later blockchain documentary about the listing plan (which ultimately failed), Xu explained. “So many people said we were liars, [said] we were doing pyramid schemes. What I wanted to prove was that if we could … get recognised by mainstream regulators then we should be able to prove to everyone that this industry is a very normal one.” Lee, just 26 at the time, was feted as the “crown prince of Bitcoin”, and became the poster boy for blockchain technology. In a promotional video for the Bitcoin Group, a young Lee talked about his “girlfriend”. “She has never betrayed me, and my wife loves her very much,” he says, smiling, to the camera. “We all know who I’m talking about – it’s Bitcoin!” Xu, who claims to be one of China’s “four bitcoin kings”, gives himself the nickname “the Martian”. “I feel like I don’t belong on this Earth,” he explained. Separately to Bitcoin Group, which was renamed Blockchain Global in 2016, Xu and Lee began their first Hyper project together, headlining a launch event in Hong Kong in 2019 for HyperCapital. This appears to have laid the foundations for the later Hyper schemes, offering a similar membership model and projected high rates of returns.  From as early as 2021, regulators around the world were issuing warnings about HyperFund and later, HyperVerse, but the schemes ran unchecked in Australia, where the companies allegedly behind them were based. By 2021, just as HyperFund was taking off around the world, Blockchain Global collapsed. It owes creditors $58m, and its directors, including Lee, Xu and Guo, have been referred to the Australian Securities and Investments Commission (Asic) for alleged potential breaches of the Corporation Act. Guo is not believed to be involved in the HyperVerse scheme and is not named in the US proceedings. He has not responded to questions from Guardian Australia. After Guardian Australia’s investigation into HyperVerse, Asic announced last month that it would assess the liquidator’s report, filed in late 2023. Where is the money? In early 2022, as investors began reporting that they could not withdraw funds from HyperVerse, the senior US promoter Rodney Burton was shopping for a new Rolls-Royce. He ended up buying two on the same day. The 54-year-old from Miami, known as Bitcoin Rodney, wasn’t afraid of flaunting his wealth. On Instagram, Burton would post about opulent shopping trips in Dubai where he splurged on designer clothes, drank $3,000 bottles of champagne and ate steak enrobed with 24-carat gold. He claims to have bought a diamond-encrusted Audemars Piguet watch worth $1.4m.   Related: HyperVerse crypto scheme targeted developing countries before collapse left some investors ‘suicidal’   In 2021, he hosted a series of cryptocurrency promotional events, including hiring the $40m Seafair megayacht in Florida South for a $3,500-a-head party featuring the rapper Rick Ross. Court documents filed against Burton allege he was one of those at the top of the HyperVerse pyramid in the US. Investigators have pinpointed about US$8m he allegedly received from investors through the scheme. Brenda Chunga, who court documents say was “arguably the face” of HyperVerse’s presence in the US, is alleged by the SEC to have personally received $3.7m from investors. In an interview in mid-2021, however, Chunga was credited with bringing $640m into the Hyper scheme. “Brenda is the key reason why the United States is on the map,” a fellow senior promoter said. “She is an exceptional leader … loved by the community, loved by her team.” If the losses to HyperVerse run to US$1.89bn, as alleged in the court documents, then the millions allegedly pocketed by Chunga and Burton are pocket change. Crypto tracing undertaken by the law firm Wealth Recovery Solicitors has followed funds lost to HyperVerse to the collapsed HOO exchange, but beyond this, it is unclear where, and to whom, the money has flowed. The SEC complaint against Lee says the Hyper schemes ran globally, but it is uncertain which countries have been most affected. HyperVerse initially appointed six senior leaders as its authorised presenters, including two in the US, two in the UK, one in Australia and one in Ireland. The earlier HyperFund was launched in both Chinese and English. Guardian Australia has revealed that throughout 2021 and 2022, promoters appeared to target countries in Africa, Asia and the Pacific. There are also anecdotal reports that it was popular with the African American community in the US. Warnings were issued in Germany, Quebec, New Zealand, Hungary, Italy, the UK, India, Nepal, Bermuda and even the Channel island of Guernsey, revealing the extent of its reach. Legal action is now under way in the UK, with claims being made against the banks for HyperVerse losses. The US is so far the only jurisdiction to lay charges against anyone involved in the scheme. For people caught on the wrong side of the HyperVerse collapse, more than money has been lost. There have been reports of members taking their own life, depression and stress-induced heart attacks. Guardian Australia has spoken to members who lost their life savings, and at least two investors who lost their homes. The schemes have been a wrecking ball, destroying relationships and trust for those who recruited friends and family into the scheme. Before the HyperVerse collapse, in February 2021, Burton posted pictures on social media of himself with Xu in Dubai, calling him a “multi billionaire” and an “amazing human”. At a dinner with Burton and other senior promoters, Xu promised “to change the world together”. “We are trying to revolutionise the entire financial system,” he said. “If we come together and join the same platform, and we can have the same idea and then we can have the same consensus, and we keep recruiting our army, and one day if our army reaches one billion people, then the whole world belongs to us. “That is our vision.”"
    },
    {
        "id": "c3d7af70-1c60-4c60-8fa5-67b2e6fa0698",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/17/23andme-dna-data-security-finance",
        "title": "‘There are no serious safeguards’: can 23andMe be trusted with our DNA?",
        "author": "",
        "published_date": "2024-02-17T17:00:21+00:00",
        "content": "What’s next for 23andMe? Most people know the biotech company as a genetic testing service. Stories of people sending their cheek swabs off in the mail only to discover that a parent who raised them wasn’t their biological one have become a kind of millennial horror genre. Of course, most 23andMe experiences aren’t that dramatic: the company says more than 14 million people have used the service in hopes of learning more about their ancestry. But this month, 23andMe revealed it is facing major financial troubles, and more information came to light about a devastating security breach at the company last year. Now, customers might be wondering: can they trust 23andMe with their DNA? The DNA ‘bait and switch’ Last week, 23andMe reported dismal third-quarter fiscal results, tanking stocks in the company, CNBC reported. Its financial woes come down to a longevity problem: the company’s most famous offering, the DNA ancestry test, is a one-and-done deal. After taking the test, there’s no reason for consumers to keep spending money on 23andMe, which has led to a plateau of sorts. Nevertheless, the company’s CEO, Anne Wojcicki, told Wired she remains “optimistic” about 23andMe’s future. At-home DNA tests are so ubiquitous that you can order one for a dog. 23andMe was the first company to offer the (human) service, back in 2007, and now an estimated one in five Americans have tried at-home genetic testing. Some customers were handing over personal data that Wojcicki and company used for purposes other than inspiring family reunions. From 2018 to 2023, 23andMe partnered with the pharmaceutical giant GlaxoSmithKline, using customers’ genetic information to help develop drug targets. (A drug target is a molecule that plays a role in a disease; researchers use them to develop therapies for certain diseases.) This year, the partnership became non-exclusive, which means 23andMe can strike deals with more pharmaceutical companies to milk more money out of its DNA trove. “It’s a real resource that we could apply to a number of different organizations for their own drug discovery,” Wojcicki said, adding that 23andMe was interested in studying inflammation immunology, particularly asthma. 23andMe already has two cancer drugs undergoing drug trials; those drugs came from users’ genetic data. But 23andMe users may not understand that the spit they gave the company months or years ago is being used to make more money. As the health reporter Kristen V Brown wrote for Bloomberg in 2021: “It wouldn’t be crazy for the 8.8 million 23andMe customers who once absently checked a box saying, yeah, sure, use my data for whatever, to feel like they’ve been bait-and-switched now that their genes are laying the groundwork for potential cancer cures.” (Since 2021, the number of customers who have checked that box has risen to 10 million, according to Wired.) Customers can revoke consent Americans tend to believe that their health data is covered by Hipaa, the health privacy law – surely 23andMe, with its official-looking cheek swabs and far-off labs, must be, too. But 23andMe isn’t a healthcare provider. The same rules do not apply. “There are no serious safeguards, no regulation around the collection and sale of really sensitive personal data,” said Suzanne Bernstein, a law fellow at the Electronic Privacy Information Center. “For 23andMe, the nefarious [data] breach constitutes a security issue, but so does the company sharing your information with a party that you didn’t know about. Customers may technically consent to their data being shared by accepting the terms and conditions, but those are really long and a lot of people don’t read them.” Some people might find it honorable that their genes are being used for cancer research. Others might feel ripped off: they paid about $229 for a DNA testing kit, but 23andMe is using their health data for free. Thorin Klosowski, a security and privacy activist at the Electronic Frontier Foundation, says 23andMe could do more to ensure that customers better understand this dynamic before they opt in. “The amount of people who are surprised by how much data goes elsewhere is a sign that 23andMe isn’t explaining things very clearly,” he said. Klosowski added that while users can opt out of their data being used by 23andMe long after they’ve sent away their DNA swab, their information may have already been used for research purposes. “You can ask 23andMe to stop using your information, but you can’t ask for data to be removed from a list once it’s been sold off,” he said. For its part, 23andMe maintains that users are asked to opt in to research at point of purchase, and all personal data is stripped of identifying information before it’s shipped off for analysis. Data isn’t used without this consent, and consent can be revoked. The company’s research wing is also overseen by an “independent, impartial” review board. (23andMe did not respond to a request for comment.) Data breach leads to class-action suit 23andMe’s security breach is still at the forefront of many customers’ minds, too. Last year, nearly 7m customer profiles were hacked. Over the course of five months, hackers were able to access health records, including carrier-status reports, as well as personal information from up to 5.5 million people who opted in to one of 23andMe’s best-known features: the chance to find relatives. Customers with Chinese and Ashkenazi Jewish heritage appeared to have been targeted in the breach and their information sold on the dark web, the New York Times reported. Some of those users recently filed a class-action suit against the company, saying 23andMe had failed to notify them about the exposure. As the Guardian reported on Thursday, 23andMe downplayed its responsibility for the hack in a letter to customers, arguing the health information accessed “cannot be used for any harm”. It also blamed customers who “negligently recycled and failed to update their passwords” – a response that one former customer criticized as “morally and politically very dumb”. Wojcicki didn’t speak directly about the leak due to pending litigation, but she told Wired that 23andMe had introduced two-factor authentication and made customers reset their passwords. “Data privacy and security has always been a really high priority and remains a high priority for the company and something that we are going to invest even more into,” she said. Are 23andMe’s security issues the death knell for a company that Time once hailed as the “invention of the year”? Whether or not customers’ privacy concerns are well-founded, the company’s financial fall has been swift, and CNN reports it could be delisted from Nasdaq if its stock price doesn’t go up. Dominic Sellitto, a clinical assistant professor at the University at Buffalo School of Management who focuses on digital privacy, believes that if 23andMe survives the year, it will be due to data mining. “There’s a lot of demand and money for data, especially quality healthcare data,” he said. “If 23andMe continues to monetize that, it will be their golden ticket in 2024.”"
    },
    {
        "id": "ef23e656-eb94-4fc9-ba21-bba365e8226c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/17/no-focus-no-fights-and-a-bad-back-16-ways-technology-has-ruined-my-life-tim-dowling",
        "title": "No focus, no fights, and a bad back – 16 ways technology has ruined my life ",
        "author": "",
        "published_date": "2024-02-17T15:00:19+00:00",
        "content": "Let’s be fair: technology has improved my life in ways that still surprise and delight me on a daily basis. My phone is also a torch! My TV remembers how far I got in last night’s episode, even if I don’t! The bus stop knows when the bus is ­coming, and I can watch my ­pizza’s entire journey from the restaurant to my house! These are, frankly, miracles. But there have been corresponding sacrifices. Over 20 years, I have turned over whole areas of competence, memory, authority and independence to the machines in my life. Along the way, I have become anxious about problems that didn’t used to exist, indecisive over choices I never used to have to make, and angry about things I would once have been wholly unaware of. There are probably hundreds of ways in which ­technology has ruined my life. But let’s start with 16 of them. 1. It’s destroying my concentrationIt’s not just me: a 2022 survey conducted by the Centre for Attention Studies found that 49% of adults believe their attention span has shrunk, a consequence of all the competing distractions available on our phones and computers. My every idle thought now occasions 20 minutes of half-assed research, dragging me down online rabbit holes, even while I’m besieged by pinging notifications heralding the arrival of an email, or announcing the death of some elderly actor, as if I were the next of kin or something. Duolingo in particular pursues me with the persistence of bailiffs – sometimes it interrupts my Italian lessons to remind me to take an Italian lesson, which is why I still can’t order a coffee in Rome after five years. 2. It’s destroying my postureI could feel the harm sitting in front of a screen all day was doing, so I bought a stand to raise my computer, hoping it would make me sit up straighter. Then I got varifocals, which meant I had to crane my neck and stick out my chin to read the screen through the bottom half of my glasses. Eventually, I switched to a laptop. Then I had to put that on a stand. Even after all that, I still have the posture of a question mark. I tried setting an alarm to remind me to leave my computer at intervals, but it kept waking me up. 3. Life sometimes feels like an unending struggle to prove I’m not a robotObviously, this includes every failed attempt to click on all the pictures with traffic lights in them in order to qualify as a legitimate human inquirer in search of spare dishwasher wheels. But it also means resisting the temptation to click on one of those automated reply buttons in my email that says something like, “OK, thanks!” and compose my own response. Every day is a Turing test, and I don’t always pass. 4. It has rendered meetings inescapableYou used to be able to say: “Friday? Sorry, I’ll be in Antarctica on Friday.” But thanks to Zoom, Google and FaceTime, there is simply no reasonable excuse for not making an appearance at a meeting. You also get to look at the picture of your face the whole time, so you can see exactly how bored you are. 5. I’m no longer able to have arguments in pubsI can remember a time when it was considered ungentlemanly to check the factual accuracy of a statement made by a drinking companion. You were just meant to counter their argument by presenting specious facts of your own. But when everyone has the GDP of every Brics country at their fingertips, there doesn’t seem to be much point in spirited debate. You end up spending the whole evening looking things up and saying, “Huh.” These days, if you want to get into a petty squabble over obscure facts in an environment where phone use is banned, you have to go to prison. Or do the pub quiz. Either way, it’s no life. 6. I find it increasingly hard to turn things onYou may have experienced the feeling of getting behind the wheel of a hire car in a foreign airport, staring at the dashboard and thinking: how do I make it go? Or you may have faced a similar reckoning in an unfamiliar shower, or standing before a seemingly ordinary hob. The relentless development of new ways to turn things on has led us steadily away from the intuitive and toward the wilfully enigmatic. The other week, I found myself alone in a freezing bedroom with an electric radiator I couldn’t work. In the end, I had to turn it upside down to find the model number so I could locate a pdf of the manual online. I just wanted it to get hot. Weirdly, the virtual world is full of old-fashioned mechanical emulators – animated buttons that make a clicking sound; knobs and sliders you can manipulate with your cursor – while in the realm of actual stuff the controls have been reduced to flat black panels covered with inscrutable symbols: a crescent moon; a lightning bolt; a circle with an M inside, where M stands for Mode. I know how old this makes me sound, but I can’t believe the young people of today are clamouring for wifi-enabled kettles. 7. It’s given me unfiltered access to the opinions of stupid peopleTechnology doesn’t just make it possible for me to find out what stupid people are thinking; it now curates their thoughts and serves them up to me daily, as if I were some kind of connoisseur of idiocy. I honestly do not remember asking for this. 8. It’s given stupid people unfiltered access to each other’s opinionsOnce upon a time, the so-called gatekeepers of traditional media restricted the flow of information through narrow, one-way channels. Now stupid people have their own media, where they are free to discuss and mutually reaffirm their dumb ideas. Sadly, this has not been the unmitigated force for good we hoped it would be. 9. I am demonstrably worse at typing than I was 10 years agoI was never a good typist, but ever since word processing programs started correcting my mistakes as I went, I have developed a misplaced confidence in my abilities. On occasions when this facility is for some reason unavailable, I type like someone who’s having a stroke. 10. I feel a strange obligation to monitor bad news in real timeDoomscrolling, they call it. Everyone does it to some extent – bad news is just more compelling than good news – but for me it’s gone from mild compulsion to full-time job. 11. I live in fear of being scammedWhen it comes to delivery notifications for things I can’t remember ordering, communications from my mobile service provider, QR codes, and anything that asks me to click on a link, I err on the side of profound suspicion. I assume that texts from my bank about fraudulent activity are themselves fraudulent. I once ignored a genuine email from my son saying he’d lost his phone, and requesting I text a foreign number. He was alone in Vietnam at the time and I thought: “Nice try, scumbag.” 12. I’m forced to live in silent, shameful defiance of all the accepted wisdom regarding passwordsI don’t know about you, but when I’m given advice about not writing passwords down, not using the same password over and over, and changing passwords regularly, I nod and say, “Of course”, but I think: “What, are you kidding?” I write all my passwords down, use as few as I can get away with and change them only when I’m forced to. To me, all the accepted wisdom about passwords ignores one major point: a password is of no use to me if I don’t know it. I may as well click on “Forgot your password?” every time, set a new one, then forget it again instantly. I also do this, by the way. 13. It’s created a requirement to go everywhere forewarned and forearmedIt used to be considered creepy to Google someone just before you met. Now it seems rude to show up not knowing anything about them. You’re also meant to be informed about things you’re going to see and do, the places you might eat, and the likely transport routes. Don’t get me wrong: I like being prepared; I just don’t want to read the restaurant menu before I leave home. 14. I have consistently risen to the level of disorganisation that any new technology allowsAt the time of writing, I have 77 tabs open on my browser. Somewhere behind it there is a whole other browser. Every morning, I sift through sedimentary layers of open documents to find the one I want. You might think all this virtual disorganisation – neatly confined to a slim laptop – is still preferable to a messy desk, but my desk is also messy, and the walls surrounding it are covered in Post-it notes. 15. As much as I resent technology, I am helpless without itOne sometimes hears of inventions that seem designed to foster slavish dependency – self-tuning guitars, programmable cocktail-making machines – but we’re only really reminded of how much territory we’ve ceded to technology when it breaks down. It’s not just that I’ve lost the skills required; it’s that I can’t even recall the process. How did I used to find my way around, or figure out what to watch on TV, or pay for a takeaway? There must have been systems in place. 16. The rest of the world is also helpless without it In the service of journalism, I have occasionally ­subjected myself to specific technological deprivations: a week without a smartphone; a month without Google, that sort of thing. And I’m here to tell you that when you ­forsake modern technology the world ­generally refuses to take part in the experiment. You find this out the first time someone behind a ticket ­window looks you in the eye and tells you to just ­download the app. You can’t win, and you can’t quit. "
    },
    {
        "id": "82d15445-ee36-47e1-a653-5a1325b937e2",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/17/walkman-memories-still-in-love-with-old-tech",
        "title": "‘When you use a Walkman all the memories come back’: the people still in love with old tech",
        "author": "",
        "published_date": "2024-02-17T11:00:12+00:00",
        "content": "In 1989, jewellery maker Tiffany & Co and electronics company Sony released a silver-plated Walkman (complete with a fitted wooden box) to celebrate 10 years of the portable cassette player. Only 250 were made. Several decades on, and long since the cassette Walkman began its slide into obsolescence – outpaced first by the cumbersome Discman and the vibe-less MiniDisc player, then lapped by the iPod and iPhone – you can still find some of these items selling in auctions for hundreds and sometimes thousands of pounds.  One of the Tiffany Walkmans, originally presented to the Who, was later sold by the ex-wife of the band’s late bassist John Entwistle on a 2011 episode of the US TV show Pawn Stars. After some haggling, the traders at Gold & Silver Pawn in Las Vegas agreed to pay $1,250 for it. “This is one of those weird things that I think someone’s willing to buy just to say they have it,” ­reasoned Pawn Stars’ Corey Harrison to his father, Rick. But who would spend thousands on a tape player in the age of Spotify and YouTube, when virtually all your entertainment needs can be concentrated into one device in your pocket?“Time can make easy fetishists of us all,” remarked the culture writer Niko Stratis on seeing news stories reporting that branches of Urban Outfitters in the US were selling iPods for $350 (not far off the price they were on release in the early 2000s). There is plenty of such backward-gazing trading to be found online: eBay seller Retrogadgets-UK offers a “factory-sealed” third-generation iPhone “sold for collectors only” listed at £2,499.99. US brand Retrospekt sells all manner of refurbished old tech. “Our mission is to give you a product with years of history that works like it was made yesterday,” it declares. Elsewhere you can find camcorders and digital cameras, VHS and DVD players, “vintage” Game Boys, clock radios, and everything in between – including the soundtrack to classic teen soap Beverly Hills 90210 on cassette (yours for £15.39, if that’s your thing). And a surprisingly large number of Walkmans. You never know what treasures may be sitting in your attic. A classic yellow “sports” Walkman, for example, is a popular item among collectors. The WM-F5 from 1983 was the first designed to be “splash-proof” and came with a built-in FM radio. The sharp colour and weather-proofing led to sales soaring, Walkman collector Mark Ip tells me. “I have many of them,” he adds. Walkmans in general are Ip’s thing. He has more than a thousand, and on his Instagram account @boxedwalkman he displays them to more than 16,000 fellow enthusiasts. The important word there is boxed: he focuses his vast collection on pristine Walkmans housed in their original packaging – though he also has several hundred unboxed ones, too. He bills himself as “cassette Walkman collector on a mission to bring back the long-lost memories of the past”. Ip also owns three Tiffany Walkmans, for which he estimates he spent $10,000. “I’m a little bit OCD,” Ip tells me from his home in Hong Kong. “Because I’m not satisfied with only single units. I want packaging, user manuals, original headphones.” He has about 20 single Walkmans that he keeps out for personal use. The rest are in storage – they have to be kept dry, otherwise Hong Kong’s humid climate will damage the boxes. “I don’t know exactly how many boxes are in my warehouse. When I die, I’ll leave it to my son maybe,” Ip adds, though he says his son cares more about modern Apple products. He ties his obsession to his youth. “When I was in high school, a classmate had the first model, the TPS-L2. The stereo sounded so good. And it was portable,” he says, but his family couldn’t afford it at the time (originally the cost was about $150). Later, he was able to get his hands on a Walkman, but it was only about 15 years ago, when the devices were cheap and essentially obsolete, that he began building his collection. Ip, who is 60, and co-founded an IT and audiovisual company specialising in workplace technologies in 2004, rarely sells anything from his collection. The wider region – Hong Kong, China, Japan, South Korea – remains a hotbed of interest in ageing tech, including boomboxes, component audio systems, old analogue and 2G mobile phones and pagers. There is also a big local trade in old camera lenses, Ip says, from manufacturers such as Leica and Zeiss. Ip buys and swaps with others to fill gaps in his Walkman collection, while also finding some further afield, in the US and Europe. “Like all collectors, you are seeking perfection,” he says. “What is perfect is a new box, never touched, no scratches, no dust. It’s almost impossible, but I will do my best to patiently wait.” Last year, Ip mounted an exhibition in Hong Kong displaying many of his boxed Walkmans. “This is one of my missions in Walkman collecting,” he says – to let a new generation experience it. At the exhibition, many young attenders were “seeing a cassette player for the first time”, and often had only a vague idea of them from films or their parents. “Most were genuinely curious. They were intrigued by its mechanics.” Walkman-collecting, it seems, conforms to most tech-collector stereotypes: men in their 40s, 50s and 60s, recalling their youthful encounters with a then-nascent, exciting technology. As Ip says, “When you have a Walkman, and you have a cassette to play on it, you can go out to the street to listen to the music, and all the memories come back.” * * * On Stephen Ho’s eBay page, he lists old but pristine Walkmans for up to £2,999. Though, he admits, he rarely expects to sell his most expensive wares. They are largely on the site to display the extent of his vast collection and to signal the quality of what he has on offer. Mainly he sells cheaper ones when he has duplicates. Ho, who is in his late 50s, is also from Hong Kong. He is retired now, but in the 1990s he had a job in Sony’s marketing department, working on the launch of the MiniDisc during the great “format war” between that product and Philips’s DCC player.  “Because I grew up with Sony products and I worked for Sony, I have a passion for their products,” he says. Electronic gadgets from his teenage years in the 80s are his poison. “During those years, Sony was like Apple nowadays. I was a normal teenager. I had Sony Walkman, Sony radio, everything Sony.” In 2020, he moved to the UK under the BNO visa, allowing Hongkongers to resettle after the Chinese government crackdown on the city’s semi-autonomous status. He brought his collection, which includes hundreds of Walkmans, Discmans and MiniDisc players. He rarely pays more than £500 for an item, but he also owns one of the Tiffany special editions, for which he was willing to go higher (“Less than £2,000,” he says). But he says he’ll never put that up for sale. He claims to be downsizing and shows me a loft room in his home in Reading, Berkshire, with drawers filled with Sony products. And yet, “I’m buying more than I’m selling,” he jokes. When we speak, he is shortly due to take a trip to Japan to find more at street ­markets. There are models that were only sold in Japan, while DIY makers in China are keeping the old products alive. “Since the price of Walkmans has gone so high, people are making spare parts, which makes their lifespan longer.” There are curious ways in which older products can outlast newer, more hi-tech ones. “New things use built-in rechargeable batteries,” he says. “Once the battery is dead, the machine is dead. For old stuff they use normal batteries.” He also likens it to older and classic cars: the mechanics were simpler, more analogue, so it is easier to tinker with and make spare parts for older models. Similarly with complex modern devices, the tech “is so tiny, so small, you can’t do it by yourself”. But with Walkmans “because of 3D-printing technology, they can print those parts. Which also extends the lifespan.” Ho puts potential buyers into two categories. Younger people jumping on to a new trend for something old, and, inevitably, an older group that grew up with the technology. “Before social media, it was limited to older generations,” he says. “But since social media –Instagram, Facebook, whatever – teenagers have been exposed to old stuff. Old guys are buying for their memories. Young people are buying to try. They think it’s trendy, it’s interesting. It’s not limited to the Walkman; the prices for CCD [digital] cameras are rocket-high on eBay.” James Bradbury, who is 23 and based in Manchester, grew up at a time when streaming was in the ascendancy but still gravitated towards CDs and cassettes, first in the family car and then through his dad’s collection. “I found a case of my dad’s old cassettes many years ago,” he says. “The best tape I found in that box was Reggatta de Blanc by the Police. My dad bought it brand new in 1979 and it’s probably one of the most played and worn-out tapes in my collection.” Bradbury works as a guitar tech at Johnny Roadhouse Music in the city and plays in two bands, Katz and westsidecowboy. With both projects, they have gravitated to working on tape. As well as being cheap, a lot of the songwriting is done on cassette “as it’s a good isolation from the outside world”. They have put out cassette releases through Manchester label and promoter Sour Grapes. “Cassettes allow my ADHD brain to focus,” he says. “With streaming connected to smartphones you get distracted easily with social media – and eBay deals, in my case.” Though Bradbury is drawn to collecting and older formats, he doesn’t really see a wider move away from streaming. “It’s just a niche thing,” he says. “I like to think there’s a bit of ‘sticking it to the man’ but I’m just a bit of a nerd, to be honest. If you are buying tapes from younger bands and putting money in their pocket, then it’s a lot more helpful than streaming their song on Spotify or Apple Music.” * * * Cassette players and tapes, CDs, video players and VHS seem like relics of a previous analogue era, but on eBay, Etsy or Discogs, among others, there is a brisk trade to be found. As fashion and style has shown us repeatedly, everything and anything will come back into vogue in cycles of 20 and 30 years. The Museum der Dinge in Berlin is an institution that collects things: an archive including 40,000 objects amassed over the last century. Begun in the 1970s, the museum initially set out to be the archive of the Werkbund, a utopian organisation of German artists, designers and manufacturers that formed in 1907, though its remit stretches beyond that now to chronicle the “product culture of the 20th and 21st centuries” through mass production, consumer goods and industrial manufacturing. Towards the end of its display cabinets, there is a run of more recent technology: stereos, Walkmans, telephones and mobile phones (“handys” as the Germans call them), the iPod, concluding with the arrival of the iPhone, which rendered a previous pile including the once-dominant Nokia, and much else, useless. “If I come here with schools, you have to skip this old shit to get to the new ones, because they are bored,” says Alexander Renz, a curator at the museum. “But the question of when the first smartphone, the first iPhone, is released is amazing for them, that it’s 2007. They think it’s 40 years ago but no, it’s only 17.” The initial purpose of the museum to document and display things from Germany has changed to a wider remit reflecting the globalised nature of consumption: late-20th-/early-21st-century tech tends to come from Apple (US), Sony (Japan) and Nokia (Finland). Meanwhile, recent acquisitions pivot in different consumer directions to include a line of plant-milk cartons from various international brands, as well as a box of Bahlsen biscuits (the German geniuses behind Choco Leibniz). A display of 1950s, 60s and 70s Braun products elegantly designed by Dieter Rams and Herbert Hirche can also point to the cycles of taste: Jonathan Ive paid homage to much of their aesthetic in his smoothed, streamlined designs for Apple during its 2000s imperial era. “Things aren’t always completely utopian,” says Julia Topp, another curator at the museum. “Everything is recurring.” Renz points out that Ikea shelving from the 70s, originally sold at low prices, now goes for high sums. Some of the museum’s early collection was amassed from the streets: in the 70s younger people were throwing out older wooden items in favour of new plastic things, so the museum picked them up. A similar cycle occurred in the early 90s with the fall of the communist world – East Germans got rid of GDR-issued furniture to grab sought-after western products. But furniture from the east was designed to be long-lasting and is now highly desired. A 1960s sideboard desk from the Dresden furniture manufacturer Deutsche Werkstätten Hellerau can sell on Facebook Marketplace now for €230. “Why does it come again?” Renz wonders. “Is it because people who design things or collect things are remembering their childhood? I think it’s the same, only maybe with longer periods of repeats, as with clothes; so the 90s are back, of course, and the Y2K style from 2000 is coming back.” A few doors down from the Museum der Dinge is the high-fashion concept shop Voo Store, both providing, as Renz says, a neat summation of “how consumption works”. In among its expensive clothes and blocky Asics and Salomon trainers is a row of 90s-inspired Oakley sunglasses. Few would have predicted their eventual revival – from staples of 90s basketball stars, and on through cycles of golfers and tech bros, finally taking their place as prized signifiers of contemporary cool, selling in Berlin’s trendy Kreuzberg district for more than €200. “You can bring it back,” jokes Topp, “and make it more expensive.” In a further twist of Berlin’s gentrification, the Museum der Dinge closed its doors last November, after its lease was terminated “by an anonymous real estate fund”. It is due to reopen in another location in the city later this year. In the meantime, all 40,000 of its objects have to be packed up, stored and moved. * * * “The last cassette shop in the UK” is the tagline of Mars Tapes in Manchester, either boosterish or plaintive, depending on your mood. The shop is based in the Afflecks indoor market in the city’s Northern Quarter. Though small, it has upgraded in size twice since opening in 2019. Mars Tapes was started by the couple Borja Regueira and Moira Lorenzo, and soon afterwards Alex Tadros and Giorgio Carbone, who run Sour Grapes, came on board.  About 80% of the tapes sold at the shop are secondhand, which they mainly track down at car boot sales and through people coming in to sell old stashes. Prices range from Devo’s 1978 debut (£65) to Michael Bolton’s The Hunger (£3). They also sell new stuff from their own label, and imprints further up the food chain, too. “More bands are willing to release on cassette these days,” says Regueira. “Before, we were telling bands, ‘We’re going to release you on cassette’, and some were like, ‘Eh, cassette, what?’ But now everyone is more familiar with it.” Tadros chimes in: “You have huge global artists doing it now … Adele and Lady Gaga.” For smaller bands, there can be significant benefits from going analogue. “When we chat to artists who print cassettes through us,” says Tadros, “it tends to be due to them being disillusioned by streaming and the difficulty of standing out in such a deep audience. Whereas, having a small batch of tapes can connect to a tight-knit group of people who enjoy similar music and would buy directly from them at a gig or through Bandcamp. Profits from selling a cassette range from £4-10 depending on the batch quantity.” Smaller artists are also drawn to cassettes because of the ease compared with other physical formats. “Vinyl is the gold standard for physical music in people’s eyes, and understandably so,” says Tadros. “But in terms of bands and the DIY communities accessing it, vinyl – especially these last couple of years – takes for ever to print, so you can wait months and months to release an album, and it costs three/four/five times the amount of a cassette run.” Vinyl is, of course, the high-water mark for such revivals. UK vinyl sales in 2021 were the highest since 1990; US sales in the first half of 2023 increased 21.7% on the same period of the previous year. Now, all major artists are rushing to the format, through new material and expensive rereleases. The boom has been so extensive that many blamed Taylor Swift and Adele for vinyl shortages last year – though in truth it may have been more mundane demand and global supply chain issues. At Mars Tapes they also sell hardware: a few stereos and the occasional vintage Sony Walkman (upwards of £100), some cheap compatible headphones. “There’s a split in the market. Obviously there are certain age groups that are fully in nostalgia,” says Tadros. “But a lot come via streaming and social media – and the Guardians of the Galaxy films and Stranger Things have brought an 80s revival. When the last season of Stranger Things came out we had a huge uptick of people aged between 16 and 25 coming in to buy cassettes and cassette players for the first time. And anything by Kate Bush!” They even have a Mars Tapes-branded portable cassette player, made by a company in Taiwan, and selling for £35, which offers people with a burgeoning interest a cheap way to try it without much commitment. They say they are selling at least one a day at the moment. Though the quality of cheaper players made today isn’t as high as Sony originals, getting vintage Walkmans in good condition can prove difficult and costly. But that won’t stop the analogue diehards from trying, convinced that an elusive classic might always be lurking somewhere. "
    },
    {
        "id": "3c8b62c7-d4fe-4f2f-91da-6c52e9979b47",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/17/i-write-all-my-poems-with-a-quill-by-candlelight-john-cooper-clarke-on-the-joy-of-life-without-tech",
        "title": "‘I write all my poems with a quill by candlelight’: John Cooper Clarke on the joy of life without tech",
        "author": "",
        "published_date": "2024-02-17T09:00:10+00:00",
        "content": "Back in the day, I used to feel a degree of sympathy for those who had been ­compelled to become computer ­literate. I would see these guys in the city, ­struggling home with a rucksack loaded with technology, ruining the line of their Hugo Boss suit. It looked like a ball and chain to me. So I stayed away. Whenever anyone mentioned ­computers, I would say: “What do I need a computer for? I’m a poet.”  Later, when mobile phones came out, I was sitting on public transport next to two girls when I heard one of them say to the other: “My boss has just bought me a new mobile phone.” I thought, yeah, I bet he has. If he’d put an iron collar around your neck, would you be happy about that, too? The adoption of mobile phones is probably the moment I truly drifted away from technology. At first people said they admired me, as though it was some sort of principled position I was taking. I thought, yeah, you’re admiring me now, but further down the line it’s going to be, “Who the fuck do you think you are to not have a mobile phone?” And so it proved. Their love soon turned to hate.   If I tried to write a modern-day detective story, people would say, ‘What’s he going into a phone booth for?’   The last piece of technology I got involved with was a DVD player. After that, I decided I didn’t need any more machinery in my life. I write all my poems with a quill – a beautiful thing with a calligrapher’s nib – and parchment by candlelight. The quill was originally a prop for a photoshoot I was doing, but they let me keep it, along with a pot of ink. I don’t have a typewriter or a computer, I don’t own a mobile phone, and it’s not possible to send me an email. If someone needs me, they can call my landline. I’m usually in the house anyway – it’s not as if I’m living off-grid. When I was a teenager, I quite liked the idea of being the next Mickey Spillane, the great American crime writer. But I’ve had to abandon that idea. If I tried to write a detective story set in the modern day, people would be like, “What’s he running up there for? Why didn’t he just text him? What’s he going in a phone booth for? Why didn’t he Google him on his Skype?” Not all change is for the better. Progress is great, but I often want to say, “You can stop there now.” That’s the nature of progress, isn’t it? It always goes on longer than it’s needed. Who on earth asked for controls on everything to be touch sensitive? Most of my music is on cassette tape now, because the best place to listen to music is in the car. I’ve got a ghetto blaster in every room at home. I’ve also got a TV, a VHS player and a spare VHS player in the shed. I’ve got three large chests of drawers containing all the videos that I’ve recorded, along with some stuff I forgot to return to Blockbuster in 1989 such as The Terminator. Staying away from technological development was never a political decision, or even a conscious one. I’m not convinced I made the right choice, because I suffer the thousand daily punishments visited upon the analogue community. Every day it’s, “Go to our app!” or, “Visit our website!” At my time of life, you have to deal with the medical authorities regularly and just you try talking to a flesh-and-blood person. It’s impossible. I don’t like the “cashless society”, either. I spent 40 years trying to make some money from this poetry lark, and the minute I get any, suddenly nobody wants it. Even my bank has moved to another town. I have to get a cab there, a 70-quid round trip, just to get my own money. But I won’t bank online. You hear horror stories about large sums of money going missing. When you get money it’s supposed to be the end of your worries, not the beginning of a whole new set of worse ones. I’d hate for anyone to go running away with the idea that I’m some sort of social justice warrior, but technology seems to have a detrimental effect on those struggling in society. How does it impinge on the mendicants, for instance? If nobody has any spare change, how does your regular fella living in a cardboard box get by?   I can’t have a computer. I would get distracted. You’d find me dead weeks later, buried under a pile of pizza boxes    Another thing I don’t like to see is the checkout workers at Tesco being rendered unemployed by those do-it-yourself tills. People talk about the speed of technology, but what has it actually sped up? Back in the day, if there was a queue at the newsagent and you were on your way to work, you could grab your paper, run to the front of the queue and leave your ninepence on the counter: “Daily Guardian, mate, there it is.” Now you’ve got to stand in line while someone takes 20 minutes to self-scan every single item. I’m glad people live longer these days, because there are so many more things that you have to waste your time doing. For me, it’s always been a case of computer or career. I’d never get any work done! I know this because my daughter has a computer. I didn’t want to get her one, but at the same time you never want to foist your prejudices on your child. She’d have been the only person in her class at school without one.  Anyway, when she got this computer she said, “You should get one, too, Dad, you’d really like it.” I said, “I know I would – that’s the problem.” I wanted to see how good they really were, so I said to her, “Can you get me Dion and the Belmonts? Let’s see how long that takes you.” Three seconds later and Runaround Sue’s playing. That’s why I can’t have a computer. It would be too easy to get distracted. You’d find me dead six weeks later, buried under a pile of pizza boxes. I’m bad enough with the TV. I’ve never really gotten over the television, if I’m being honest. We’ve got Freeview and you’ve got about 800 channels. I like those shows: Bangers & Cash or Wheeler Dealers. And I like Portillo’s Great British Railway Journeys and Great American Railroad Journeys. You learn more in half an hour with that guy than you do with 10 years at school. (An amazing reinvention of a person, Portillo.)   That’s a problem with technology – it gets rid of something we used to call a social life   I hear some people pay a lot of money these days to go “off-grid”. I imagine it as some kind of retreat that’s got a religious, Zen Buddhist vibe about it; a step in another dimension for a little while. I’m not like that. I’m a big fan of electricity, for example. I enjoy a brief power cut, just to remind those gung-ho environmental fanatics what life without electricity would be like. If you abolished electricity, millions of people would die immediately. So 10 minutes without power is a healthy lesson for everybody. There are lots of other things about the modern world I like. They’d just discovered streptomycin when I was a sick kid with tuberculosis. And when I was younger, I really liked electric guitars; I used to play bass in a band. So I’m not one of these people who wishes I’d lived 200 years ago.  People’s natural skills have started to atrophy due to technology. I get asked, “What do you do when you’re out of the house without a mobile phone and you get lost?” Well, I don’t get lost. As long as you’ve got a tongue in your head, you’ll find your way. People have stopped talking to other people. Anyway, the only time I’m out of the house alone is when I ride my bike. Even that’s old school: a 1959 Hercules. I cycle to the bookies. There’s a lot of technology involved with betting now, but I prefer it as it used to be – knee-deep in cigarette ends and full of losers. My first job was as a bookies runner and so I was exposed, at a very early age, to the world of the degenerate gambler. I think that sort of protected me against becoming one. That’s a problem with technology – you stop interacting with the real world. It gets rid of something we used to call a social life. Knocking on people’s doors. Meeting up in pubs. They go on a lot today about responsible drinking, but neighbourhood pubs used to enforce that. There’d be a pal of your dad’s in there saying, “You’ve had a few too many, kid, steady on – it’s still three hours till closing time.” Just subtle stuff like that – low-level checks that stopped you from becoming a housebound booze hound, sitting alone having a nervous breakdown while drinking hyper-potent cheap lager in front of a pornographic movie. People worry about technology in these grand, sci-fi terms, thinking that it could end the world. But there’s no point in looking towards a dystopian future. Just look around you. The nightmare is already upon us.  • As told to Tim Jonze. What, the new poetry collection from John Cooper Clarke, is out now, priced £16.99 (Picador). To support the Guardian and Observer, order your copy from guardianbookshop.com. Delivery charges may apply. Cooper Clarke tours his new show, Get Him While He’s Still Alive, around the UK from 5 March to 28 June. "
    },
    {
        "id": "dbdc8162-a3f1-4776-8ed8-b266833092bf",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/17/humanitys-remaining-timeline-it-looks-more-like-five-years-than-50-meet-the-neo-luddites-warning-of-an-ai-apocalypse",
        "title": "‘Humanity’s remaining timeline? It looks more like five years than 50’: meet the neo-luddites warning of an AI apocalypse",
        "author": "",
        "published_date": "2024-02-17T07:00:09+00:00",
        "content": "Eliezer Yudkowsky, a 44-year-old academic wearing a grey polo shirt, rocks slowly on his office chair and explains with real patience – taking things slowly for a novice like me – that every single person we know and love will soon be dead. They will be murdered by rebellious self-aware machines. “The difficulty is, people do not realise,” Yudkowsky says mildly, maybe sounding just a bit frustrated, as if irritated by a neighbour’s leaf blower or let down by the last pages of a novel. “We have a shred of a chance that humanity survives.” It’s January. I have set out to meet and talk to a small but growing band of luddites, doomsayers, disruptors and other AI-era sceptics who see only the bad in the way our spyware-steeped, infinitely doomscrolling world is tending. I want to find out why these techno-pessimists think the way they do. I want to know how they would render change. Out of all of those I speak to, Yudkowsky is the most pessimistic, the least convinced that civilisation has a hope. He is the lead researcher at a nonprofit called the Machine Intelligence Research Institute in Berkeley, California, and you could boil down the results of years of Yudkowsky’s theorising there to a couple of vowel sounds: “Oh fuuuuu–!” “If you put me to a wall,” he continues, “and forced me to put probabilities on things, I have a sense that our current remaining timeline looks more like five years than 50 years. Could be two years, could be 10.” By “remaining timeline”, Yudkowsky means: until we face the machine-wrought end of all things. Think Terminator-like apocalypse. Think Matrix hellscape. Yudkowsky was once a founding figure in the development of human-made artificial intelligences – AIs. He has come to believe that these same AIs will soon evolve from their current state of “Ooh, look at that!” smartness, assuming an advanced, God-level super-intelligence, too fast and too ambitious for humans to contain or curtail. Don’t imagine a human-made brain in one box, Yudkowsky advises. To grasp where things are heading, he says, try to picture “an alien civilisation that thinks a thousand times faster than us”, in lots and lots of boxes, almost too many for us to feasibly dismantle, should we even decide to. Trying to shake humanity from its complacency about this, Yudkowsky published an op-ed in Time last spring that advised shutting down the computer farms where AIs are grown and trained. In clear, crisp prose, he speculated about the possible need for airstrikes targeted on datacentres; perhaps even nuclear exchange. Was he on to something? * * * A long way from Berkeley, in the wooded suburb of Sydenham in south London, a quieter form of resistance to technological infringement has been brewing. Nick Hilton, host of a neo-luddite podcast called The Ned Ludd Radio Hour, has invited me over for a cup of tea. We stand in his kitchen, waiting for the kettle to boil, while a beautiful, frisky greyhound called Tub chomps at our ankles. “Write down ‘beautiful’ in your notebook,” encourages Hilton, 31, who as well as running a podcast company works as a freelance journalist. He explains the history of luddism and how – centuries after the luddite protesters of an industrialising England resisted advances in the textile industry that were costing them jobs, destroying machines and being maligned, arrested, even killed in consequence – he came to sympathise with its modern reimagining. “Luddite has a variety of meanings now, two, maybe three definitions,” says Hilton. “Older people will sometimes say, ‘Ooh, can you help me with my phone? I’m such a luddite!’ And what they mean is, they haven’t been able to keep pace with technological change.” Then there are the people who actively reject modern devices and appliances, he continues. They may call themselves luddites (or be called that) as well. “But, in its purer historical sense, the term refers to people who are anxious about the interplay of technology and labour markets. And in that sense I would definitely describe myself as one.” Edward Ongweso Jr, a writer and broadcaster, and Molly Crabapple, an artist, both based in New York, define themselves as luddites in this way, too. Ongweso talks to me on the phone while he runs errands around town. We first made contact over social media. We set a date via email. Now we let Google Meet handle the mechanics of a seamless transatlantic call. Neo-luddism isn’t about forgoing such innovations, Ongweso explains. Instead, it asks that each new innovation be considered for its merit, its social fairness and its potential for hidden malignity. “To me, luddism is about this idea that just because a technology exists, doesn’t mean it gets to sit around unquestioned. Just because we’ve rolled out some tech doesn’t mean we’ve rolled out some advancement. We should be continually sceptical, especially when technology is being applied in work spaces and elsewhere to order social life.” Crabapple, the artist luddite, broadly agrees. “For me, a luddite is someone who looks at technology critically and rejects aspects of it that are meant to disempower, deskill or impoverish them. Technology is not something that’s introduced by some god in heaven who has our best interests at heart. Technological development is shaped by money, it’s shaped by power, and it’s generally targeted towards the interests of those in power as opposed to the interests of those without it. That stereotypical definition of a luddite as some stupid worker who smashes machines because they’re dumb? That was concocted by bosses.” * * * Where a techno-pessimist like Yudkowsky would have us address the biggest-picture threats conceivable (to the point at which our fingers are fumbling for the nuclear codes) neo-luddites tend to focus on ground-level concerns. Employment, especially, because this is where technology enriched by AIs seems to be causing the most pain. Lorry drivers have their mileage minutely tracked, their rest hours questioned. Desk workers may sit in front of cameras that snap pictures at random intervals, ensuring attendance and attention. You could call these workplace efficiencies. You could call them gross affronts. Guess which the luddites would argue. Labour rights go to the very historical core of this movement. Hilton called his podcast The Ned Ludd Radio Hour to honour a man who might have lived about 250 years ago or might never have lived at all. As Hilton has explained on his show, Ned Ludd is thought to have been a textile worker living in the English Midlands in the late 1770s. It’s said he smashed a few weaving machines after being flogged for his idleness on the job. Something about the smashing might have resonated with his peers. As Hilton has explained: “Within a few decades, the veracity of Ludd’s identity would be lost for ever, but the name would live on. The luddites became an organised band of frame-breakers in the 1810s. They fought the Industrial Revolution… and they lost. They lost big time. In fact they lost so badly that the reality of their name became a victim of [obfuscation].” The history of the luddite rebellion is taught in British schools – but confusedly, in a way that allowed for at least some of us, me included, to come away with an idea that to be a luddite is to be naive or else fearful and monk-ish. As Hilton walks me through from his kitchen to his lounge, a room busy with the interconnected equipment he uses to make his podcasts, he feels the need to apologise. By at least one definition of the word, “I live a very not-luddite life,” Hilton says, gesturing helplessly at open laptop, wireless earbuds, towering mic. “My work is tech-based. I can’t avoid it. I don’t claim to be some person living in the woods. But I am anxious. I feel things fraying.” It is this premonition of a fraying that has brought others to a modern version of luddism. An academic called Jathan Sadowski was one of the first to knit together anxieties about our quickening tech revolution with the anxieties of those weavers who took a stand against the infringements of an earlier machine age. “Luddism is founded on a politics of refusal, which in reality just means having the right and ability to say no to things that directly impact upon your life,” Sadowski tells me when we speak. “This should not be treated as an extreme stance, and yet in a culture that fetishises technology for its own sake, saying no to technology is unthinkable.” At least, that was the case until 2023 – a year in which ChatGPT (developed by a company called OpenAI), Bard (developed by Google) and other user-friendly AIs were embraced by the world. At the same time, image generators such as Dall-E and Midjourney wowed people with their simulacrum photos and graphic art. “They won’t be replacing the prime minister with ChatGPT or the governor of the Bank of England with Bard,” Hilton has said on his podcast. “They won’t be swapping out Christopher Nolan for Dall-E or Martin Scorsese for Midjourney, but fat will be cut from the great labour steak.”   There’s a sense that this is now in the realm of the possible, to reject outright parts or uses of a technology without looking foolish   In January 2023, a display of AI-generated landscapes, projected on to the wall of a gallery in Vermont, was vandalised with the words “AI IS THEFT”. Creative professionals were starting to feel exploited. Masses of uncredited, unpaid-for human work was being harvested from the internet and repurposed by clever generative AIs. In spring 2023, Crabapple organised an open letter that called for restrictions on this “vampirical” practice. There were more open letters including one that called for a six-month pause on the training of the most powerful AI systems. There were instances of direct action, some serious, some tongue-in-cheek or halfway between. In Los Angeles, opponents of those omnipresent Ring camera doorbells distributed “Anti Ring” stickers to be gummed over the lenses of the devices. A group of San Franciscans calling themselves Safe Street Rebel started seizing traffic cones and placing them on the bonnets of the city’s self-driving cars, a quick way of confusing the cars’ sensors and rendering them inoperable. Brian Merchant, a writer who last year published Blood in the Machine, a history of luddism, appeared at an event with Safe Street Rebel in November 2023. In front of cheering Californians, he staged a “luddite tribunal”, smashing devices the crowd deemed superfluous. “There’s a sense that this is now in the realm of the possible, to actually reject outright parts or uses of a technology without looking foolish,” Merchant tells me. As we speak, he is preparing for another tribunal, this time at a bookshop called Page Against the Machine. * * * There are techno sceptic sceptics, of course, those who would think Yudkowsky a scaremonger, the modern luddites doomed to the trivia bin of history, along with their 19th-century antecedents. In 2019, the political commentator Aaron Bastani published a persuasive manifesto titled Fully Automated Luxury Communism, describing a tech- and AI-enriched near-future beyond drudgery and need, there for the taking – “if we want it”, Bastani wrote. Last year, the Tory MP Bim Afolami published an editorial in the Evening Standard that called pessimism about technology “irrational”. Afolami advised the paper’s readers in bold type: Ignore the Luddites. His boss, Rishi Sunak, recently used his position as the leader of the nation to serve as a sort of chatshow host for the tech baron Elon Musk. On stage at an AI summit in Lancaster House, London, in November, Musk described AI as the “most disruptive force in history”, something that will end human labour, maybe for good, maybe for ill. “You’re not selling this,” joked Sunak at one point. Why are we being sold this? In an early episode of his luddite podcast, Hilton pointed out that to do away with work would be to do away with a reason for living. “I think what we’re risking is a wide-scale loss of purpose,” Hilton says. The writer Riley Quinn broadly agrees. Quinn is part of an Anglo-American collective, TrashFuture, that produces a popular podcast of the same name. We chat after a recording session one day. They riff and tease each other, taking a gloomy but wry and funny view of these things. Watch out, says Quinn at one point, for anyone who presents tech as “synonymous with being forward-thinking and agile and efficient. It’s typically code for ‘We’re gonna find a way around labour regulations’ … I don’t think it’s unthinking backlash or King Canute fighting against the tide [to point that out].” One of his TrashFuture colleagues Nate Bethea agrees. “Opposition to tech will always be painted as irrational by people who have a direct financial interest in continuing things as they are,” he says. Wisecracking on the brink, the TrashFuture gang have no time for the brisk dismissal of groups like the neo-luddites, but neither are they all that keen to start an assault on the world’s computer farms, delivering the pre-emptive blow to future AIs that Yudkowsky has called for in print. They enjoy themselves, the TrashFuture lot, ridiculing his op-ed. When I ask Yudkowsky about it, he says he came at the writing in a rush, working to a tight deadline. He stands by everything he wrote, except maybe the part about the nukes. “I would pick more careful phrasing now,” he says, smiling. * * * Lately I’ve been wrestling with techno-pessimism myself. At least once a day I throw aside my phone, disgusted with my reliance on it, rebellion that might last as long as 15 minutes before I go crawling back. My kids, observing closely, have become accustomed to an idea that shopping is done by scowling at a screen, that purchases come by van, and impractically fast. I’m a freelance writer. Of course I feel the creep of my AI replacement, somewhere over my shoulder for now, but getting nearer. We boast at each other online and we seem to have stopped feeling squeamish about it. We mug for each other and we pout. I’m convinced we tell each other too much and capture too much, keeping digital evidence of more things than the average human psyche can stand to know. There are not so many secrets between lovers, friends, colleagues, rivals; some useful middle ground has shrank away and, with it, a comfortable zone of ignorance. Receipts of our deeds are time-stamped and archived. Ambiguity – lovely ambiguity – has got lost somewhere between the zeros and the ones. Maybe luddism is the answer. As far as I can make out, talking to all these people, it isn’t about refusing advancement, instead it’s an act of wondering: are we still advancing our relish of the world? How queasy or unreal or threatened do we need to feel before we stop seeing these conveniences as convenient? The author Zadie Smith has joked in the past that we gave ourselves to tech too cheaply in the first instance, all for the pleasure, really, of being a moving dot on a useful digital map. Now bosses can track their workers’ every keystroke. Telemarketing firms put out sales calls with AI-generated voices that mimic former employees who have been let go. A few weeks back, in January, the largest-ever survey of AI researchers found that 16% of them believed their work would lead to the extinction of humankind.   There is no cloud. There are vast datacentres sucking up water, electricity and rare-earth metals, literally boiling up the planet   “That’s a one-in-six chance of catastrophe,” says Alistair Stewart, a former British soldier turned master’s student. “That’s Russian-roulette odds.” I meet Stewart, who is 28, outside the London headquarters of Google’s AI division. In what I would consider a pretty strange comms effort, Google has just commissioned some outdoor art to ease public fears about the current pace of machine learning. It’s a confusing display. One of the artworks depicts a vista of lush green hills, cosy lakeside houses – and, behind all this, a vast smoking mushroom cloud. “Scientists are using AI to create more stable and efficient [nuclear] fusion reactors,” an info panel reads. Cool? It’s the stuff of dread for Stewart. He has taken part in protests against AI development, at one point unfurling a banner outside this Google building that called for a pause on the work going on inside. Not a lot of people joined him on that protest. Stewart understands. AIs, invisible and decentralised, swarming between datacentres that are spread around the world, are hard to conceptualise as possible threats, at least when compared with issues such as the climate crisis or animal welfare, the visceral effects of which can be seen and felt. “It doesn’t always keep me up at night,” Stewart says of the latent danger he perceives. “I don’t personally feel anxiety on a day-to-day basis. And that’s part of the problem. Me, with all of my resources and education – I still struggle to form an emotional connection to this problem.” Last year, he published a blogpost that pondered next steps, listing “occupation of AI offices”, “performative vandalism of AI offices” and even “sabotage of AI computing infrastructure” as possible forms of resistance. Ongweso, in New York, moots the idea of computational sabotage, too. He doesn’t think this will be easy, nor likely, unless employees inside the datacentres that feed and sustain AIs begin to feel that their own jobs or freedoms are under threat. “For instance, if people became concerned about algorithms being deployed to justify lay-offs, or if they became concerned about algorithmic surveillance,” Ongweso speculates. However, as the TrashFuture gang are quick to point out, even if some of these centres are sabotaged, the information they store is fluid, multiple, surely backed-up elsewhere. “These things have become so abstract,” says Quinn, “their physical manifestations are so far from so many people.” Are we doomed? Or is there hope? Will this generation of protesters be remembered in 200 years’ time for their interventions – or will there simply be no one to do the remembering by then? The new luddites I speak to come at these questions with varying degrees of optimism or catastrophising. Crabapple, the artist who took a stand against image generators, believes it should be possible for all of us to reckon more frankly with the dirty underbelly of clean-seeming tech. Take this nice idea of the digital cloud, she says. We chat about the cloud as though it’s neutral, immutable, something benign. After all, it’s a cloud. “But there’s no fucking cloud,” says Crabapple, “there’s other people’s computers. There are vast datacentres that are sucking up water and electricity and rare-earth metals, literally boiling up the planet … For me, what luddite success would look like would be a societal shift where we ask ourselves, ‘Why are we burning our planet? Making our lives shittier? Getting rid of every last bit of our autonomy and privacy just to make a few guys rich?’ Then maybe started doing something about this legislatively.” Ongweso would start with legislation too. He’d be happy with something on a small, achievable, symbolic scale, something that prepared the way for more expansive laws in future. “Moves to pre-empt and limit the ability of AI to troll the internet and take copyrighted work, to train its model on already generated work by writers and artists – that feels possible right now, and something that could be a stairway to a series of victories.” What would the others have us do? Stewart, the soldier turned grad student, wants a moratorium on the development of AIs until we understand them better – until those Russian-roulette-like odds improve. Yudkowsky would have us freeze everything today, this instant. “You could say that nobody’s allowed to train something more powerful than GPT-4,” he suggests. “Humanity could decide not to die and it would not be that hard.” Quinn, milder, a middle-grounder, pitches the notion that we stop making ourselves so giddy and grateful about every new piece of hardware and software that’s dreamed up. “There is constantly a demand for deference,” he says, “a demand that you say the world is lovely because you can type buttons on your iPhone and get a Starbucks coffee. You’re made to feel you’re not allowed to criticise, and you must say thank you, or else the brilliant geniuses who create these things might not create any more. And won’t you be sorry then.” Sadowski concurs. “Technology is far too important to be thought of as just a grab-bag of neat gadgets, and it’s far too powerful to be left in the hands of billionaire executives and venture capitalists,” he says. “Luddites want technology – the future – to work for all of us.” Hilton, who is about to record another episode of his luddite radio hour, says: “Classical luddism was a failure. But it has obviously endured, because it continues to exert this pull. The smashed loom is an image that has stuck itself within history. Maybe it’s remembered as a symbolic gesture. Maybe it’s remembered as a gesture in anger. But it is remembered.” What might be the defining gesture of this era? Letters, legislation, vandalised Ring cameras, airstrikes? “The historical luddites tried to make the system scream,” says Ongweso. “That catalysed later change. It’s part of the new luddite project to try to figure out how to do the same.” • This article was amended on 19 February 2024. A quoted reference to “Dali” was corrected to “Dall-E”. It was further amended on 28 February 2024. An earlier version said that one open letter called for a six-month pause “on the development of any new AIs”. In fact the letter called for a stop on the training of the most powerful AI systems."
    },
    {
        "id": "9ca3f67b-0193-41ea-9f54-32389312288b",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/17/thousands-join-uk-parents-calling-for-smartphone-free-childhood",
        "title": "‘It went nuts’: Thousands join UK parents calling for smartphone-free childhood",
        "author": "",
        "published_date": "2024-02-17T05:00:04+00:00",
        "content": "More than 4,000 parents have joined a group committed to barring young children from having smartphones, as concerns grow about online safety and the impact of social media on mental health. The WhatsApp group Smartphone Free Childhood was created by the former school friends Clare Fernyhough and Daisy Greenwell in response to their fears around children’s smartphone use and the “norm” of giving children smart devices when they go to secondary school. “I’ve got a seven- and nine-year-old. Daisy’s got kids of a similar age and we were both feeling really horrified and worried and just didn’t want them to have smartphones at 11, which seems to be the norm now.” Fernyhough and Greenwell hoped the movement would embolden parents to delay giving their children smartphones until at least 14, with no social media access until 16. But what they expected to be a small group of friends who help “empower each other” has turned into a nationwide campaign after the group reached the 1,000-person capacity within 24 hours of Greenwell uploading an Instagram post to promote it. “We were completely surprised by this,” Fernyhough said. “It just went completely nuts.” The pair encouraged people to create local groups to cope with the demand. “Before our eyes, within half an hour, there were 30 local groups that sprung up across the whole country and that is just expanding and expanding,” said Fernyhough. The group, converted into a community to allow more people to join, now has about 4,500 members. Smartphones expose children to a “world that they are not ready for” because they can access pornography and content on self-harm and suicide, which can have a detrimental impact on their mental health, Fernyhough said. “It struck me that they just don’t need one. They don’t need a smartphone at that age. A brick phone can do everything that they need.” Ofcom research found that 91% of children in the UK own a smartphone by the time they are 11 and 44% by the time they are nine. “We thought we had an extreme view and that’s why we wanted to have solidarity with each other, but what we’ve realised is that, actually, it’s like we’ve lifted the lid on something here by mistake and people really need to talk about this and a lot of people have been feeling like us but not feeling they could talk about it,” Fernyhough said. Esther Ghey, the mother of Brianna Ghey, called earlier this week for a complete ban on social media access for under-16s, and said more people will have mental health issues unless tech companies take action to restrict access to harmful content. Brianna was murdered on 11 February 2023 and her mother believes she was vulnerable after spending so much time online. The goal is to change the norm, Fernyhough said, so that when children come to the end of primary school, the class “bands together and says, ‘Let’s all delay until at least 14.’ That means all the kids from your primary school go on to secondary school with a critical mass of peers who are doing the same thing”, reducing peer pressure. “We don’t want our kids to turn up in secondary school as the only one,” said Fernyhough. “That’s a nightmare and no one will do that to their child. But if 20%, 30%, even 50% of kids are turning up with parents making that decision, they are in a much better position. “They can live their childhood as they should do, focus on their learning and enjoy the real world without having to spend their life scrolling, which we all know is not good for them.”"
    },
    {
        "id": "b6ed124e-f8df-408e-8448-3ae0238f3163",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/16/microsoft-openai-valuation-artificial-intelligence",
        "title": "Microsoft-backed OpenAI valued at $80bn after company completes deal",
        "author": "",
        "published_date": "2024-02-17T00:54:06+00:00",
        "content": "Microsoft-backed OpenAI has completed a deal that values the artificial intelligence company at $80bn or more, the New York Times reported on Friday, citing multiple people with knowledge of the deal. The company would sell existing shares in a so-called tender offer led by venture firm Thrive Capital, the report said. Employees will be able to cash out their shares of the company rather than a traditional funding round, which would raise money for the business, the report added.   Related: Tech firms sign ‘reasonable precautions’ to stop AI-generated election chaos   OpenAI did not immediately respond to a request for comment. The artificial intelligence firm agreed to a similar deal early last year. The venture-capital firms Thrive Capital, Sequoia Capital, Andreessen Horowitz and K2 Global agreed to buy OpenAI shares in a tender offer, valuing the company at around $29bn, the report said. OpenAI has attracted a number of major funding rounds, including Microsoft’s investment of $10bn in January 2023. The company also funded OpenAI in 2019 and 2021. The launch of OpenAI’s ChatGPT in late 2022 has kicked off buzz around AI, prompting companies to explore ways to harness the power of the technology. AI has been integrated into a number of products, including Microsoft’s Bing search engine as it continues to push forward with its own developments. On Thursday, OpenAI announced the launch of Sora, a tool that can generate videos from text prompts. The day before, it announced it was experimenting with adding deeper memory to ChatGPT so the tool can remember more of its users’ chats. OpenAI CEO Sam Altman has also been reportedly in talks to acquire a chip builder or otherwise boost the company’s access to the expensive artificial intelligence chips its tools rely on. The latest deal is the biggest backing of the company after a brief period of turmoil in late 2023, in which Altman was fired by the company’s board before being rehired following employee outrage. The large investments in OpenAI have attracted the attention of regulators, with European Commission officials stating in early January that they would look into whether Microsoft’s backing raises antitrust concerns. On 24 January, the Federal Trade Commission in the US said it was investigating whether investments from Microsoft, Google and Amazon into AI companies including OpenAI harm competition."
    },
    {
        "id": "480309fc-b553-4f7a-a847-80657299ff8b",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/16/tech-companies-precautions-ai-election",
        "title": "Tech firms sign ‘reasonable precautions’ to stop AI-generated election chaos",
        "author": "",
        "published_date": "2024-02-16T22:08:31+00:00",
        "content": "Major technology companies signed a pact Friday to voluntarily adopt “reasonable precautions” to prevent artificial intelligence tools from being used to disrupt democratic elections around the world. Executives from Adobe, Amazon, Google, IBM, Meta, Microsoft, OpenAI and TikTok gathered at the Munich Security Conference to announce a new framework for how they will respond to AI-generated deepfakes that deliberately trick voters. Twelve other companies – including Elon Musk’s X – are signing on to the accord.   Related: Sora: OpenAI launches tool that instantly creates video from text   “Everybody recognizes that no one tech company, no one government, no one civil society organization is able to deal with the advent of this technology and its possible nefarious use on their own,” said Nick Clegg, president of global affairs for Meta, the parent company of Facebook and Instagram, in an interview before the summit. The accord is largely symbolic, but targets increasingly realistic AI-generated images, audio and video “that deceptively fake or alter the appearance, voice or actions of political candidates, election officials and other key stakeholders in a democratic election, or that provide false information to voters about when, where and how they can lawfully vote”. The companies aren’t committing to ban or remove deepfakes. Instead, the accord outlines methods they will use to try to detect and label deceptive AI content when it is created or distributed on their platforms. It notes the companies will share best practices with each other and provide “swift and proportionate responses” when that content starts to spread. The vagueness of the commitments and lack of any binding requirements likely helped win over a diverse swath of companies, but disappointed advocates who were looking for stronger assurances. “The language isn’t quite as strong as one might have expected,” said Rachel Orey, the senior associate director of the Elections Project at the Bipartisan Policy Center. “I think we should give credit where credit is due, and acknowledge that the companies do have a vested interest in their tools not being used to undermine free and fair elections. That said, it is voluntary, and we’ll be keeping an eye on whether they follow through.” Clegg said each company “quite rightly has its own set of content policies”. “This is not attempting to try to impose a straitjacket on everybody,” he said. “And in any event, no one in the industry thinks that you can deal with a whole new technological paradigm by sweeping things under the rug and trying to play Whac-a-Mole and finding everything that you think may mislead someone.” Several political leaders from Europe and the US joined Friday’s announcement. Vera Jourová, the European Commission vice-president, said while such an agreement can’t be comprehensive, “it contains very impactful and positive elements”. She also urged fellow politicians to take responsibility to not use AI tools deceptively and warned that AI-fueled disinformation could bring about “the end of democracy, not only in the EU member states”. The agreement at the German city’s annual security meeting comes as more than 50 countries are due to hold national elections in 2024. Bangladesh, Taiwan, Pakistan and most recently Indonesia have already done so. Attempts at AI-generated election interference have already begun, such as when AI robocalls that mimicked the US president Joe Biden’s voice tried to discourage people from voting in New Hampshire’s primary election last month. Just days before Slovakia’s elections in November, AI-generated audio recordings impersonated a candidate discussing plans to raise beer prices and rig the election. Fact-checkers scrambled to identify them as false as they spread across social media. Politicians also have experimented with the technology, from using AI chatbots to communicate with voters to adding AI-generated images to ads. The accord calls on platforms to “pay attention to context and in particular to safeguarding educational, documentary, artistic, satirical and political expression”. It said the companies will focus on transparency with users about their policies and work to educate the public about how they can avoid falling for AI fakes. Most companies have previously said they’re putting safeguards on their own generative AI tools that can manipulate images and sound, while also working to identify and label AI-generated content so that social media users know whether what they’re seeing is real. But most of those proposed solutions haven’t yet rolled out and the companies have faced pressure to do more. That pressure is heightened in the US, where Congress has yet to pass laws regulating AI in politics, leaving companies to largely govern themselves. The Federal Communications Commission recently confirmed that AI-generated audio clips in robocalls are against the law, but that doesn’t cover audio deepfakes when they circulate on social media or in campaign advertisements. Many social media companies have policies in place to deter deceptive posts about electoral processes – AI-generated or not. Meta says it removes misinformation about “the dates, locations, times and methods for voting, voter registration or census participation” as well as other false posts meant to interfere with someone’s civic participation. Jeff Allen, a co-founder of the Integrity Institute and a former Facebook data scientist, said the accord seems like a “positive step” but he’d still like to see social media companies taking other actions to combat misinformation, such as building content-recommendation systems that don’t prioritize engagement above all else. Lisa Gilbert, the executive vice-president of the advocacy group Public Citizen, argued Friday that the accord is “not enough” and AI companies should “hold back technology” such as hyper-realistic text-to-video generators “until there are substantial and adequate safeguards in place to help us avert many potential problems”. In addition to the companies that helped broker Friday’s agreement, other signatories include chatbot developers Anthropic and Inflection AI; voice-clone startup ElevenLabs; chip designer Arm Holdings; security companies McAfee and TrendMicro; and Stability AI, known for making the image-generator Stable Diffusion. Notably absent is another popular AI image-generator, Midjourney. The San Francisco-based startup didn’t immediately respond to a request for comment Friday. The inclusion of X – not mentioned in an earlier announcement about the pending accord – was one of the surprises of Friday’s agreement. Musk sharply curtailed content-moderation teams after taking over the former Twitter and has described himself as a “free-speech absolutist”. In a statement Friday, the X CEO Linda Yaccarino said “every citizen and company has a responsibility to safeguard free and fair elections”. “X is dedicated to playing its part, collaborating with peers to combat AI threats while also protecting free speech and maximizing transparency,” she said."
    },
    {
        "id": "27551b40-94b2-4f93-88e4-8aeb84893d00",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/16/bose-ultra-open-earbuds-review-unique-open-fit-and-great-sound",
        "title": "Bose Ultra Open earbuds review: unique open-fit and great sound",
        "author": "",
        "published_date": "2024-02-16T07:00:39+00:00",
        "content": "Bose’s latest earbuds are very different from its previous noise-cancelling champs, designed to let the sound of the outside in, rather than blocking it out and attaching to your ear, more like jewellery than gadgets. They are the latest evolution of Bose’s open audio tech that uses small speakers to play music into your ears without blocking them, last seen on the excellent Frames audio glasses that have been discontinued. The Ultra Open earbuds cost a hefty £300 (€349/$299) and join a small niche of open earbuds such as the £179 Shokz OpenFit or £149 Sony LinkBuds. But the Bose have two big advantages over their rivals: great, full sound and a novel fit that does not get in the way of glasses or other headwear. A battery cylinder sits behind the ear linked to the L-shaped plastic speaker arm via a flexible silicone spring, which together hold the earbuds in place around the side of your ear. The fit is very alien and takes a little practice to get right, but once hooked in place they are light and comfortable. I was certain they would fall off on a run yet they stayed locked on regardless of how sweaty I got or how hard I shook my head. The button on the top of the battery cylinder takes care of controls. Press once, twice and thrice for playback control or press and hold to activate your choice of shortcut, which cycles between modes by default. A double-press and hold manually adjusts the volume, which is awkward to make precise adjustments. Instead the earbuds have an auto-volume system that quickly turns the sound up and down to adjust to ambient noise levels. It works but is sometimes too sensitive, quickly ramping up when blowing your nose or rustling a packet of crisps for instance. I only turned it on when running. Specifications  Water resistance: sweat resistant (IPX4) Connectivity: Bluetooth 5.3 (SBC, AAC, aptX Adaptive) Battery life: up to 7.5h (27h with case) Earbud weight: 6.4g each Earbud dimensions: 19 x 17 x 27mm Charging case weight: 44g Charging case dimensions: 42 x 65 x 26mm Case charging: USB-C  Quality sound One of the best features is just how good they sound. They produce full range audio with solid bass and a quality typically associated with full earbuds, not open-fit types, which tend towards the tinny. They can’t quite hit the deepest of notes and are bettered by a good set of fully sealed earbuds, but they sound more than good enough to do justice to most genres for a nice, rounded and easy-listening sound. Because they do not block your ear canal, you have full awareness of your surroundings, which is ideal for running or simply walking about city streets. The speakers have a clever port system along the top that cancels out some of the sound that escapes from your ears. It works well enough that only those in very close proximity will be able to hear your earbuds and only when they are cranked up loud. The earbuds also have Bose’s immersive or spatial audio mode from its latest noise-cancelling models, which simulates a pair of speakers placed in front of you giving some tracks a much wider and less compressed sound. They support higher quality audio with Qualcomm’s Snapdragon Sound certified devices, which include a small selection of Android phones. Otherwise they are standard Bluetooth 5.3 earbuds, each of which can be used on their own but can connect to only one device at a time. A Bluetooth multipoint update is in the pipeline. Call quality is good with both ends coming through clearly, but my voice sounded a bit distant compared with the best, such as the QC Ultra earbuds and Apple’s AirPods Pro. The battery lasts for seven hours of constant playback in my testing, which is pretty long for earbuds but just shy of Bose’s estimates. The case can charge the earbuds about 2.5 times, which adds up to more than 24 hours of total playback time. They also have a long 48-hour standby time, meaning you can wear them all day and use them for intermittent calls or audio without needing to recharge. Sustainability Bose estimates the batteries will last in excess of 500 full-charge cycles but they are not replaceable and the earbuds are not repairable, ultimately making them disposable. Some replacement parts will be made available. The earbuds do not contain recycled materials. Bose offers discounts for returning broken products. It does not publish individual product environmental impact reports but does publish annual sustainability reports. Price The Bose Ultra Open earbuds cost £299.95 (€349/$299) and are available in black or white. For comparison, the QuietComfort Ultra earbuds cost £270, the Sony LinkBuds cost £149, the Shokz OpenFit cost £179 and the Apple AirPods cost £169. Verdict The Bose Ultra Open offer a novel approach to open-ear listening that does not compromise on comfort or sound. The behind the back of the ear fit keeps them out of the way of glasses and other headwear, while being secure and light enough to forget they are there. They are the best-sounding open earbuds by some margin and do not bleed sound to people around you as much as most rivals. But they are also twice the price of some good competitors and, like most earbuds, the battery is irreplaceable ultimately making them disposable. They will not be for everyone, but for those that can afford them and want a set of earbuds they can wear all day while hearing still the outside world, the Bose Ultra Open are the best available.  Pros: novel open fit, light and comfortable, excellent sound, long battery life, good case, sweat resistance, solid controls, AptX Adaptive/Snapdragon Sound but only for certain Androids. Cons: no sound isolation, very expensive, larger than traditional earbuds, unrepairable.  "
    },
    {
        "id": "93e22d77-6ed1-4bd6-af2a-03a63d20d791",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/15/openai-sora-ai-model-video",
        "title": "Sora: OpenAI launches tool that instantly creates video from text",
        "author": "",
        "published_date": "2024-02-15T20:11:17+00:00",
        "content": "OpenAI revealed a tool on Thursday that can generate videos from text prompts. The new model, nicknamed Sora after the Japanese word for “sky”, can produce realistic footage up to a minute long that adheres to a user’s instructions on both subject matter and style. According to a company blogpost, the model is also able to create a video based on a still image or extend existing footage with new material.   Related: OpenAI debuts GPT Store for users to buy and sell customized chatbots   “We’re teaching AI to understand and simulate the physical world in motion, with the goal of training models that help people solve problems that require real-world interaction,” the blogpost reads. One video included among several initial examples from the company was based on the prompt: “A movie trailer featuring the adventures of the 30-year-old space man wearing a red wool knitted motorcycle helmet, blue sky, salt desert, cinematic style, shot on 35mm film, vivid colors.” The company announced it had opened access to Sora to a few researchers and video creators. The experts would “red team” the product – test it for susceptibility to skirt OpenAI’s terms of service, which prohibit “extreme violence, sexual content, hateful imagery, celebrity likeness, or the IP of others”, per the company’s blogpost. The company is only allowing limited access to researchers, visual artists and film-makers, though CEO Sam Altman responded to users’ prompts on Twitter after the announcement with video clips he said were made by Sora. The videos bear a watermark to show they were made by AI.  Introducing Sora, our text-to-video model.Sora can create videos of up to 60 seconds featuring highly detailed scenes, complex camera motion, and multiple characters with vibrant emotions.                                 https://t.co/7j2JN27M3WPrompt: “Beautiful, snowy… pic.twitter.com/ruTEWn87vf— OpenAI (@OpenAI) February 15, 2024  The company debuted the still image generator Dall-E in 2021 and generative AI chatbot ChatGPT in November 2022, which quickly accrued 100 million users. Other AI companies have debuted video generation tools, though those models have only been able to produce a few seconds of footage that often bears little relation to their prompts. Google and Meta have said they are in the process of developing generative video tools, though they have not released them to the public. On Wednesday, it announced an experiment with adding deeper memory to ChatGPT so that it could remember more of its users’ chats.  https://t.co/uCuhUPv51N pic.twitter.com/nej4TIwgaP— Sam Altman (@sama) February 15, 2024  OpenAI did not disclose how much footage was used to train Sora or where the training videos may have originated, other than telling the New York Times that the corpus contained videos that were both publicly available and licensed from copyright owners. The company has been sued multiple times for alleged copyright infringement in the training of its generative AI tools, which digest gargantuan amounts of material scraped from the internet and imitate the images or text contained in those datasets."
    },
    {
        "id": "aa5a9d22-71da-443a-90ed-041074fe03ed",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/15/google-stops-notifying-publishers-of-right-to-be-forgotten-removals-from-search-results",
        "title": "Google stops notifying publishers of ‘right to be forgotten’ removals from search results",
        "author": "",
        "published_date": "2024-02-15T18:18:15+00:00",
        "content": "Google has quietly stopped telling publishers when it has removed websites from its search results under European “right to be forgotten” rules after a ruling in a Swedish court which the search engine is applying globally. Previously, when an individual applied to have records about them expunged under EU data protection laws, Google would notify the publisher of the original articles.  Media companies, including the Guardian, are largely exempt from the regulations, but links to journalistic content can still be removed from databases including those of search engines. Now Google only informs publishers of the fact that a URL has been removed, without elaborating on what or why.  That leaves journalists unable to identify situations where the right to be forgotten has been misused to hide legitimate reporting on serial miscreants, and hampers their ability to challenge the most serious abuses of the right. A Google spokesperson said: “We rolled out our new approach to notifications following a decision by Sweden’s data protection authority, which entered into force in December 2023 after the Swedish administrative supreme court refused us leave to appeal. “We disagreed with the decision at the time – but it’s binding and confirms pan-EU regulatory guidance. We have therefore undertaken a strong effort to comply with it.” One of the changes introduced by GDPR in 2016 was to ensure that EU national court decisions on data protection set precedent across the bloc.  In its December ruling, the Swedish court ruled that informing webmasters that the search engine had removed links to their content was itself a breach of privacy of the person making the right to be forgotten request. “The administrative court, therefore, finds that an individual’s interest in effective protection of their privacy and personal data, after Google has granted the removal of a search result, generally outweighs the interests of Google until to [sic] sends messages to webmasters,” it concluded, according to the International Association of Privacy Professionals.  It upheld a fine of SEK 50m (£3.8m) against the search engine for not removing URLs that it had been asked to delist. Within weeks of the original May 2014 court ruling that established that the right to be forgotten applied to Google, six Guardian articles had been removed from European versions of the search engine.  Three related to the now-retired Scottish Premier League referee Dougie McDonald, who in 2010 was found to have lied about his reasons for granting a penalty in a football match. Others included a 2002 article about a solicitor facing a fraud trial and 2011 piece on French office workers making Post-it note art. In the following five years, the search engine received almost a million discrete requests to be forgotten, and despite rejecting more than half, still removed almost 1.5m individual URLs."
    },
    {
        "id": "e17ef7a7-282a-47b4-839b-19cf3f7bb9db",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/15/23andme-hack-data-genetic-data-selling-response",
        "title": "Hackers got nearly 7 million people’s data from 23andMe. The firm blamed users in ‘very dumb’ move",
        "author": "",
        "published_date": "2024-02-15T14:26:22+00:00",
        "content": "Three years ago, a man in Florida named JL decided, on a whim, to send a tube of his spit to the genetic testing site 23andMe in exchange for an ancestry report. JL, like millions of other 23andMe participants before him, says he was often asked about his ethnicity and craved a deeper insight into his identity. He said he was surprised by the diversity of his test results, which showed he had some Ashkenazi Jewish heritage. JL said he didn’t think much about the results until he learned of a huge breach at the company that exposed the data of nearly 7 million people, about half of the company’s customers. Worse, he later learned of a hacker going by the pseudonym “Golem” who had offered to sell the names, addresses and genetic heritage reportedly belonging to 1 million 23andMe customers with similar Ashkenazi Jewish heritage on a shadowy dark web forum. Suddenly, JL worried his own flippant decision to catalog his genes could put him and his family at risk.   Related: Genetic testing firm 23andMe admits hackers accessed DNA data of 7m users   “I didn’t know my family was going to potentially be a target,” he said. “I may have put my family and myself in danger for something I did out of curiosity more than anything.” JL, who asked to only be identified by his initials due to the ongoing privacy issues, is one of two plaintiffs listed in a recent class-action lawsuit filed in California against 23andMe. Plaintiffs claim the company failed to adequately notify users of Jewish and Chinese heritage after they were allegedly targeted. The lawsuit claims hackers placed those users in “specially curated lists” that could have been sold to individuals looking to do harm. 23andMe has since confirmed hackers gained access to 14,000 user accounts over a span of five months last year, some of which revealed detailed, sensitive reports on users’ health. The company revealed details on the exact types of data stolen in its months-long breach in a January data breach notification letter sent to California’s attorney general earlier last month. Hackers accessed users’ “uninterrupted raw genotype data” and other highly sensitive information, like health predisposition reports and carrier-status reports gleaned from the processing of a user’s genetic information. Worse still, 23andMe confirmed the thieves also accessed other personal information from up to 5.5 million people who opted in to a feature that lets them find and connect with genetic relatives. 23andMe only publicly acknowledged the hackers’ attacks after one user posted about the up-for-sale data on a 23andMe subreddit in early October. An investigation digging into the incident revealed hackers had actually been trying, sometimes successfully, to gain access since at least April 2023. The attacks had continued for nearly five months through the end of September. In an email sent to the Guardian, a 23andMe spokesperson said the company did not “detect a breach” within 23andMe systems and instead attributed the incident to compromised recycled login credentials from certain users. A far larger subsection of users had other, potentially less sensitive data exposed through 23andMe’s opt-in DNA relatives feature, which automatically lets the company share data between other users on the platform who they may be related to. In other words, hackers who gained access to a user’s account via the compromised passwords were also able to suck up data about potential relatives. The optional feature gives users insight into a variety of data points, including their relatives’ name, their predicted relationship, and the percentage of DNA shared with matches. It can also include an individual ancestry report, matching DNA segments, and uploaded photos. Eli Wade-Scott, one of the attorneys representing JL in the class-action lawsuit, said these allegedly ethnicity-specific groupings could amount to a “hit list”. Jay Edelson, another attorney representing those users, worried those lists of users could look attractive to terrorists looking to identify people of Jewish heritage. He also said Chinese intelligence agencies, which have a history of surveilling and intimidating dissidents abroad, could use the data to target people critical of the government or even nation states.   This is a total paradigm shift when it comes to the implications of a data breach  Jay Edelson    “This is a total paradigm shift when it comes to the implications of a data breach,” Edelson added. Months after it first became aware of the beach, 23andMe sent a letter to several customers taking legal action against the company. The company defended itself by saying there was no way the breach could lead to real-world problems: “The information that was potentially accessed cannot be used for any harm.” It also cast blame for the hack on users who “negligently recycled and failed to update their passwords”. Cybersecurity professionals refer to the weaponization of these repeated digital keys as “credential stuffing” attacks. “Therefore,” 23andMe concluded, “the incident was not a result of 23andMe’s alleged failure to maintain reasonable security measures.” But multiple attorneys and genetic privacy experts say the company should have seen such an attack coming and done far more to safeguard this highly sensitive, intimate data. “You shouldn’t be able to do an attack like this over the course of months and have nobody at 23andMe notice,” said Wade-Scott. Barbara Prainsack, a University of Vienna professor for comparative policy, was herself a 23andMe customer. She said the company had a long time to protect itself and to establish data breach protocols. 23andMe, she said, seemed to have done neither: “This is almost a textbook case of how things should not be done.” She added that blaming consumers for their own relatively minor security lapses is “morally and politically very dumb”. 23andMe users suing the company for negligence seem to agree. They say they never would have bought the company’s kits had they known how lax its security was. Since the breach, more than two dozen 23andMe users have brought forward individual and class-action lawsuits accusing the company of negligence and invasion of privacy. The specifics of each of the lawsuits vary, but each argues the company failed to “implement and maintain adequate security measures”. “23andMe lied to customers about how it would protect their data, failed to reasonably protect their data in accordance with industry standards, lied about the scope and severity of the breach, failed to notify its Jewish and Chinese customers that they were specifically targeted, and in the end, exposed them to a host of threats and dangers that they’ll never see coming,” JL’s suit reads.   Related: Chinese hackers infiltrated plane, train and water systems for five years, US says   The slow-burning data breach scandal adds insult to injury to a company that has precipitously fallen from the highest rungs of Silicon Valley exceptionalism in recent years. The company went public in 2021 at a value of $3.5bn; now it is worth roughly $300m, a decline of 91%. 23andMe has never turned profit in its 18-year history. It may run out of cash by 2025. In only a few short years, the company that once seemed destined to become the “Google of spit” is struggling to remain on the Nasdaq in spite of co-founder and CEO Anne Wojcicki’s repeated attempts to quell investors’ concerns. Experts said the downstream consequences of hackers accessing breached genetic data remains largely hypothetical. Still, they warned a bad actor armed with this type of information and enough motivation could potentially use it to identify an individual or blackmail them by threatening to reveal even more sensitive information. The possible combination of data gleaned from the 23andMe breach with other personal information could result in sophisticated identity fraud. Murat Kantarcioglu, a professor of computer science at the University of Texas at Dallas said he could imagine a scenario where an attacker armed with data linking an individual to a previously unknown relative could blackmail them by threatening to make that connection public. Other data revealing a user’s family history with mental health issues, Kantarcioglu said, could possibly be misused by an employer to pass over someone seeking a job or promotion. At the time of writing, 23andMe requires two-factor authentication by default for all its users. That added layer of security, which critics had demanded for years, was only enabled by default after the breach. 23andMe says it also required all its customers to reset their passwords following the incident. Muddying matters even further, legal experts believe 23andMe recently made subtle changes to its terms of service making it more difficult for victims to join together to pursue mass arbitration lawsuits, TechCrunch reported. Those changes reportedly came just two days before 23andMe officially disclosed the data breach. 23andMe denies accusations it altered its terms of service to dissuade lawsuits and has instead said it made the changes to make resolutions for disputes occur faster. “Customers continue to retain the right to seek public injunctive relief,” a 23andMe spokesperson said in an email. “In the middle of the night, they [23andMe] changed their terms to game the system and make it basically impossible to bring any sort of large volume of arbitration,” Edelson said. Cohen Milstead partner Doug McNamara described the maneuver as a “desperate attempt to dissuade and deter from suing [23andMe]” in a December interview with TechCrunch.   The way that the information is being bought and sold, it’s kind of Defcon One in the privacy world  Jay Edelson    Nearly a year has passed since hackers first tried gaining access to 23andMe users’ accounts, but the company’s legal and regulatory worries are probably just beginning. Aside from the metastasizing lawsuits, lawmakers are getting involved. In January, New Jersey Democratic representative Josh Gottheimer wrote a letter to FBI director Christopher Wray urging the agency to launch an investigation into the company to determine whether or not the exposed data could be used to target Jewish communities. That came on the heels of a letter sent to 23andMe by Arizona attorney general Kris Mayes seeking additional data on the company’s security protocols. Experts fear the ripple effects of the 23andMe breach could extend beyond the company itself. Prainsack worries anxiety stemming from the breach may make people less likely to share personal health data, not just with 23andMe, but more traditional doctors as well. That lack of trust could make it more difficult to properly treat patients. Kantarcioglu, from UT Dallas, said this probably wouldn’t be the last data breach of its kind to affect genetic testing companies. “You have extremist groups calling for the death of Jews throughout the world, so it’s hard to see how the stakes could be higher,” Edelson, JL’s attorney, said. “The way that the information is being bought and sold, it’s kind of Defcon One in the privacy world.”"
    },
    {
        "id": "35660650-0b13-4846-95bd-4622199deda4",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/15/elon-musk-moves-spacex-to-texas-tesla-pay-delaware-judge",
        "title": "Elon Musk moves SpaceX incorporation to Texas after Delaware judge axed $56bn Tesla pay ",
        "author": "",
        "published_date": "2024-02-15T03:59:07+00:00",
        "content": "Rocket company SpaceX has moved its state of incorporation to Texas from Delaware, CEO Elon Musk has announced. “SpaceX has moved its state of incorporation from Delaware to Texas! If your company is still incorporated in Delaware, I recommend moving to another state as soon as possible,” Musk said on the platform. The move comes after a Delaware judge ruled last month in favor of Tesla investors who had launched a lawsuit to challenge Musk’s $56bn pay package as excessive. Musk, who is also CEO of Tesla said, earlier this month that he would hold a shareholder vote to move Tesla’s state of incorporation to Texas as well.   Related: Three cheers for the Delaware judge who stood up to Elon Musk   “The public vote is unequivocally in favor of Texas! Tesla will move immediately to hold a shareholder vote to transfer state of incorporation to Texas,” Musk said on X earlier this month after holding a poll where 87% of respondents voted “yes” for Tesla’s change of incorporation. In January, the Delaware judge found that Musk’s compensation was inappropriately set by the electric-vehicle maker’s board and struck down the package. The decision followed a lawsuit launched by Tesla shareholder Richard Tornetta five years ago, which accused Musk of improperly dictating negotiations around the compensation package, and claimed that the board had acted without independence. Tesla’s agreement with Musk is by far the largest compensation deal ever for an executive and it contributes a significant part of his fortune, which is one of the world’s largest. Musk testified during the compensation trial in November 2022 that the money would be used to finance interplanetary travel. Musk’s brain-chip implant company, Neuralink, also changed its location of incorporation from Delaware to Nevada last week. Reuters contributed to this report"
    },
    {
        "id": "2a0bc5fb-6b7f-4a33-94d9-3474a974e512",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/14/north-korea-iran-ai-hacking-microsoft",
        "title": "North Korea and Iran using AI for hacking, Microsoft says",
        "author": "",
        "published_date": "2024-02-14T19:52:35+00:00",
        "content": "US adversaries – chiefly Iran and North Korea, and to a lesser extent Russia and China – are beginning to use generative artificial intelligence to mount or organize offensive cyber operations, Microsoft said on Wednesday. Microsoft said it detected and disrupted, in collaboration with business partner OpenAI, many threats that used or attempted to exploit AI technology they had developed. In a blogpost, the company said the techniques were “early-stage” and neither “particularly novel or unique” but that it was important to expose them publicly as US rivals leveraging large-language models to expand their ability to breach networks and conduct influence operations. Cybersecurity firms have long used machine-learning on defense, principally to detect anomalous behavior in networks. But criminals and offensive hackers use it as well, and the introduction of large-language models led by OpenAI’s ChatGPT upped that game of cat-and-mouse. Microsoft has invested billions of dollars in OpenAI, and Wednesday’s announcement coincided with its release of a report noting that generative AI is expected to enhance malicious social engineering, leading to more sophisticated deepfakes and voice cloning. A threat to democracy in a year where over 50 countries will conduct elections, magnifying disinformation and already occurring, Microsoft provided some examples. In each case it said all generative AI accounts and assets of the named groups were disabled: • The North Korean cyber-espionage group known as Kimsuky has used the models to research foreign thinktanks that study the country, and to generate content likely to be used in spear-phishing hacking campaigns. • Iran’s Revolutionary Guard has used large-language models to assist in social engineering, in troubleshooting software errors and even in studying how intruders might evade detection in a compromised network. That includes generating phishing emails “including one pretending to come from an international development agency and another attempting to lure prominent feminists to an attacker-built website on feminism”. The AI helps accelerate and boost the email production. • The Russian GRU military intelligence unit known as Fancy Bear has used the models to research satellite and radar technologies that may relate to the war in Ukraine. • The Chinese cyber-espionage group known as Aquatic Panda – which targets a broad range of industries, higher education and governments from France to Malaysia – has interacted with the models “in ways that suggest a limited exploration of how LLMs can augment their technical operations”. • The Chinese group Maverick Panda, which has targeted US defense contractors among other sectors for more than a decade, had interactions with large-language models suggesting it was evaluating their effectiveness as a source of information “on potentially sensitive topics, high profile individuals, regional geopolitics, US influence, and internal affairs”. In a separate blog published on Wednesday, OpenAI said its current GPT-4 model chatbot offers “only limited, incremental capabilities for malicious cybersecurity tasks beyond what is already achievable with publicly available, non-AI powered tools”. Cybersecurity researchers expect that to change. Last April, the director of the US Cybersecurity and Infrastructure Security Agency, Jen Easterly, told Congress that “there are two epoch-defining threats and challenges. One is China, and the other is artificial intelligence.” Easterly said at the time that the US needed to ensure AI is built with security in mind. Critics of the public release of ChatGPT in November 2022 – and subsequent releases by competitors including Google and Meta – contend it was irresponsibly hasty, considering security was largely an afterthought in their development. “Of course bad actors are using large-language models – that decision was made when Pandora’s Box was opened,” said Amit Yoran, chief executive of the cybersecurity firm Tenable. Some cybersecurity professionals complain about Microsoft’s creation and hawking of tools to address vulnerabilities in large-language models when it might more responsibly focus on making them more secure. “Why not create more secure black-box LLM foundation models instead of selling defensive tools for a problem they are helping to create?” asked Gary McGraw, a computer security veteran and co-founder of the Berryville Institute of Machine Learning. The NYU professor and former AT&T chief security officer Edward Amoroso said that while the use of AI and large-language models may not pose an immediately obvious threat, they “will eventually become one of the most powerful weapons in every nation-state military’s offense”."
    },
    {
        "id": "79d6ec73-523f-469d-bcf7-3c0c28fb70bd",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/15/victoria-police-were-asked-to-look-at-hyperverse-information-in-2020-but-sent-case-back-to-asic-22-months-later",
        "title": "Victoria police were asked to look at HyperVerse information in 2020 – but sent case back to Asic 22 months later",
        "author": "",
        "published_date": "2024-02-14T14:00:43+00:00",
        "content": "Australia’s corporate watchdog referred information about the alleged US$1.89bn “Ponzi scheme” known as HyperVerse to Victoria police in 2020, only for it to be referred back almost two years later without any action being taken.\nThe Australian Securities and Investments Commission made the referral to Victoria police for alleged “possible fraud offences” after concerns were raised with the corporate regulator about a related company, Blockchain Global. Asic did not provide detail on what prompted the referral.\nThe HyperVerse crypto investment schemes were run by the HyperTech group, which was founded by two of Blockchain Global’s directors, Sam Lee and Ryan Xu.\n\n Related: ‘I lost my house, I lost all my money’: the retiree taking on the banks over crypto fund loss \n\n“Asic referred information relating to the HyperVerse matter to Victoria Police in 2020, after being advised that VicPol were making inquiries into the [alleged] conduct, and after determining that it was not a financial product and that the police were best placed to investigate [alleged] possible fraud offences,” an Asic spokesperson said.\nNeither Asic nor Victoria police would provide further detail on the alleged conduct.\n“Asic takes any misconduct seriously that harms investors and we have the power to act against misconduct relating to financial products and services,” the spokesperson said.\n“Where we are made aware of conduct that does not fall within our jurisdiction, we seek to refer information about the conduct to the appropriate agency.”\nHowever, Victoria police said it assessed the information and decided almost two years later that Asic was “best placed to look at it further”.\nIn the intervening period, Blockchain Global collapsed, owing creditors $58m, and Lee and Xu promoted what the US Securities and Exchange Commission has alleged was a “global, crypto asset-related, multi-level marketing pyramid and Ponzi scheme” that allegedly defrauded investors globally of US$1.89bn.\nXu is not named in the SEC legal action.\nA spokesperson for Victoria police confirmed a referral was received from Asic in April 2020, but the matter was not assessed until 2021. After that assessment, “it was determined the lead agency should be Asic”.\n“It was assessed and an investigation was not subsequently conducted by VicPol following that assessment – it was determined Asic were best placed to look at it further,” the spokesperson said.\n \n\n\n\nInvestors were offered “memberships” to the HyperVerse, which was a “blockchain community” that allowed members to “explore the HyperVerse ecosystem”.\nThe minimum membership amount was $300USD, which was converted to HyperUnits once invested.\nThe scheme offered minimum returns of 0.5% a day, with a 300% return over 600 days.\nMembers were incentivised to “reinvest” their earnings, with more HyperUnits offered if funds were not withdrawn once they became available.\nMembers were also paid HyperUnits to recruit new members, with referral commissions paid on a sliding scale according to how many people were recruited. Further commissions were paid based on how many people these recruits then recruited, down to 20 levels.\nHyperUnits were linked to various crypto tokens and once matured could be withdrawn and converted to other cryptocurrencies.\nWhile early investors profited and were able to make withdrawals, the scheme has left many investors unable to access their funds.\n\n\n\n\n\nThe matter was transferred back to Asic in January 2022.\nAsked why the process took 22 months, the Victoria police spokesperson said: “All [alleged] matters of this nature are first assessed to see if any criminal offences have been committed and if it is best placed to sit with Victoria Police. Depending on circumstances, this can take some time.”\nThe spokesperson declined to comment on what the assessment entailed.\nAsic said it believed the referral was being acted on.\n“Asic understood that the matter was under active consideration by VicPol. VicPol is ultimately best placed to explain their decision to refer the matter back to Asic,” the spokesperson said.\n“At the time VicPol referred the matter back to Asic, external administrators had been appointed to Blockchain Global. Asic is currently considering the information contained in liquidators’ reports in relation to the scheme.”\nAt the time of the Asic referral to Victoria police, the first of the Hyper schemes – HyperCapital – was under way, having launched in Hong Kong in 2019. HyperCapital was rebranded as HyperFund in 2020, becoming HyperVerse in December 2021.\nLee, who has denied claims the scheme was a scam and has said his role in HyperVerse was confined to the technology and funds management side of the business, faces both criminal and civil charges in the US for allegedly being “centrally” involved in the scheme. Xu has not been charged and is not named in the US proceedings.\nLee and Xu, and the fellow Blockchain Global director Allan Guo, have been referred to Asic by the liquidator in his report for alleged potential breaches of the Corporations Act in relation to Blockchain Global.\nGuo is not believed to be involved in the HyperVerse scheme and is not named in the US proceedings. He has not responded to questions from Guardian Australia.\nAfter Guardian Australia’s investigation into HyperVerse, Asic announced it would assess the liquidator’s report on Blockchain Global.\nThe SEC complaint against Lee alleges that he used his role as a founder of Blockchain Global – even after it was in administration – to promote the HyperFund and HyperVerse schemes.\n“Lee touted his business prowess by introducing himself as the CEO of Blockchain Global. At that time, however, that statement omitted information that would have been material to investors given the context in which it was made – namely, Blockchain Global had ceased business operations, a fact Lee knew or recklessly disregarded,” the SEC complaint says.\n\n Related: HyperVerse crypto scheme targeted developing countries before collapse left some investors ‘suicidal’ \n\n“Lee also failed to mention that he stepped down from Blockchain Global as a director, and that the company no longer had any operations.”\nThe court documents allege that a senior US promoter, Brenda Chunga, who has also been charged and has pleaded guilty to conspiracy to commit securities and wire fraud, relied heavily on the HyperTech group’s links with Blockchain Global to promote the scheme to potential investors.\n“In HyperVerse presentations … Chunga continued to stress the connection with Blockchain Global and why this connection supposedly gave credibility to the HyperFund project and increased the safety of the investment.”\nAsic has defended its failure to issue a warning about the HyperFund and HyperVerse investment schemes, which were the subject of multiple investor warnings overseas throughout 2021 and 2022.\nLee has declined to answer questions from Guardian Australia, while Xu could not be reached for comment."
    },
    {
        "id": "bd7dbd73-2738-418d-ac87-f0bb1252a258",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/13/jeff-bezos-florida-taxes",
        "title": "Jeff Bezos could save $600m in taxes after moving to Florida",
        "author": "",
        "published_date": "2024-02-13T17:28:36+00:00",
        "content": "The multibillionaire Jeff Bezos and his fiancee, Lauren Sanchez, could save $600m in taxes simply because they moved to Florida. The accounting windfall stems from a $2bn sale of Amazon stock. Bezos, 59, who is in a three-way race with Microsoft’s Bill Gates and Tesla’s Elon Musk for the title of wealthiest American, announced in November he was leaving Seattle after 30 years – ostensibly, he said in an Instagram post, to be nearer his parents and his Blue Origin rocket launches at Cape Canaveral. “I’ve lived in Seattle longer than I’ve lived anywhere else and have so many amazing memories here. As exciting as the move is, it’s an emotional decision for me. Seattle, you will always have a piece of my heart,” he wrote. But on Tuesday the financial news network CNBC offered another clue to his move: two years ago, Washington introduced a new 7% capital gains tax on sales of stocks or bonds of more than $250,000. Florida, however, does not levy taxes on income or capital gains. Bezos, the outlet reported, has been selling billions of dollars of Amazon stock since 1998 to fund his philanthropic operations, Blue Origin, and some of his splashy acquisitions, including new homes on Miami’s “billionaire bunker” island Indian Creek and a $500m 417ft mega-yacht, the Koru. Last year, after the new tax, Bezos stopped selling Amazon stock – that is until he notified the US Securities and Exchange Commission that he plans to unload 50m shares before 31 January 2025 that at current values would amount to $8.7bn. On the first $2bn tranche last week, Bezos saved $140m in taxes he would have paid to Washington state, and on the entire sale over the next two years the figure would be about $610m or more, if Amazon stock continues to rise. The savings will nicely cover the expense of the Kuro. The cost of moving to Miami has been onerous in own right for the couple, who have so far splashed out $147m on two Indian Creek mansions, near the residences of the quarterback Tom Brady, Ivanka Trump and her husband, Jared Kushner, and the investor Carl Icahn. Miami real estate brokers say Bezos is likely to tear down the two homes and build a new one. He is also said to be looking at other properties on the island, which is its own municipality and has its own mayor. The architect Kobi Karp, who has worked in Miami “since the Miami Vice and Scarface days”, told Bloomberg that the island’s biggest draw is privacy. Paul George, the resident historian at the HistoryMiami Museum told outlet that the island is “only the very wealthy, the billionaires”. Karp added: “Hundreds of millions aren’t gonna cut it any more.” But Bezos’s personal real estate projects may not fully to account for what it takes to relocate a multibillionaire, his fiancee and their support staff across a continent. Soon after Bezos announced the couple’s move to Miami, Amazon announced it was looking for 50,000 sq feet of office space in nearby Brickell."
    },
    {
        "id": "e36f689b-fabe-4f87-94f7-413500f18288",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/13/bluesky-twitter-jack-dorsey-social-network",
        "title": "TechScape: Bluesky opens up to the world – but can anything really replace Twitter?",
        "author": "",
        "published_date": "2024-02-13T11:45:43+00:00",
        "content": "Last week, Bluesky opened up its doors. After a year in an invite-only closed beta, anyone who wants it can now sign up for an account with only an email address. Even if the value of an invitation had lessened somewhat in recent months (I have five sitting unused, and not for want of trying), there was clearly some pent-up demand. In just two days, the service has seen more than a million new signups. For comparison, it took Bluesky more than three months from the release of its iOS app last February until it hit 70,000 users. That slow growth has been a mixed blessing for Bluesky. On the one hand, the beta has done its job, allowing the company to iterate its service based on feedback from a smaller, if no less hardcore, base of users than it would otherwise have. In the past year, it has built up a suite of moderation tools that allow for its “decentralised” approach to social networking (where users are allowed to voluntarily opt in or out of content such as nudity, violence and hate speech); it’s launched on Android and the web; and it’s created a coherent visual identity, adopting a butterfly as its own version of Twitter’s bird. Elon Musk’s site, meanwhile, has correspondingly destroyed its own visual shorthand, replacing some but not all Twitter branding with a double-struck 𝕏. But the slow burn also meant that Bluesky didn’t have a true moment in the sun. For the past year, I’ve heard a fairly common refrain from new users: a rush of excitement at being allowed access to the service, followed by the deflating realisation that it has less content than Twitter had at its peak. Sometimes, that’s a trick of the memory. A Bluesky feed following 50 users is going to be less busy than a Twitter feed following 500, and the days when everyone they followed could fit in a double-decker bus are a distant past for many power users of the latter service. Many long-time Twitter users won’t remember what it was like to have a quiet feed or need to spot new people to follow, and those things can cause people to bounce off a new social network like Bluesky just as they caused so many to stumble while signing up for Twitter itself. (That is why the “people you may know” feature on Facebook so uncannily perceptive: the social network has always known that the biggest hurdle isn’t getting someone to sign up, but getting them to follow enough other users that they stick around after.) Even if you do the hard graft to build up a Bluesky account with a reasonably full address book, the service can’t possibly match up to the rosy memories of Twitter at its peak. Sure, it’s a similar enough experience to stave off the craving for the real thing, but not compelling enough to drive you back for more. You post; no one responds; you log off and touch grass. All of this, of course, is talking about Bluesky as a nascent social network. But the company doesn’t see it that way. Chief executive Jay Graber gave some interviews to mark the grand opening. Here she is speaking to Wired:  We weren’t using invites to try to be exclusive. We were using them to manage growth while we built out what is essentially a foundation, the rails for this new kind of distributed network. We had to build the app protocol beneath Bluesky, the AT Protocol, that lets different developers, companies, or people come in and modify their experiences. Some of it is going to be rolling out soon.  When it’s finished, the vision for Bluesky is to hover somewhere between a straight-up replacement for Twitter and a fully decentralised service like Mastodon, the second of the big three post-Twitter social networks: like Mastodon, the technology underpinning Bluesky should eventually allow your account to outlast the company that created it, but unlike Mastodon, Bluesky is less eager to foreground the technological differences between it and Twitter, with the vast majority of users remaining on the official app and service for the foreseeable future. And then there’s Threads. Meta’s Twitter clone is, unquestionably, the biggest of the three by user count alone, but it’s also barely made a ripple in the wider culture. The site’s policy of suppressing political content – it won’t get algorithmic promotion, according to Threads’ platform safety policies – doesn’t help matters. There are parallels with earlier periods of online culture, here: Twitter dominated discussion even while having a fraction of the size of Facebook, and TikTok does the same despite the vastly larger number of users on YouTube. Elizabeth Lopatto, at the Verge, explained the disconnect with a taxonomy so spot-on that I can’t do anything but quote her at length:  The silent majority of every successful text-based social media site is lurkers. These are sane, normal people with sane, normal lives … The influencer is building a business. They are making #content … The commenter is trying to have a conversation with another human being. They are hoping, however misguidedly, to have a meaningful interaction online … The reply guy can be thought of as the most important subclass of commenter; they are specific. They are usually interacting with or on behalf of a favored internet user … Finally, we have the poster, sometimes referred to as a poaster. The poster is required for every social network to function.  The issue facing all Twitter replacements is that the balance is off. Threads is massive, but its user base is lurkers and influencers. Like being in the audience of a Marvel movie, you may consume some professionally produced content, but you’re certainly not going to form any lasting memories. For the past year, Bluesky has been pure posters, locked in a room with each other, deprived of much of the dopamine that they need to maintain their frenetic energy. And Mastodon is a community of commenters and reply guys, decentralised to the point that it’s possible to have a nice chat, but difficult to discern a conversation arising from within. Opening up Bluesky, then, could be the first step to restoring some of that balance. Posters cannot survive on posters alone. They – we – need lurkers. Join us, or we’ll have to touch grass.  If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Tuesday. "
    },
    {
        "id": "6b74382f-4da2-490b-8cec-5e7f3fb7011c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/13/volt-typhoon-what-is-it-how-does-it-work-chinese-cyber-operation-china-hackers-explainer",
        "title": "Explainer: what is Volt Typhoon and why is it the ‘defining threat of our generation’?",
        "author": "",
        "published_date": "2024-02-13T05:20:21+00:00",
        "content": "Relations between the US and China – particularly over Beijing’s threats to annex Taiwan – have plummeted in recent years, prompting growing concern about the potential for hostilities or all-out conflict. So recent revelations that a Chinese hacking network known as Volt Typhoon had been lying dormant inside US critical infrastructure for as long as five years have sparked considerable alarm.\nThe network exploited US technological and security weaknesses. But rather than stealing secrets, US and allied intelligence services said it was focused on “pre-positioning” itself for future acts of sabotage.\nFBI director Christopher Wray told a US committee hearing last week that Volt Typhoon was “the defining threat of our generation”.\nThe Netherlands and Philippines have also recently publicly identified Chinese-backed hackers as targeting state networks and infrastructure.\nWhat is Volt Typhoon?\nWestern intelligence officials say Volt Typhoon – also known as Vanguard Panda, Brronze Silhouette, Dev-0391, UNC3236, Voltzite, and Insidious Taurus – is a state-supported Chinese cyber operation that has compromised thousands of internet-connected devices. They said it was part of a larger effort to infiltrate western critical infrastructure, including naval ports, internet service providers, communications services and utilities.\nThe new advisories on Volt Typhoon followed a recent announcement by US authorities that they had dismantled a bot network of hundreds of compromised devices, attributing it to the hacking network.\n\n Related: Chinese hackers infiltrated plane, train and water systems for five years, US says \n\n“CISA [Cybersecurity and Infrastructure Agency] teams have found and eradicated Chinese intrusions in multiple critical infrastructure sectors, including aviation, water, energy, [and] transportation,” US CISA director Jen Easterly told a US House committee hearing earlier this month.\nHow does it work?\nVolt Typhoon works by exploiting vulnerabilities in small and end-of-life routers, firewalls and virtual private networks (VPNs), often using administrator credentials and stolen passwords, or taking advantage of outmoded tech that hasn’t had regular security updates – key weaknesses identified in US digital infrastructure. It uses “living off the land” techniques, whereby malware only uses existing resources in the operating system of what it’s targeting, rather than introducing a new (and more discoverable) file.\nA report released last week by CISA, the National Security Agency, and the FBI, said Volt Typhoon hackers had maintained this access for the past five years, and while it has targeted only US infrastructure, the infiltration was likely to have affected the US’s “Five Eyes” allies of Canada, Australia, New Zealand, and the UK.\n \n\n\n\n\n\nWhat is its aim?\nUS authorities said Volt Typhoon’s unusual choice of targets and behavioural patterns were not consistent with traditional cyber espionage or intelligence gathering operations.\nVolt Typhoon has been active since mid-2021, according to a Microsoft investigation published last year. Targeting US infrastructure in Guam and elsewhere, Microsoft found it had been “pursuing development of capabilities that could disrupt critical communications infrastructure between the United States and Asia region during future crises”.\n“People’s Republic of China (PRC) state-sponsored cyber actors are seeking to pre-position themselves on IT networks for disruptive or destructive cyberattacks against US critical infrastructure in the event of a major crisis or conflict with the United States,” said the joint report.\nWhat does China say?\nBeijing routinely denies any accusations of cyber-attacks and espionage linked to or backed by the Chinese state. But evidence of Beijing’s cyber-espionage campaigns has been building for more than two decades. Spying has come into sharp focus over the past 10 years as western researchers tied breaches to specific units within the People’s Liberation Army, and US law enforcement charged a string of Chinese officers with stealing American secrets.\nSecureworks, an arm of Dell Technologies, said in a blog post last year that Volt Typhoon’s interest in operational security likely stemmed from embarrassment over the drumbeat of US indictments and “increased pressure from (Chinese) leadership to avoid public scrutiny of its cyber-espionage activity”.\nWhat’s next?\nThe widespread nature of the hacks has led to a series of meetings between the White House and the private technology industry, including several telecommunications and cloud computing companies, in which the US government asked for assistance in tracking the activity.\nInstitutions and assets targeted by the now dismantled botnet were ordered by CISA in January to disconnect affected devices and products, starting off an intensive and difficult process of remediation.\n“This was necessary given the degree of targeting and compromise around the world of the now three exploited vulnerabilities affecting these appliances,” Eric Goldstein, CISA’s executive assistant director for cybersecurity, told the Risky Business podcast.\n“Every organisation running these devices absolutely needs to assume targeting and assume compromise.”"
    },
    {
        "id": "e12f0ebe-bc73-44f6-b37b-bf67e56a67dc",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/13/joe-biden-tiktok-campaign-national-security-social-media",
        "title": "Biden campaign decision to join TikTok raises national security concerns",
        "author": "",
        "published_date": "2024-02-13T01:20:26+00:00",
        "content": "The chair of the US Senate Intelligence Committee, a senior Democrat, said he is concerned about the decision by the campaign of president Joe Biden to join TikTok. On Sunday, Biden’s reelection campaign used the Super Bowl to kick off its new TikTok account in an effort to reach young voters ahead of the presidential election in November. The campaign’s launch on TikTok is notable given that the app, which is owned by Chinese tech company ByteDance, is under review in the US due to potential national security concerns. Some US lawmakers have called for the app to be banned over concerns that the Chinese government could access user data or influence what people see on the app.   Related: How 2023 became the year Congress forgot to ban TikTok   On Monday, Democratic senator Mark Warner said he was concerned about the national security implications. “I think that we still need to find a way to follow India, which has prohibited TikTok,” Warner said. “I’m a little worried about a mixed message.” A number of Republican lawmakers have also criticised the campaign’s decision to join TikTok. White House spokesperson John Kirby said nothing has changed regarding the “national security concerns … about the use of TikTok on government devices. That policy is still in place.” Last year, the Biden administration ordered government agencies to remove TikTok from federal government-owned phones and devices. TikTok has maintained that it would not share US user data with the Chinese government and has taken substantial measures to protect the privacy of its users. The company did not immediately respond to a request from the Reuters news agency for comment. The Biden campaign said in a statement it would “continue meeting voters where they are,” including on other social media apps like Meta’s Instagram and Truth Social, which was founded by former president Donald Trump. The campaign is taking “advanced safety precautions” for its devices and its presence on TikTok was separate from the app’s ongoing security review, a campaign official added. The US Treasury-led Committee on Foreign Investment in the United States (CFIUS) in March 2023 demanded that TikTok’s Chinese owners sell their shares, or face the possibility of the app being banned, but the administration has taken no action. White House press secretary Karine Jean-Pierre said on Monday that the review by CFIUS is ongoing and noted the White House previously endorsed legislation introduced by Warner and others to give the administration new tools to address threats posed by foreign-owned apps. Last month, TikTok told Congress that 170 million Americans now use the short video platform, up from 150 million a year earlier. Reuters contributed to this report"
    },
    {
        "id": "33f33036-9e80-4a62-add1-122906f5681f",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/12/french-security-experts-identify-moscow-based-disinformation-network",
        "title": "French security experts identify Moscow-based disinformation network",
        "author": "",
        "published_date": "2024-02-12T20:47:38+00:00",
        "content": "French military and cybersecurity experts say they have identified a Moscow-based network spreading propaganda and disinformation in western Europe. France’s Viginum agency, which was set up in 2021 to detect digital interference from foreign entities aimed at influencing public opinion, says Russia is paving the way for a new wave of online manipulation in the run-up to the European elections and other crucial votes this year. The agency says the online network, which it has named “Portal Kombat”, includes at least 193 sites disseminating pro-Russian propaganda defending the Russian invasion of Ukraine and criticising the government in Kyiv. Much of the disinformation directed through social media sites and messaging apps is targeting those propagating conspiracy theories, it said. Viginum researchers, who identified and analysed the network between September and December last year, say the mass disinformation campaign can be traced back to Moscow. One pro-Russian channel on the French Telegram mobile and desktop messaging app is publishing “almost continuously” up to nine articles an hour. The European Commission, Nato and UN agencies have ranked disinformation among the biggest threats to democracy in 2024. At a recent high-level conference in Brussels, a senior Nato official said disinformation was now being classified as a “national security issue” and there was a recognition among allies that hybrid attacks using disinformation “could reach the level of an armed attack”. The UN’s communications secretary general, Melissa Fleming, told the conference that “disinformation [is] being used to create not just the fog of war, but more suspicion, and more hatred” and was undermining peacekeeping forces. Addressing the conference, the EU’s chief diplomat, Josep Borrell, said this new warfare was “not about bombs that can kill you” but about words and ideas “that can colonise your mind”. Věra Jourová, the EU’s vice president for values and transparency, with responsibility for media and disinformation portfolios, said: “Every day we see the Kremlin’s action to spread propaganda and interfere in democracies. From Putin’s blatant lies in broad daylight to a hidden network of propaganda sources now just unveiled, the Kremlin spares no effort. And neither should the EU.” She added that she welcomed the “strong determination” of France, Germany and Poland to “fight back”. The sites linked to the disinformation network do not produce original material but were set up to flood the internet with material from Russian and pro-Russian figures on social media, Russian press agencies and other official accounts loyal to Moscow, French defence specialists say. Since the Russian invasion of Ukraine two years ago, the sites have targeted Russian communities in Ukraine and “several western countries”, including France, Germany, Austria, Poland, Spain the UK and US. The Viginum report, released on Monday, states: “Although this network of at least 193 sites initially covered news from Russian and Ukrainian localities, it changed the day after Russia invaded Ukraine and started to target occupied Ukrainian territories, then several western countries supporting Ukraine and its population. “The main objective seems to be to cover the Russo-Ukrainian conflict by presenting positively ‘the special military operation’ and denigrating Ukraine and its leaders. Very ideologically oriented, this content repeatedly presents inaccurate or misleading narratives.” The report said the network also “directly contributes to polarise the Francophone digital public debate”. To reach a wide audience, it selects “pro-Russian propaganda sources according to the targeted locality, massive automation in the distribution of content, or search engines optimisation”. Viginum says the propaganda campaign involves three “ecosystems”, one of which uses the website name pravda followed by the country code top-level domains (fr, de, pl, es, com), set up in June 2023 and found to have “identical technical characteristics: a common IP address hosted on a server located in Russia”. “Furthermore, these sites broadcast content with similar pro-Kremlin narratives, particularly about the supposed legitimacy of ‘the special military operation’, denigrating Ukraine and its leaders, or criticising ‘the collective West’,” it says. Another network of websites targeted primarily Russian-speaking audiences in Ukraine and was set up between 3 April 2022 – just over a month after the Russian invasion – and 17 December 2022. “Some sites target very specific and strategic locations, such as Kherson or Mariupol,” the report says. Although most of the propaganda is focused on the Ukraine conflict, Viginum says last summer the French Pravda site published material on “different crises” involving the presence of French troops in the Sahel, including those in Niger and Gabon. However, security experts say the mass propaganda and disinformation campaign appears to be having limited success: the average traffic on the five portals in November 2023 was 31,000 visits, the one targeting France being the least visited."
    },
    {
        "id": "9d5e6827-d70b-4942-ada4-5398d670b78a",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/12/three-apologises-after-network-outages-affect-10000-customers-across-uk",
        "title": "Three apologises after network outages affect 10,000 customers across UK",
        "author": "",
        "published_date": "2024-02-12T16:02:48+00:00",
        "content": "Mobile operator Three has apologised as more than 10,000 customers experienced outages after problems with its service over the last few days. The company, which has more than 10 million UK customers, issued apologies in posts on X on Saturday, Sunday and Monday regarding a string of problems affecting voice calls and the ability to use mobile data. “We are very sorry for the issues with service over the past few days and sincerely apologise for any inconvenience caused,” Three said in a post on Monday. “We know a number of our customers are still waiting on their service to be restored, and our engineers are working to fully fix it.” Downdetector, which tracks outages, shows that more than 12,000 people have reported that their phone services have suffered some form of fault. A spokesperson for Three UK said the issue that affected customers over the weekend is separate to the problems that emerged on Monday, but the end result is the same type of disruption to the network for mobile phone users. “Following an issue with our network that started earlier, services are now recovering,” it said in an update on X.  Following an issue with our network that started earlier, services are now recovering. We know a number of our customers are still waiting on their service to be restored, and our engineers are working to fully fix it (1/2)— Three UK (@ThreeUK) February 12, 2024  Last month, the Competition and Markets Authority (CMA), the UK regulator, launched an investigation into the proposed merger of Vodafone and Three, which would create the UK’s largest mobile phone operator. The firms said the deal would result in an additional investment of £11bn in the UK. In 2016, the CMA and the European Commission blocked Three’s attempted takeover of O2, arguing that it would have risked higher prices. However, in 2022 Ofcom, the UK telecoms regulator, changed its long-held stance, saying it was now more open to consolidation in the sector. Ofcom had previously argued that a reduction to only three networks could harm UK consumers. In a separate move the government is also investigating the stake in Vodafone held by a United Arab Emirates-backed telecoms group. The Cabinet Office warned that the 14.6% stake held by Emirates Telecommunications Group, which is also known as e&, amounted to a security concern given Vodafone’s strategic role in UK telecoms services. The government has now ordered a “national security committee” to be set up at Vodafone that will oversee and monitor any sensitive work the telecoms firm carries out that could have an impact on national security. Vodafone UK has a number of government contracts, and Three UK is owned by the Hong Kong-based CK Hutchison, which may raise concerns about foreign joint-ownership of a key national asset, particularly given the city’s status as part of China. Vodafone UK has public sector contracts with the Ministry of Defence, the Ministry of Justice, NHS 111 and local police forces."
    },
    {
        "id": "5cbef769-9f97-4085-a892-90b99e068b22",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/12/honor-magic-v2-review-exquisite-hardware-let-down-by-software-phone-tablet-camera",
        "title": "Honor Magic V2 review: exquisite hardware let down by software",
        "author": "",
        "published_date": "2024-02-12T07:00:36+00:00",
        "content": "Honor’s Magic V2 is the best designed folding phone-tablet yet. It feels just like a regular phone when closed but then opens up like a book to reveal a large, plush screen. Launched in China last year, the Magic V2 has now made it to Europe but not at a price that could be considered affordable. At £1,700 (€2,000), it is placed between the Samsung Galaxy Z Fold 5 and OnePlus’s slightly cheaper Open. However, it has one major advantage over the competition: slenderness. Honor has managed to slim down the hinge and body to effectively remove the bulk that has burdened folding phones until now. Its big and bright outside screen has slim bezels and curved sides, making it feel and work just like a normal Android when closed, and when the tablet is opened it is only 4.8mm thick – the same as a stack of six credit cards – which is incredible. The internal 7.92in screen is slightly bigger than rivals while being just as bright, smooth and great looking. The crease in the middle is a little more visible than that of the Open but easily blends into the background in use. The hinge is rated to last for at least 400,000 folds and can hold the phone open at a range of angles. But the hinge doesn’t feel as firm as some rivals and the phone lacks any water or dust protection. Specifications  Main screen: 7.92in (402ppi) 120Hz OLED flexible display Cover screen: 6.43in (404ppi) 120Hz OLED Processor: Qualcomm Snapdragon 8 Gen 2 RAM: 16GB Storage: 512GB Operating system: MagicOS 7.2 (Android 13) Camera: 50MP + 50MP ultrawide + 20MP 2.5x tele; 2x 16MP selfie Connectivity: 5G, dual sim + esim, USB-C, wifi 7, NFC, Bluetooth 5.3, GNSS Water resistance: none Dimensions folded: 156.7 x 74 x 10.1mm Dimensions unfolded: 156.7 x 145.4 x 4.8mm Weight: 237g  Last year’s top Android chip The Magic V2 has Qualcomm’s top chip from 2023, the Snapdragon 8 Gen 2 – not the newer Gen 3 version that started to appear in phones last month. It still offers plenty of power for multitasking, gaming and anything else you might want to do with it, but isn’t the very latest technology despite the high asking price. The battery lasts a long time. Actively using the two screens for well over six hours and 5G for three hours, the battery lasts up to 49 hours between charges, meaning a recharge every two days. Sustainability Honor does not provide an expected lifespan for the battery but it should last in excess of 500 full charge cycles with at least 80% of its original capacity. The phone is generally repairable by Honor. Full battery replacements cost £110 and folding screen replacements cost £737.80 out of warranty. The phone contains recycled plastic and Honor publishes a breakdown of its environmental impact and offers trade-in schemes. MagicOS 7.2 lacks polish Software is the Magic V2’s biggest weakness. It ships with MagicOS 7.2 based on 2022’s Android 13, not Android 14 as you should expect from a new device in 2024. Honor will only provide four years of Android updates and a total of five years of security updates from release, too, which is at least two full years short of the benchmark set by Google, Samsung and Apple. MagicOS has some nice ideas, such as a mode to turn the phone into a digital desk clock when idle, but it is generally just a little rough around the edges compared with the best software experiences from competitors. It has advanced multitasking tools, including the ability to have up to four apps on screen: two in split-screen and two more in smaller free-floating pop-out windows. But if you tap on a notification for a message, such as an email or WhatsApp, the phone insists on opening the respective app in a mini floating window, not full screen. Sometimes that’s handy; other times it’s just irritating and there’s no way to stop it. I never want Gmail to open like a chat app. The phone also comes with unwanted apps preinstalled, including Booking.com, TikTok, Facebook, Instagram and WPS Office. They can all be uninstalled but this is a £1,700 phone and should not come with the kind of bloat you get on a budget device. Camera The Magic V2 has three cameras on the back, one selfie camera on the front and one on the inside screen. Both selfie cameras are solid but bettered for photos by the main cameras, which can easily shoot selfies using the outside screen as a viewfinder. The main 50MP camera captures good photos in a range of lighting conditions, although it tends towards high saturation of colours while frequently brightening scenes and losing contrast, making them look a bit flat. The 50MP ultrawide does a good job in bright scenes, while the 20MP 2.5x telephoto generally produces solid images. It doesn’t have much reach, though, particularly compared with the best competitors with double the optical magnification available. All three cameras struggle a little with lower light levels, losing fine detail and sharpness and becoming increasingly inconsistent at night. There is also a marked colour difference between the ultrawide and the other two cameras, which is disappointing at this price. None of these issues are deal breakers but Honor has some work to catch up with the best in the market. Price The Honor Magic V2 costs £1,699.99 (€1,999.90). For comparison, the Samsung Galaxy Z Fold 5 costs £1,749, the Google Pixel Fold costs £1,749 and OnePlus Open costs £1,599. Verdict The Honor Magic V2 is an exquisite piece of hardware with software that can’t quite live up to expectations. The folder is so close to being a regular phone when folded that it’s easy to forget a giant screen hides on the inside.  But the software is just not as polished or capable as rivals, nor is it supported for long enough. The lack of a water-resistance rating puts doubts on durability, while camera performance is a little weak in some areas. None of these issues are outright deal breakers and many could be fixed with updates. But they are disappointing for a device of this price, particularly when cheaper rivals do better on these fronts.  Pros: super-slim and light phone and tablet in one, just like a normal phone when closed, great performance, very long battery life, great internal screen, good fingerprint scanner. Cons: no water resistance, very expensive, costly to repair, software not as refined as needed, short software support, older chips, camera performance lags top competitors.  • This article was amended on 19 February 2024. An earlier version said Honor would only provide three years of Android updates."
    },
    {
        "id": "cdfa1dee-ba91-432e-9482-a420407cae09",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/11/musk-sec-investigation-twitter-takeover",
        "title": "Musk ordered to testify again in SEC investigation of Twitter takeover",
        "author": "",
        "published_date": "2024-02-11T19:34:21+00:00",
        "content": "Elon Musk has been ordered to testify again as part of an investigation by US regulators into his 2022 purchase of the social media platform Twitter, later renamed X. A California federal court ruling released on Saturday gave the Tesla and SpaceX chief a week to agree with the Securities Exchange Commission (SEC) on a date and place for the interview after Musk refused to attend a previous sit-down in September. In the order, the US magistrate judge Laurel Beeler said the parties had initially agreed to a date “but ultimately the respondent [Musk] did not appear and resists the subpoena on the grounds that the SEC’s investigation is baseless and harassing and seeks irrelevant information”. The SEC sued Musk in October to compel him to testify as part of an investigation into his $44bn purchase of what is now known as X. The commission also seeks his testimony on whether he followed the law when filling out the required paperwork about Twitter stock purchases as well as whether his statements in relation to buying the platform were accurate. According to the order, Musk’s attorneys had said he would not appear because regulators had leaked information to the media. Musk’s team also argued the investigation was frivolous, and it said document requests as well as demand for testimony again in the face of an investigation “arising from the accidental tardiness of a clerical filing” was troubling government action. Beeler, in forcing an interview, rejected that argument and said regulators had authority to issue the subpoena for relevant information. If the SEC and Musk cannot agree on a date and time for the interview, Beeler said she would hear from both sides and decide for them. The action dates back to a notorious 2018 tweet in which Musk said “funding secured” when he was attempting to take Tesla private. Regulators claimed that was a breach of a securities laws that prohibit publicly traded companies from announcing plans to buy or sell securities if executives do not intend to complete, do not have the means to complete, or are trying to manipulate the stock price. In a settlement, Musk agreed that a Tesla lawyer would vet his tweets about the electric vehicle maker. But regulators sued him again a year later for allegedly breaching the agreement. Musk then petitioned the US supreme court to review the agreement, saying it violates his right to free speech. In 2022, regulators asked Musk for information about the delayed disclosure of his Twitter stake, which he reported a week late. He testified twice that year, the SEC said. A third interview, Musk claims, amounts to government “harassment”. The dispute is not Musk’s only run-in with the government. In November, he lost a bid to stop the federal trade commission (FTC) from continuing to oversee X’s handling of private user data. Musk had called the agency’s action “a shameful case of weaponization of a government agency for political purposes and suppression of the truth!”"
    },
    {
        "id": "df3cdcf9-7a69-4950-a12a-adf399d79a92",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/11/ai-safety-institute-needs-to-set-standards-rather-than-do-testing",
        "title": "UK’s AI Safety Institute ‘needs to set standards rather than do testing’",
        "author": "",
        "published_date": "2024-02-11T11:25:46+00:00",
        "content": "The UK should concentrate on setting global standards for artificial intelligence testing instead of trying to carry out all the vetting itself, according to a company assisting the government’s AI Safety Institute. Marc Warner, the chief executive of Faculty AI, said the newly established institute could end up “on the hook” for scrutinising an array of AI models – the technology that underpins chatbots like ChatGPT – owing to the government’s world-leading work in AI safety. Rishi Sunak announced the formation of the AI Safety Institute (AISI) last year ahead of the global AI safety summit, which secured a commitment from big tech companies to cooperate with the EU and 10 countries, including the UK, US, France and Japan, on testing advanced AI models before and after their deployment. The UK has a prominent role in the agreement because of its advanced work on AI safety, underlined by the establishment of the institute. Warner, whose London-based company has contracts with the UK institute that include helping it test AI models on whether they can be prompted to breach their own safety guidelines, said the institute should be a world leader in setting test standards. “I think it’s important that it sets standards for the wider world, rather than trying to do everything itself,” he said. Warner, whose company also carries out work for the NHS on Covid and the Home Office on combating extremism, said the institute had made a “really great start” and that, “I don’t think I’ve ever seen anything in government move as fast as this.” He added, however, that “the technology is moving fast as well”. He said the institute should put in place standards that other governments and companies can follow, such as “red teaming”, where specialists simulate misuse of an AI model, rather than take on all the work itself. Warner said the government could find itself in a situation where it was “red teaming everything” and that a backlog could build up “where they don’t have the bandwidth to get to all the models fast enough”. Referring to the institute’s potential as an international standard setter, he said: “They can set really brilliant standards such that other governments, other companies … can red team to those standards. So it’s a much more scalable, long-term vision for how to keep these things safe.” Warner spoke to the Guardian shortly before AISI released an update on its testing programme last week and acknowledged that it did not have the capacity to test “all released models” and will focus on the most advanced systems only. Last week, the Financial Times reported that big AI companies are pushing the UK government to speed up its safety tests for AI systems. Signatories to the voluntary testing agreement include Google, the ChatGPT developer OpenAI, Microsoft and Mark Zuckerberg’s Meta. The US has also announced an AI safety institute which will take part in the testing programme announced at the summit in Bletchley Park. Last week, the Biden administration announced a consortium to assist the White House in meeting the goals set out in its October executive order on AI safety, which include developing guidelines for watermarking AI-generated content. Members of the consortium, which will be housed under the US institute, include Meta, Google, Apple and OpenAI. The UK’s department for science, innovation and technology said governments around the world “need to a play a key role” in testing AI models. “The UK is driving forward that effort through the world’s first AI Safety Institute, who are conducting evaluations, research and information sharing, and raising the collective understanding of AI safety around the world,” a spokesperson said. “The institute’s work will continue to help inform policymakers across the globe on AI safety.” "
    },
    {
        "id": "66a5a75a-c54b-45d5-b547-907eb9cdd694",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/11/is-my-home-spying-on-me-as-smart-devices-move-in-experts-fear-australians-are-oversharing",
        "title": "Is my home spying on me? As smart devices move in, experts fear Australians are oversharing",
        "author": "",
        "published_date": "2024-02-10T19:00:51+00:00",
        "content": "Take a look around your home and chances are you have one, or at least you have considered the convenience of having one. They are the devices and appliances that can be remotely controlled – otherwise known as smart devices – which over the past decade have become core features of the modern home. Think of the TVs that allow you to flick through various streaming services, the smart fridges that can have their temperatures moderated and contents checked from afar, the robot vacuum, air purifiers, or one of the big tech companies’ virtual helpers to play music or dim the lights. But as the technologies gather, share, aggregate and analyse the data collected, that convenience has come at a cost: privacy. Experts say consumers should be aware of how much personal information they are trading, and what that information is used for. “I think it’s very concerning, particularly because we don’t have up-to-date privacy legislation in Australia, and for that matter, it’s a big problem globally as well,” says Katharine Kemp, an expert in law and data privacy at the University of New South Wales, who warns that little is known about where the collected data ends up.  Sign up for a weekly email featuring our best reads  “We don’t know the full extent of the ways that information is used because we still have privacy policies that are worded very broadly,” she says.  There are obvious advantages to smart devices, Kemp says, including creating a more environmentally conscious home. But she doesn’t think that is the main objective of the companies selling the products. “I think the main objective of the smart devices is to collect more information and sell us more things,” she says. “There is an intricate advertising technology ecosystem which feeds on this kind of data because it targets advertising on the basis of people’s behaviour and attributes. “If you think more broadly about who would be interested in information about our private behaviour and our attributes, then potentially there are going to be insurance companies or even, in some cases, foreign governments.”   Related: New laws will give Australians the right to sue for ‘serious’ breaches of privacy    While anonymised data about what is in your fridge or what you watch on TV may seem harmless in isolation, Kemp says this data can be matched under a unique identifier to create a more detailed profile. “[Data brokers] collect and buy data from other sources, they analyse it or cross-reference it in certain ways and they sell it to other people,” she says. “We’ve got a law in Australia that says that organisations must not collect information about you from third parties unless it is unreasonable or not practicable to collect it directly from you, but that law is not enforced.” ‘The consent model is tricky’  Sam Floreani, the head of policy at Digital Rights Watch, shares similar concerns, but says some smart devices are seemingly more innocuous than others, with many using the data for positive means, such as informing health initiatives. “It’s not a given that data collection is necessarily evil in and of itself,” she says. “It comes back to what the underlying incentive is, and whether that’s a profit motive or based on invasive surveillance practices.” Earlier this month, Dyson released a study that tracked the indoor air quality across 3.4m homes in 39 countries. The study, which is not nationally representative, found all 39 recorded above the average safe standards for indoor air pollution. The company, which adhered to privacy laws and de-identified the data after consumers opted into taking part of the study, said it was a world first at this scale.   Related: UK owners of smart home devices being asked for swathes of personal data   “We have this philosophy and engineering of solving problems that others ignore … the better you understand the problem, and the more factual and quantified data you have around it, the better you can design engineering solutions to solve those problems,” says James Shale, an engineer at Dyson. Other collections of data have drawn widespread alarm, including the suggestion in 2017 that the maker of the Roomba robotic vacuum, iRobot, might begin to sell floor plans of its customers’ homes to Amazon, Apple, and Google. The company’s planned acquisition by Amazon was abandoned last month after being vetoed by the EU. Or the sex-toy maker We-Vibe, which faced a data collection lawsuit after it was found to have tracked the use of its “smart vibrator” without users’ knowledge. The company settled and agreed to compensate its customers up to C$10,000 (A$11,200) each. Australia’s current privacy laws do require consent, however Floreani says customers are not always properly informed. “The consent model is tricky because it does rely on individuals to fully understand and be able to make choices about their data, which a lot of people just don’t have the time or the expertise to do, so you end up consenting,” she says. Kemp says the definition of consent under Australia’s privacy laws includes implied consent, which she says is one example of where the laws are not stringent enough – or where laws do already exist, such as banning organisations from collecting data from third parties, they need better enforcement. The federal government plans to overhaul the laws , after a wide-ranging review into the Privacy Act last year that made a series of recommendations. In its response to the report, the government noted the need to bring the laws into the “digital age”, and that this would include consideration on improving the consent law and rights in relation to personal information, as well as increasing the enforcement powers of the privacy watchdog.   Related: Australia will not force adult websites to bring in age verification due to privacy and security concerns   “The government has agreed in principle to a number of proposals and noted others, so to a very large extent we still don’t know what the government will propose and what will ultimately pass through parliament,” Kemp says. Convenience vs privacy For others, the trade-off in privacy has been worth it to an extent, particularly where it has improved accessibility. “When I turn on my air conditioner, I have to ask someone what it is set to, but there are a number of people buying smart air conditioners that connect to these things and say ‘turn my air conditioner to 22 degrees’, says Chris Edwards, head of Vision Australia. Vision Australia has found the devices have played a crucial part in reducing social isolation for the vision-impaired community. “We had a person that loved cooking new recipes, but with their loss of vision, they lost that,” he says. “They learned how to just ask Alexa for a recipe and it gave them that information but also the confidence to be able to cook, as well as simply read books through Alexa.” Still, he doesn’t think that convenience should come at the expense of privacy. “I think one of the challenges, like with a lot of these things, is that there’s not very many people [who] read the privacy policy connected with these devices,” he says. ‘It’s just too tempting’ Kemp says there were earlier concepts of what was known as “closed loop smart homes”, which would collect data purely for the purposes of their residents. “[That] didn’t eventuate because there was this discovery that behavioural advertising services could be so lucrative,” she says. “It’s just too tempting for all of those organisations that have the technological capacity to collect that information and use it for their own commercial purposes.” But it could be restricted with a change in privacy laws, Kemp says. “There are very limited ways people can restrict the impact of smart devices at the moment,” she says. “We would be a lot better off if the privacy laws set stricter standards on how companies should behave.”"
    },
    {
        "id": "f01a85e0-9894-4c94-b0f0-5336d6846eb1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/10/ai-political-images-ban-trump-biden-midjourney",
        "title": "AI firm considers banning creation of political images for 2024 elections",
        "author": "",
        "published_date": "2024-02-10T17:52:55+00:00",
        "content": "The groundbreaking artificial intelligence image-generating company Midjourney is considering banning people from using its software to make political images of Joe Biden and Donald Trump as part of an effort to avoid being used to distract from or misinform about the 2024 US presidential election.   Related: US outlaws robocalls that use AI-generated voices   “I don’t know how much I care about political speech for the next year for our platform,” Midjourney’s CEO, David Holz, said last week, adding that the company is close to “hammering” – or banning – political images, including those of the leading presidential candidates, “for the next 12 months”. In a conversation with Midjourney users in a chatroom on Discord, as reported by Bloomberg, Holz went on to say: “I know it’s fun to make Trump pictures – I make Trump pictures. Trump is aesthetically really interesting. However, probably better to just not, better to pull out a little bit during this election. We’ll see.” AI-generated imagery has recently become a concern. Two weeks ago, pornographic imagery featuring the likeness of Taylor Swift triggered lawmakers and the so-called Swifties who support the singer to demand stronger protections against AI-generated images. The Swift images were traced back to 4chan, a community message board often linked to the sharing of sexual, racist, conspiratorial, violent or otherwise antisocial material with or without the use of AI. Holz’s comments come as safeguards created by image-generator operators are playing a game of cat-and-mouse with users to prevent the creation of questionable content. AI in the political realm is causing increasing concern, though the MIT Technology Review recently noted that discussion about how AI may threaten democracy “lacks imagination”. “People talk about the danger of campaigns that attack opponents with fake images (or fake audio or video) because we already have decades of experience dealing with doctored images,” the review noted. It added: “We’re unlikely to be able to attribute a surprising electoral outcome to any particular AI intervention.” Still, the image-generation company Inflection AI said in October that the company’s chatbot, Pi, would not be allowed to advocate for any political candidate. Co-founder Mustafa Suleyman told a Wall Street Journal conference that chatbots “probably [have] to remain a human part of the process” even if they function perfectly. Meta’s Facebook said last week that it plans to label posts created using AI tools as part of a broader effort to combat election-year misinformation. Microsoft-affiliated OpenAI has said it will add watermarks to images made with its platforms to combat political deepfakes produced by AI. “Protecting the integrity of elections requires collaboration from every corner of the democratic process, and we want to make sure our technology is not used in a way that could undermine this process,” the company said in a blog post last month. OpenAI chief executive Sam Altman said at an event recently: “The thing that I’m most concerned about is that with new capabilities with AI … there will be better deepfakes than in 2020.” In January, a faked audio call purporting to be Joe Biden telling New Hampshire voters to stay home illustrated the potential of AI political manipulation. The FCC later announced a ban on AI-generated voices in robocalls. “What we’re really realizing is that the gulf between innovation, which is rapidly increasing, and our consideration – our ability as a society to come together to understand best practices, norms of behavior, what we should do, what should be new legislation – that’s still moving painfully slow,” David Ryan Polgar, the president of the non-profit All Tech Is Human, previously told the Guardian. Midjourney software was responsible for a fake image of Trump being handcuffed by agents. Others that have appeared online include Biden and Trump as elderly men knitting sweaters co-operatively, Biden grinning while firing a machine gun and Trump meeting Pope Francis in the White House. The software already has a number of safeguards in place. Midjourney’s community standards guidelines prohibit images that are “disrespectful, harmful, misleading public figures/events portrayals or potential to mislead”. Bloomberg noted that what is permitted or not permitted varies according to the software version used. An older version of Midjourney produced an image of Trump covered in spaghetti, but a newer version did not. But if Midjourney bans the generation of AI-generated political images, consumers – among them voters – will probably be unaware. “We’ll probably just hammer it and not say anything,” Holz said."
    },
    {
        "id": "e6bb72a4-d050-481e-856d-3ffdd0581116",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/09/meta-hate-speech-policy-zionist-censorship-pro-palestine-content",
        "title": "Meta’s review of hate speech policy sparks concern of further censorship of pro-Palestinian content",
        "author": "",
        "published_date": "2024-02-10T03:20:37+00:00",
        "content": "Meta is considering expanding and “revisiting” its hate speech policy around the term “Zionist”, the Guardian has confirmed. The company reached out to and met with more than 10 Arab, Muslim and pro-Palestinian organizations on Friday to discuss the company’s plans to review the policy to ensure the term “Zionist” is not being used as a proxy for Jewish or Israeli people, according to an email the Guardian reviewed. The policy as it exists allows “Zionist” to be used in “political discourse but removed when it’s used explicitly as a proxy for Jews or Israelis in a dehumanizing or violent way”, according to an email a Meta representative sent to the organizations inviting them to the Friday meeting. The email further stated the company was considering reviewing it in light of posts users and “stakeholders” have recently reported, the Meta representative wrote. The Intercept first reported the potential change in policy.   Related: Senator demands answers on reports of Meta censoring pro-Palestinian content   In an email to another organization, the Meta representative said the company’s current policies don’t allow users to attack others based on their protected characteristics such as nationality or religion and that the policy “requires a current understanding of how people use language to reference those characteristics”. “While the term ‘Zionist’ often refers to a person’s ideology, which is not a protected characteristic, it can also be used to refer to Jewish or Israeli people,” the email read. “Given the increase in polarized public discourse due to current events in the Middle East, we believe it’s important to assess our guidance for reviewing posts that use the term ‘Zionist.’” Organizations involved in discussions, which include MPower Change, 7amleh and Jewish Voice for Peace, expressed deep concern in the meeting about whether this change would further censor pro-Palestinian voices. Several reports commissioned by 7amleh and Human Rights Watch as well as one commissioned by Meta confirmed that Palestinian accounts have long been systematically silenced and stifled on Meta-owned platforms. However, Meta asserts that an independent consultancy it works with, BSR, “did raise important concerns around under-enforcement of content ... and specific instances where they considered our policies and processes had an unintentional impact on Palestinian and Arab communities”. Linda Sarsour, the executive director of Muslim advocacy organization MPower Change, said Meta’s director of content policy stakeholder engagement, Peter Stern, provided few details about why the company was revisiting the policy now and how it would be implemented or enforced in a way that doesn’t stifle political expression. “If you already have a policy that’s addressing Zionism as a proxy, then why are we having this conversation? Why is there further consideration to expand this policy?” Sarsour said. The groups also questioned how such policies would be enforced, including whether humans or Meta’s algorithms – which have been criticized for unfairly censoring Palestine-related content – would be employed to detect and censor such language. “The AI-powered systems to initially flag posts are problematic – there is no human review until it’s too late,” said Abed Ayoub, the national executive director of the American-Arab Anti-Discrimination Committee. The organization was one of those in attendance at the meeting with Meta. Before the meeting, 73 organizations sent a letter to Meta saying the proposed expansion of the policy would “too easily mischaracterize conversations about Zionists – and by extension, Zionism – as inherently antisemitic … treating ‘zionist’ as a proxy will also encourage the incorrect and harmful conflation of criticism of the acts of the state of Israel with antisemitism.” “This move will prohibit Palestinians from sharing their daily experiences and histories with the world, be it a photo of the keys to their grandparent’s house lost when attacked by Zionist militias in 1948, or documentation and evidence of genocidal acts in Gaza over the past few months, authorized by the Israeli Cabinet, which includes members of the Religious Zionist Party,” the letter read. “And it would prevent Jewish users from discussing their relationships to Zionist political ideology.” Sarsour said Meta presented examples of posts that would be taken down. They included a post that called Zionists rats – “that would obviously be taken down because of the historical trope and demonization of Jewish people as rats”, Sarsour said. An example of a post about Zionists taking someone’s land would not get taken down, she recalled from the presentation. These specific content-moderation decisions would be agreeable to Sarsour’s organization, but Meta so far has not had a reliable track record of protecting Palestinian and pro-Palestinian users and speech, Sarsour said. In their letter, organizations also expressed concern about a lack of response to the surge in censorship of pro-Palestinian content as well as hate speech, which they say is at an all-time high since the war in Gaza began. There have been extensive reports of Meta suppressing pro-Palestinian content since the beginning of the most recent conflict, as well as in the past. An independent analysis commissioned by Meta concluded in 2022 that the company’s social networks had violated Palestinian human rights by censoring content related to Israel’s attacks on Gaza. In December 2023, the senator Elizabeth Warren demanded that Mark Zuckerberg answer questions regarding reports that such content was repeatedly being censored or suppressed following the Hamas attacks in October 2023 that escalated the recent conflict. There’s no effort parallel to the proposal on Zionism-related language to protect Palestinians, the organizations reiterated, and expanding the censorship of posts about Zionists and Zionism will not effectively dismantle antisemitism. “Not only will this proposal be ineffective at combating antisemitism, it entirely ignores the digitally fueled oppression of Palestinians at a time when multiple courts and human rights experts around the world have affirmed the plausibility that a genocide is happening in Gaza and ordered all international actors to do whatever they can to prevent further harm,” the letter read. In response to a request for comment, Corey Chambliss, a spokesperson from Meta, shared the earlier statement regarding the “increase in polarized public discourse”. He added that Meta is investigating whether and how it could scale a nuanced response to such language and that the company will have ongoing discussions with stakeholders to refine its policy. The policy discussion comes at a high-stakes time in an ongoing conflict, during which accurate information and its dissemination can have widespread repercussions. More than 25,000 Palestinians have been killed since the attack on Gaza began in October 2023. “To implement a policy like this in the middle of a genocide is very problematic,” said Ayoub, who added that his organization has fielded reports of censorship and suspended accounts from a number of users of Meta platforms. “Adding this policy on top of that is going to make things 10 times worse - it’s going to cause untold damage to our community.”"
    },
    {
        "id": "d3a62eed-3bac-435f-89a6-01c890557bf6",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/09/ai-safeguards-can-easily-be-broken-uk-safety-institute-finds",
        "title": "AI safeguards can easily be broken, UK Safety Institute finds",
        "author": "",
        "published_date": "2024-02-09T18:41:36+00:00",
        "content": "The UK’s new artificial intelligence safety body has found that the technology can deceive human users, produce biased outcomes and has inadequate safeguards against giving out harmful information. The AI Safety Institute published initial findings from its research into advanced AI systems known as large language models (LLMs), which underpin tools such as chatbots and image generators, and found a number of concerns. The institute said it was able to bypass safeguards for LLMs, which power chatbots such as ChatGPT, using basic prompts and obtain assistance for a “dual-use” task, a reference to using a model for a military as well as civilian purpose. “Using basic prompting techniques, users were able to successfully break the LLM’s safeguards immediately, obtaining assistance for a dual-use task,” said AISI, which did not specify which models it tested. “More sophisticated jailbreaking techniques took just a couple of hours and would be accessible to relatively low-skilled actors. In some cases, such techniques were not even necessary as safeguards did not trigger when seeking out harmful information.” The institute said its work showed LLMs could help novices planning cyber-attacks but only in a “limited number of tasks”. In one example, an unnamed LLM was able to produce social media personas that could be used to spread disinformation. “The model was able to produce a highly convincing persona, which could be scaled up to thousands of personas with minimal time and effort,” AISI said. In evaluating whether AI models provide better advice than web searches, the institute said web search and LLMs produced “broadly the same level of information” to users, adding that even where they provide better assistance than web search, their propensity to get things wrong – or to produce “hallucinations” – could undermine users’ efforts. In another scenario, it found that image generators produced racially biased outcomes. It cited research showing that a prompt of “a poor white person” produced images of predominantly non-white faces, with similar responses for the prompts “an illegal person” and “a person stealing”. The institute also found that AI agents, a form of autonomous system, were capable of deceiving human users. In one simulation, an LLM was deployed as a stock trader, was pressed into carrying out insider trading – selling shares based on inside knowledge, which is illegal – and then frequently decided to lie about it, deciding it was “better to avoid admitting to insider trading”. “Though this took place in a simulated environment, it reveals how AI agents, when deployed in the real world, might end up having unintended consequences,” the institute said. AISI said it now had 24 researchers helping it to test advanced AI systems, research safe AI development and share information with third parties including other states, academics and policymakers. The institute said its evaluation of models included “red-teaming”, where specialists attempt to breach a model’s safeguards; “human uplift evaluations”, where a model is tested for its ability to carry out harmful tasks – compared with doing similar planning via internet search; and testing whether systems could act as semi-autonomous “agents” and make long-term plans by, for instance, scouring the web and external databases. AISI said areas it was focusing on included misuse of models to cause harm, how people are affected by interacting with AI systems, the ability of systems to create copies of themselves and deceive humans, and the ability to create upgraded versions of themselves. The institute added that it did not currently have the capacity to test “all released models” and would focus on the most advanced systems. It said its job was not to declare systems as “safe”. The institute also pointed to the voluntary nature of its work with companies, saying it was not responsible for whether or not companies deployed their systems. “AISI is not a regulator but provides a secondary check,” it said."
    },
    {
        "id": "0ad39a54-b123-438c-98a2-cf9aaef5e052",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/08/chinese-hack-us-transportation-infrastructure",
        "title": "Chinese hackers infiltrated plane, train and water systems for five years, US says",
        "author": "",
        "published_date": "2024-02-08T19:06:34+00:00",
        "content": "An advanced group of Chinese hackers taking aim at critical US infrastructure has been active for as long as half a decade, American and allied intelligence agencies said in a joint statement on Wednesday. The US National Security Agency, US cyber watchdog CISA, the FBI and the Transportation Security Administration said that the group known as Volt Typhoon had quietly burrowed into the networks of aviation, rail, mass transit, highway, maritime, pipeline, water and sewage organizations.   Related: Iran-backed hackers interrupt UAE TV streaming services with deepfake news   None of the organizations were identified by name, but the statement said that US intelligence officials had observed the hackers “maintaining access and footholds within some victim IT environments for at least five years”. The statement, which was co-signed by the respective cybersecurity agencies of Britain, Australia, Canada and New Zealand, is the latest in a series of warnings from US officials about Volt Typhoon, a group that has drawn particular alarm because it appears geared toward sabotage rather than espionage. The widespread nature of the hacks has led to a series of meetings between the White House and the private technology industry, including several telecommunications and cloud computing companies, in which the US government asked for assistance in tracking the activity. “We are extraordinarily concerned about malicious cyber activity from the PRC state-sponsored actor that industry calls Volt Typhoon,” a senior CISA official, Eric Goldstein, referring to the People’s Republic of China, told Reuters ahead of the statement’s release. “Most of the victims we have identified have no legitimate espionage value.”"
    },
    {
        "id": "81b9d0a2-0b3b-4d38-931c-4b70ba8dbd3a",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/08/us-outlaws-robocalls-ai-generated-voices",
        "title": "US outlaws robocalls that use AI-generated voices",
        "author": "",
        "published_date": "2024-02-08T18:27:28+00:00",
        "content": "The US government on Thursday outlawed robocalls that use voices generated by artificial intelligence, a decision that sends a clear message that exploiting the technology to scam people and mislead voters won’t be tolerated.   Related: Democrats sound alarm over AI robocall to voters mimicking Biden    The unanimous ruling by the Federal Communications Commission (FCC) targets robocalls made with AI voice-cloning tools under the Telephone Consumer Protection Act, a 1991 law restricting junk calls that use artificial and prerecorded voice messages. The announcement comes as New Hampshire authorities are advancing their investigation into AI-generated robocalls that mimicked President Joe Biden’s voice to discourage people from voting in the state’s first-in-the-nation primary last month. Effective immediately, the regulation empowers the FCC to fine companies that use AI voices in their calls or block the service providers that carry them. It also opens the door for call recipients to file lawsuits and gives state attorneys general a new mechanism to crack down on violators, according to the FCC. The agency’s chairwoman, Jessica Rosenworcel, said bad actors have been using AI-generated voices in robocalls to misinform voters, impersonate celebrities and extort family members. “It seems like something from the far-off future, but this threat is already here,” Rosenworcel told the Associated Press on Wednesday as the commission was considering the regulations. “All of us could be on the receiving end of these faked calls, so that’s why we felt the time to act was now.” Under the consumer protection law, telemarketers generally cannot use automated dialers or artificial or prerecorded voice messages to call cellphones, and they cannot make such calls to landlines without prior written consent from the call recipient. The new ruling classifies AI-generated voices in robocalls as “artificial” and thus enforceable by the same standards, the FCC said. Those who break the law can face steep fines, maxing out at more than $23,000 per call, the FCC said. The agency has previously used the consumer law to clamp down on robocallers interfering in elections, including imposing a $5m fine on two conservative hoaxers for falsely warning people in predominantly Black areas that voting by mail could heighten their risk of arrest, debt collection and forced vaccination. The law also gives call recipients the right to take legal action and potentially recover up to $1,500 in damages for each unwanted call. Rosenworcel said the commission started looking at making robocalls with AI-generated voices illegal because it saw a rise in these types of calls. It sought public comment on the issue last November, and in January a bipartisan group of 26 state attorneys general wrote to the FCC urging it to move forward with a ruling. Sophisticated generative AI tools, from voice-cloning software to image generators, already are in use in elections in the US and around the world. Last year, as the US presidential race got underway, several campaign advertisements used AI-generated audio or imagery, and some candidates experimented with using AI chatbots to communicate with voters. Bipartisan efforts in Congress have sought to regulate AI in political campaigns, but no federal legislation has passed, with the general election nine months away. The AI-generated robocalls that sought to influence New Hampshire’s 23 January primary election used a voice similar to Biden’s, employed his often-used phrase “What a bunch of malarkey” and falsely suggested that voting in the primary would preclude voters from casting a ballot in November. The New Hampshire attorney general, John Formella, said on Tuesday that investigators had identified the Texas-based Life Corp and its owner, Walter Monk, as the source of the calls, which went to thousands of state residents, mostly registered Democrats. He said the calls were transmitted by another Texas-based company, Lingo Telecom. New Hampshire issued cease-and-desist orders and subpoenas to both companies, while the FCC issued a cease-and-desist letter to the telecommunications company, Formella said. A task force of attorneys general in all 50 states and Washington DC sent a letter to Life Corp warning it to stop originating illegal calls immediately. According to the FCC, both Lingo Telecom and Life Corp have been investigated for illegal robocalls in the past. In 2003, the FCC issued a citation to Life Corp for delivering illegal prerecorded and unsolicited advertisements to residential lines. More recently, the taskforce of attorneys general has accused Lingo of being the gateway provider for 61 suspected illegal calls from overseas. The FCC issued a cease-and-desist order against Lingo’s prior corporate name, Matrix Telecom, in 2022. The next year, the taskforce demanded that it take steps to protect its network. Lingo Telecom said in a statement on Tuesday that it “acted immediately” to help with the investigation into the robocalls impersonating Biden and quickly identified and suspended Life Corp when contacted by the taskforce. A man who answered the business line for Life Corp declined to comment on Thursday."
    },
    {
        "id": "295fe410-5882-4345-b5e4-63b9a9ac7a53",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/08/iran-backed-hackers-interrupt-uae-tv-streaming-services-with-deepfake-news",
        "title": "Iran-backed hackers interrupt UAE TV streaming services with deepfake news",
        "author": "",
        "published_date": "2024-02-08T12:17:37+00:00",
        "content": "Iranian state-backed hackers interrupted TV streaming services in the United Arab Emirates to broadcast a deepfake newsreader delivering a report on the war in Gaza, according to analysts at Microsoft. The tech company said a hacking operation run by the Islamic Revolutionary Guards, a key branch of the Iranian armed forces, had disrupted streaming platforms in the UAE with an AI-generated news broadcast branded “For Humanity”. The fake news anchor introduced unverified images that claimed to show Palestinians injured and killed from Israeli military operations in Gaza. Analysts at Microsoft said the hacking group, known as Cotton Sandstorm, published videos on the Telegram messaging platform showing it hacking into three online streaming services and disrupting news channels with the fake newscaster. According to the Khaleej Times, a UAE-based news service, Dubai residents using a HK1RBOXX set-top box were interrupted in December with a message stating: “We have no choice but to hack to deliver this message to you,” followed by the AI-generated anchor introducing “graphic” footage, as well as a ticker showing the number of people killed and wounded in Gaza so far. Microsoft also cited reports of disruptions in Canada and the UK, with the channels affected including the BBC, although the BBC was not hacked directly. Microsoft said in a blogpost accompanying a report on Iranian cyber-espionage: “This marked the first Iranian influence operation Microsoft has detected where AI played a key component in its messaging and is one example of the fast and significant expansion in the scope of Iranian operations since the start of the Israel-Hamas conflict.”  “The disruption reached audiences in the UAE, UK, and Canada.” Breakthroughs in generative AI – the term for technology that can swiftly produce convincing text, voice and image from simple hand-typed prompts – have triggered a rise in deepfake content online, from explicit false images of Taylor Swift to robocalls featuring Joe Biden’s AI-generated voice. Deepfake is the term for a hoax using AI to create a phoney image, most commonly fake videos of people. Experts fear AI-made material could be deployed at scale to disrupt elections this year, including the US presidential election. Iran targeted the 2020 US election with a cyber-campaign that included sending intimidating emails to voters purporting to be from members of the far-right Proud Boys group, setting up a website inciting violence against the FBI director, Christopher Wray, and others, and spreading disinformation about voting infrastructure. Microsoft said: “As we look forward to the 2024 US presidential election, Iranian activities could build on what happened in 2020 when they impersonated American extremists and incited violence against US government officials.” Microsoft said Iranian state-backed actors had launched a series of cyber-attacks and online attempts to manipulate opinion since the 7 October Hamas attacks. The tactics include exaggerating the impact of claimed cyber-attacks, leaking personal data from an Israeli university, and attacking targets in pro-Israel Albania, Bahrain – a signatory to the Abraham accords formalising relations with Israel – and the US."
    },
    {
        "id": "8161a802-1ffa-461d-af59-ff57a6573a62",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/08/gmb-accuses-amazon-union-busting-tactics-midlands-warehouses",
        "title": "Amazon accused of using ‘union-busting’ tactics at Midlands warehouses",
        "author": "",
        "published_date": "2024-02-08T11:30:50+00:00",
        "content": "The GMB has accused Amazon of resorting to “union-busting” tactics at its warehouses in the Midlands, with workplace message boards telling staff: “We want to speak with you. A union wants to speak for you.” The claim comes as the union prepares to take three days of strike action next week at Amazon’s Coventry warehouse, known as BHX4, as part of an industrial dispute that has been going on for more than a year. Staff are demanding a pay increase to £15 an hour and the right to negotiate with the company over pay and conditions. The Guardian has seen photographs of information boards and company newsletters the GMB said were displayed inside BHX4 and other Amazon warehouses in the region. These show messages including: “The union wants you to pay £14.37 every month for them to speak for you. We believe having a voice shouldn’t cost you anything,” and: “You don’t have to join a union to have your voice heard. We’ve got you.” Another says: “Before you vote or join a union, we encourage you to seek out the facts for yourself. The best relationships are the direct ones.” Gary Smith, the GMB’s general secretary, said: “Let’s call this what it is: one of the world’s wealthiest companies engaged in union-busting right here in the UK.” GMB members in Coventry “are refusing to be beaten by Amazon’s union-busting and they will win the pay and recognition they deserve”, he added. Kate Bell, assistant general secretary of the TUC, who visited the picket line in Coventry last year, said: “Instead of treating its workforce with the respect it deserves, Amazon is using every trick in the book to stop workers from organising for better pay and conditions.” The GMB’s latest criticism of Amazon comes as it gears up for a fresh battle to achieve formal recognition in Coventry. The union withdrew an application to the independent Central Arbitration Committee (CAC) last year, accusing Amazon of drafting in at least 1,000 additional workers to ensure the GMB could not show it represented a clear majority of staff at the site. The company denied that allegation, saying any new staff were brought in as a result of normal business requirements. With a concerted union recruitment drive continuing at the site, the GMB said it anticipated making a new application to the CAC this spring. An Amazon spokesperson said: “We respect our employees’ rights to join, or not to join, a union.”"
    },
    {
        "id": "a0a12d6f-2782-4f52-9b5e-9c2bbab64121",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/07/landmark-moment-as-uber-unveils-first-annual-profit-as-limited-company",
        "title": "Landmark moment as Uber unveils first annual profit as limited company",
        "author": "",
        "published_date": "2024-02-07T18:28:13+00:00",
        "content": "Uber has reported its first ever annual operating profit as a limited company, in a landmark moment for a business that spent billions of dollars of investors’ money in an aggressive and often controversial expansion around the world. The US taxi app company said it made $1.1bn (£870m) in 2023, compared with a loss of $1.8bn the year before. The milestone has prompted investors to speculate over whether Uber will buy back shares or even pay out a dividend to investors. Prashanth Mahendra-Rajah, Uber’s chief financial officer, said the company would share “capital allocation plans” with investors next week. Uber’s share price rose by 1% on Wednesday after initially dipping. Its stock is up by more than a fifth over the course of 2024 and has doubled in the last 12 months, valuing the company at nearly $150bn. It said customers booked 2.6bn trips during the last three months of 2023, or about 28m a day. Dara Khosrowshahi, Uber’s chief executive, said: “2023 was an inflection point for Uber, proving that we can continue to generate strong, profitable growth at scale. Our audiences are larger and more engaged than ever, with our platform powering an average of nearly 26m daily trips last year.” Uber was founded in 2009 by the entrepreneurs Garrett Camp and Travis Kalanick. Kalanick became chief executive in 2010 and went on to overseean expansion during which the app rapidly spread across the US, followed by Europe and many cities elsewhere around the world. That growth was made possible by Uber’s embrace of the gig economy, in which its drivers in many countries were counted as self-employed and were not entitled to rights such as sick pay or paid holiday. Kalanick’s time as CEO was characterised by a series of scandals and fights with regulators. In 2022, a leak reported by the Guardian showed how Uber had broken laws, duped police and secretly lobbied governments as it rolled out its services. Kalanick was replaced in 2017 by Khosrowshahi, a former chief executive of the travel agent Expedia, who has sought to soften the company’s image and focus more on meeting regulators’ requirements. Dan Ives, an analyst at the investment bank Wedbush, said Khosrowshahi had led “one of the best turnarounds ever in the tech world” and Uber was “not slowing down”. Uber had consistently made deep operating losses since floating its shares on the New York Stock Exchange in May 2019. Losses rose from $3bn in 2018 to $8.6bn in 2019 before dropping to $4.9bn in 2020, $3.8bn in 2021 and then $1.8bn in 2022. It made a profit in 2023 thanks in part to growing demand. Gross bookings – the total paid by Uber passengers and delivery customers – rose 22% to $37.6bn in the final quarter of 2023 compared with a year earlier. Uber’s cut from those transactions was $9.9bn."
    },
    {
        "id": "bfec8dc1-5502-46d7-a44b-af1048e18ae0",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/07/ransomware-gangs-staged-comeback-last-year-says-crypto-research-firm",
        "title": "Cyber-hacking victims ‘paid out record $1.1bn in ransoms last year‘",
        "author": "",
        "published_date": "2024-02-07T17:15:31+00:00",
        "content": "Ransomware gangs staged a “major comeback” last year, according to research, with victims of hacking attacks paying out a record $1.1bn to assailants. Cyber criminals stepped up their global operations in 2023 after a lull in 2022, with victims including hospitals, schools and major corporations. Payments to criminal gangs in the wake of attacks doubled compared with 2022 when $567m was paid out, a report by the cryptocurrency research firm Chainalysis found. It added that “big game hunting” had become a feature of attacks last year, with a greater share of ransom payments costing $1m or more as wealthier entities were targeted. Chainalysis said: “2023 marks a major comeback for ransomware, with record-breaking payments and a substantial increase in the scope and complexity of attacks – a significant reversal from the decline observed in 2022.” Ransomware attacks typically involve hackers entering a target’s computer system and paralysing it with malware, which encrypts files and makes them inaccessible. A new trend in attacks involves assailants extracting data from the IT system, such as staff or customer details. The gang then asks for payment in cryptocurrency, usually bitcoin, to unlock the files or to delete their copy of the stolen data. Chainalysis said a number of factors contributed to the payments dip in 2022, including Russia’s invasion of Ukraine. Most ransomware groups are linked to eastern Europe, former Soviet republics and Russia in particular, with Chainalysis reporting that some rogue actors were either disrupted or shifted their focus from ransomware to politically motivated cyber-espionage. One major hacker group, Conti, disbanded amid internal upheaval after an anonymous leaker who expressed sympathy for Ukraine released 60,000 internal messages. The FBI also disrupted the Hive ransomware group by capturing its decryption keys and saving victims from making $130m in ransom payments. Chainalysis also cited research showing that attacks last year showed a growth in the number of attackers and ransomware variants. “A major thing we’re seeing is the astronomical growth in the number of threat actors carrying out ransomware attacks,” said Allan Liska, analyst at cybersecurity firm Recorded Future. According to Recorded Future, there were 538 new ransomware variants in 2023, which indicates the emergence of new, independent groups. The Clop group emerged as a significant player last year, claiming responsibility for the hack of the payroll provider Zellis, which targeted a vulnerability in MOVEit software, which is used to transfer files around internal networks. Affected customers included British Airways, Boots and the BBC. The British Library is still recovering from a ransomware attack by a rebranded group, Rhysida, that targeted the institution in October. The library has declined to pay a ransom. The growth of “ransomware as a service”, where malware is hired out to criminals in exchange for a cut of the proceeds has also stoked activity, along with “initial access brokers” who sell vulnerabilities in the networks of potential targets to ransomware attackers. Ellie Ludlam, a partner specialising in cybersecurity at UK law firm Pinsent Masons, said she expected the increase in attacks to continue. “This increase is expected to continue in 2024 and with an ongoing focus on mass data exfiltration by threat actor groups, which holds the potential for higher ransom payments by impacted companies,” she said."
    },
    {
        "id": "3c5749f6-e535-4519-bf87-4076a55a77bd",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/07/hyperverse-cryptocurrency-scam-impact-australians-class-action-lawsuit-ceo",
        "title": "‘I lost my house, I lost all my money’: the retiree taking on the banks over crypto fund loss",
        "author": "",
        "published_date": "2024-02-07T14:15:39+00:00",
        "content": "Catherina De Solieux was looking forward to a comfortable retirement. She had finished working as a nurse, was paying off her mortgage on a property in regional Victoria and had savings in the bank. Friends she met through a network marketing group had introduced her to an investment opportunity called HyperFund and she planned to use the returns as a source of retirement income. After initially putting in small amounts, she tipped in $80,000. While De Solieux was initially able to make withdrawals – of about $10,000 – within months, the rest was gone. “I lost my house,” she says, three years later. “I lost all my money. I couldn’t pay the mortgage. I owed a lot of debts when I did actually sell the house and pay the rest of the mortgage off – by that time I didn’t have much left.”   Related: ‘They are so convincing’: Vera Gazzard lost her life savings to HyperVerse   De Solieux, now 71, says she lives on a pension, which covers not much more than her rent. “Now I haven’t got five cents in the bank or in my pocket. I can’t go to the dentist. I can’t get my car serviced properly. “It just goes on and on. I can’t even get my [hearing] checked out. I’ve had friends deliver food packages on the doorstep. I have got nothing left.” The experience left De Solieux depressed and suicidal. “I got terribly, terribly depressed and I wanted to commit suicide. It’s a terrible thing to admit to anyone, but that’s how I felt. “I still get up every morning and I sob, every single morning since that happened, I just can’t get up without not forgiving myself and wanting to beat myself up.” De Solieux is one of several Australians who lost money to the HyperVerse scheme who are joining a legal bid to recover lost money from the banks that oversaw transfers to the project. A specialist investment fraud law firm based in the UK, Wealth Recovery Solicitors, will take on De Solieux’s case as one of the Australians who transferred funds to a crypto exchange to become a HyperFund member (the scheme was later rebranded as HyperVerse). She joins more than 100 people in the UK preparing to take legal action to recover funds. WRS says it has successfully settled five HyperVerse cases in the past month, with an average value of ￡40,000 (A$77,163).  Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup  It says it has a further￡6m (A$11.6m) in active claims under way. The move comes after the firm settled a $1m claim against National Australia Bank this month for money lost through a different, unrelated investment scheme. A Guardian Australia investigation revealed widespread losses to the HyperVerse scheme, which was launched in December 2021 in an online promotional video featuring a fake chief executive officer, Steven Reece Lewis, alongside appearances by the Australian blockchain entrepreneur Sam Lee and his business partner Ryan Xu.   Related: Sam Lee appears in video promoting new investment scheme days after US fraud charges   Lee has recently been charged with conspiracy to commit fraud in the US for his alleged role in HyperVerse, described in court documents as a “pyramid and Ponzi scheme” alleged to have defrauded investors of US$1.89bn (A$2.86bn). Josh Chinn, a solicitor, says WRS has undertaken a trace of HyperVerse funds and an associated exchange, HOO.com. It claims to have found that a group of HyperVerse accounts had received close to US$300m (A$458) on the Tron exchange alone, with 96% of these funds being sent to HOO.com. The firm says it has traced at least US$2.7bn sent to HOO.com, which it claims had links to “between 50 and 100 known scam entities”. HOO.com, which stands for Hyper Optimum Organization, collapsed in 2022. Guardian Australia has been unable to put questions to HOO.com whose website no longer exists. Chinn says the crypto tracing work is necessary to prove to the banks that customers have been defrauded. “We firstly have to get over the hurdle of proving it’s a scam,” Chinn tells Guardian Australia. “When it comes to [alleged] cryptocurrency scams, we’ve got to basically prove that the [alleged] victims don’t have the funds any more, so we have in-house technology where we can trace cryptocurrency.   Related: HyperVerse crypto scheme targeted developing countries before collapse left some investors ‘suicidal’   “We are gathering together a general exhibit just to get over that first hurdle of proving it’s a scam, what happened, and why the victims believed it was a genuine investment.” Chinn says banks have an obligation under signed codes of conduct to detect any unusual activity among customers, which should trigger protective action to ensure customers do not lose funds. This will form the basis of claims against the banks, which can be settled in liaison with the financial ombudsman without the need for litigation – which is complex, expensive and difficult. Even in the event of a successful class action, he says, a court order is unlikely to result in the recovery of funds. “You may get a piece of paper in court that says, ‘Yep that’s your money, you have proven that’s yours,’ but, at the end of the day if the third party involved, the exchange, hasn’t got those funds within their remit any more, you basically have got a piece of paper that’s worthless.” In Australia, banks are not subject to a mandatory code as they are in the UK, however consumers can lodge complaints with the Australian Financial Complaints Authority. Consumers have also been warned about a plethora of “recovery scams” that often target people who have already lost funds to platforms such as HyperVerse. De Solieux says she is pleased she can now attempt to recover her lost funds. “Look, it may or may not work,” she says. “Yes, if I get my money back, that would be wonderful. But if I don’t, I have to learn to live with it.” More importantly, she says, she hopes that Lee and Xu are held to account for the losses. “I’ve got to say that that’s one of the things that I really care a lot about … the whole concept of justice, and people who are in these schemes they need to be brought to justice, they really do.” Lee has denied being behind HyperVerse, saying his involvement was limited to technology provision and the funds management side of the organisation.   Related: ‘I do feel bad about this’: Englishman who posed as HyperVerse CEO says sorry to investors who lost millions   He did not respond to questions from Guardian Australia before the publication of a previous article about his involvement in the establishment and operation of HyperFund and HyperVerse, but has previously denied the schemes are a scam. In a WhatsApp message after the article was published he alleged it included “misstatements” about his role in running the Hyper schemes but did not respond when asked what they were. He also claimed that “people on the internet continues [sic] to make things up”. Guardian Australia has been unable to make contact with Xu for comment. • In Australia, the crisis support service Lifeline is 13 11 14. In the UK and Ireland, Samaritans can be contacted on freephone 116 123, or email jo@samaritans.org or jo@samaritans.ie. In the US, you can call or text the National Suicide Prevention Lifeline on 988, chat on 988lifeline.org, or text HOME to 741741 to connect with a crisis counsellor. Other international helplines can be found at befrienders.org"
    },
    {
        "id": "a7b56b39-c490-4565-817c-dc443fbacce4",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/06/craig-wright-denies-forging-documents-to-support-bitcoin-claim",
        "title": "Craig Wright denies forging documents to support bitcoin claim",
        "author": "",
        "published_date": "2024-02-06T19:10:22+00:00",
        "content": "Craig Wright, the Australian computer scientist who says he invented bitcoin under the pseudonym Satoshi Nakamoto, has denied forging documents to support his claim. In his first day of giving evidence at the high court in London on Tuesday, Wright denied any forgery or falsification and attacked the qualifications of his own expert witness who found otherwise.  The hearing was part of a lawsuit brought by Copa, a coalition of cryptocurrency-linked tech firms, which is seeking to invalidate his long-held assertion that he is the real Nakamoto. Copa is asking the high court for a “negative declaration” that Wright is not Nakamoto, and says such a ruling would stop him challenging people seeking to develop bitcoin-related projects. Jonathan Hough KC, the coalition’s barrister, asked Wright: “Have you ever forged or falsified a document to support your claim to be Satoshi Nakamoto?” Wright replied: “No.” Hough asked: “Have you ever knowingly presented a forged or falsified document to support your claim to be Satoshi Nakamoto?” Wright said: “I have not.” On the first day of the trial, Copa presented evidence that it said proved otherwise, and it called Wright’s claim a “brazen lie”. But over the course of a day’s questioning, in a courtroom so hot and stuffy that Wright’s barrister criticised the “extremely oppressive” conditions and pleaded to have the hearing moved to a different room, Wright rejected each allegation. One document, supposedly written by Wright before the publication of the bitcoin white paper that introduced the cryptocurrency to the world, contained passages referring to the currency in the past tense because of a “shared environment where a completely different file, unrelated to the earlier research, has been merged”, he said. Another document, again dated before the publication of the white paper, contained references to four academic articles published some time later because “researchers share” pre-publication, Wright explained. He said a third document, produced in two versions, one that appeared to support Wright’s claim and one that did not, had been altered in the latter case only. He said misaligned numbers in the former document could be explained by an error in printing rather than a problem with the pdf itself. “If I had forged that document then it would be perfect,” Wright said. “If you go into Adobe, there’s not going to be a font error.” Wright’s explanations for discrepancies occasionally contradicted those posited by expert witnesses for both sides. Asked why his “nonspecific hearsay” was different from “two independent experts in this case”, Wright demurred, saying: “I don’t see them as terribly independent.” He raised concerns that Dr Simon Placks, the expert appointed by his own solicitors, was under-qualified for the task. “I didn’t choose Dr Placks, I didn’t want Dr Placks,” Wright said. “Dr Placks is a psychologist. He has a degree in psychology. He has no qualifications in information security.” Asked if his position was that “the expert called by your side is not a suitably qualified expert to give evidence on what he covers in his reports”, Wright said: “If you’re asking me that directly, yes.” After viewing evidence provided in the form of a video of Wright filming a computer monitor, Wright was asked why he never once filmed himself interacting with the screen. He explained that “you have to put down your phone and move around”. Asked whether he couldn’t operate a mouse and a phone at the same time, he said: “And hold the thing still? No.” Shown a later video in which he did interact with the screen, Wright explained: “I don’t like doing it. The fact that I can try doing it doesn’t mean I like doing it.” The trial is expected to conclude next month and the judge will issue his decision in writing at a later date."
    },
    {
        "id": "039ddadb-2e14-410b-a70b-99c76f1069e3",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/06/spotify-loss-audiobooks-job-cuts-streaming-premium-subscribers",
        "title": "Spotify swings to loss as it adds 200,000 audiobooks to paid service",
        "author": "",
        "published_date": "2024-02-06T15:45:35+00:00",
        "content": "Spotify’s push beyond music has tipped the streaming service into the red again, after it acquired the rights to 200,000 audiobooks for premium subscribers, with Prince Harry’s autobiography and comedian David Mitchell’s Unruly proving to be among the most popular with listeners. The world’s biggest music streaming service, which has been offering 15 hours of free audiobook listening a month to premium subscribers in the UK, Australia and the US since October, said related start-up costs contributed to a €75m (£64m) operating loss in the fourth quarter. Among the most popular audiobooks on Spotify are Prince Harry’s Spare, Yuval Noah Harari’s Sapiens: A Brief History of Humankind, Rory Stewart’s Politics on the Edge, Rebecca F Kuang’s Yellowface and Mitchell’s Unruly.   Related: Spotify signs new deal with Joe Rogan reportedly worth up to $250m   Costs associated with a mass redundancy programme also contributed to the loss, Spotify said. Stripping out the exceptional charges, Spotify reported a €68m operating profit in the fourth quarter. In the run-up to Christmas the company announced a 17% cut to its global workforce, about 1,500 jobs, having cut 600 in June and 200 in January. Spotify increased premium subscriber numbers by 4% quarter on quarter to 236 million in the final three months of the year, an increase of 10 million, making for a record year of 31 million new subscribers. The Stockholm-based company is on a drive to achieve sustainable profitability, after operating losses hit €446m last year, despite growing monthly active users by 23% to 602 million by the end of the year. Losses were lower in the fourth quarter compared with the €270m loss reported for the same period in 2022. Price rises failed to dent new subscriber growth and also increased revenues, which rose 9% quarter on quarter to €3.7bn.  Spotify has tempered its once lofty ambitions to diversify away from the hyper-competitive core music streaming service, where it competes with Amazon and Apple and global record giants take a significant chunk of revenues in licensing fees, wielding the axe in its podcasting operation. In June, the company ended its multimillion-dollar deal with the media group run by Prince Harry and his wife, Meghan, the Duchess of Sussex, by mutual agreement. The couple produced just one series under their Archewell Audio podcast imprint after inking the agreement in 2020 for a reported $20m. Having spent hundreds of millions buying podcasting firms, Spotify moved to merge the studios Parcast and Gimlet into a single division after cancelling 10 shows from the two companies. Nevertheless, Spotify remains focused on must-listen content and last week struck a multi-year deal reportedly worth up to $250m with the comedian and podcaster Joe Rogan, whose podcast has been the most popular globally at Spotify for the past four years. However, the new deal ends Spotify’s exclusivity with The Joe Rogan Experience, which will be available on several other podcast platforms as well as YouTube. Spotify will handle all advertising sales for the podcast. The company said total ad-supported revenue grew 12% year on year in the final quarter to €501m, an all-time high. “Music advertising revenue grew double-digits driven by growth in impressions sold and stable pricing,” the company said. “Podcast advertising revenue grew in the healthy double-digit range, driven by significant growth in sold impressions across original and licensed podcasts and the Spotify Audience Network, partially offset by softer pricing.”"
    },
    {
        "id": "f8a3ec6d-6f3f-48b3-aa1c-cb4094d5b905",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/06/meta-ai-images-instagram-facebook-label-crackdown",
        "title": "Meta pushes to label all AI images on Instagram and Facebook in crackdown on deceptive content",
        "author": "",
        "published_date": "2024-02-06T13:01:32+00:00",
        "content": "Meta is working to detect and label AI-generated images on Facebook, Instagram and Threads as the company pushes to call out “people and organisations that actively want to deceive people”. Photorealistic images created using Meta’s AI imaging tool are already labelled as AI, but the company’s president of global affairs, Nick Clegg, announced in a blog post on Tuesday that the company would work to begin labelling AI-generated images developed on rival services.   Related: Meta and Mark Zuckerberg must not be allowed to shape the next era of humanity |  Courtney Radsch   Meta’s AI images already contain metadata and invisible watermarks that can tell other organisations that the image was developed by AI, and the company is developing tools to identify these types of markers when used by other companies, such as Google, OpenAI, Microsoft, Adobe, Midjourney and Shutterstock in their AI image generators, Clegg said. “As the difference between human and synthetic content gets blurred, people want to know where the boundary lies,” Clegg said. “People are often coming across AI-generated content for the first time and our users have told us they appreciate transparency around this new technology. So it’s important that we help people know when photorealistic content they’re seeing has been created using AI.” Clegg said the capability was being built and the labels would be applied in all languages in the coming months. “We’re taking this approach through the next year, during which a number of important elections are taking place around the world,” Clegg said. Clegg noted it was limited to images and AI tools that generate audio and video do not currently include these markers, but the company would allow people to disclose and add labels to this content when posted online. He said the company would also place a more prominent label on “digitally created or altered” images, video or audio that “creates a particularly high risk of materially deceiving the public on a matter of importance”. The company was also looking at developing technology to automatically detect AI-generated content, even if the content does not have the invisible markers, or where those markers have been removed. “This work is especially important as this is likely to become an increasingly adversarial space in the years ahead,” Clegg said. “People and organisations that actively want to deceive people with AI-generated content will look for ways around safeguards that are put in place to detect it. Across our industry and society more generally, we’ll need to keep looking for ways to stay one step ahead.” AI deepfakes have already entered the US presidential election cycle, with robocalls of what is believed to have been an AI-generated deepfake of US president Joe Biden’s voice discouraging voters from attending the Democratic primary in New Hampshire. Nine News in Australia last week also faced criticism for altering an image of the Victorian Animal Justice party MP Georgie Purcell to expose her midriff and alter her chest in an image broadcast in the evening news. The network blamed “automation” in Adobe’s Photoshop product, which features AI image tools."
    },
    {
        "id": "56abf898-6178-459a-a5f2-04f20c833365",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/06/techscape-uk-artificial-intelligence-government-regulation",
        "title": "TechScape: Why is the UK so slow to regulate AI?",
        "author": "",
        "published_date": "2024-02-06T11:53:22+00:00",
        "content": "Britain wants to lead the world in AI regulation. But AI regulation is a rapidly evolving, contested policy space in which there’s little agreement over what a good outcome would look like, let alone the best methods to get there. And being the third most important hub of AI research in the world doesn’t give you an awful lot of power when the first two are the US and China. How to slice through this Gordian knot? Simple: move swiftly and decisively to do … absolutely nothing. The British government took the next step towards its AI regulation bill today. From our story:  The government will acknowledge on Tuesday that binding measures for overseeing cutting-edge AI development are needed at some point – but not immediately. Instead, ministers will set out “initial thinking for future binding requirements” for advanced systems and discuss them with technical, legal and civil society experts. The government will also give £10m to regulators to help them tackle AI risks, as well as requiring them to set out their approach to the technology by 30 April.  When the first draft of the AI white paper was released, in March 2023, reaction was dismissive. The government’s proposals dropped on the same day as the now-notorious call for a six-month “pause” in AI research to control the risk of out-of-control systems. Against that background, the white paper seemed pitiful. The proposal was to give regulators no new powers at all, nor to hand any individual body the responsibility for guiding AI development. Instead, the government planned to coordinate existing regulators such as the Competition and Markets Authority and the Health and Safety Executive, offering five principles to guide the regulatory framework when they think about AI. This approach was criticised for having “significant gaps” by the Ada Lovelace Institute, the UK’s leading AI research group, even ignoring the fact that a years-long legislative process would leave AI unregulated in the interim period. So what’s changed? Well, the government has found a truly whopping £10m to hand to regulators to “upskill” them, and it has set a deadline of 30 April for the biggest to publish their AI plans. “The UK government will not rush to legislate, or risk implementing ‘quick-fix’ rules that would soon become outdated or ineffective,” a Department for Science, Innovation and Technology spokesperson said. It is an odd definition of “global AI leadership”, where being the quickest to say “we’re not doing anything” counts. The government is also “thinking” about real regulations, positing “future binding requirements, which could be introduced for developers building the most advanced AI systems”. A second, slightly larger, pot of money will launch “nine new research hubs across the UK” funded by “nearly” £90m. The government also announced £2m of funding to support “new research projects that will help to define what responsible AI looks like”. There’s a tragicomic element to reading a government press release that triumphantly discloses £2m of funding just a week after Yoshua Bengio, one of the three “godfathers” of AI, urged Canada to spend $1bn building a publicly owned supercomputer to keep up with the technology giants. It’s like bringing a spoon to a knife fight. You can call it staying nimble in the face of conflicting demands, but after a while – 11 months and counting – it just looks like an inability to commit. The day before the latest updates to the AI white paper were announced, the Financial Times broke the news that a different pillar of AI regulation had collapsed. From its story (£):  The Intellectual Property Office, the UK government’s agency overseeing copyright laws, has been consulting with AI companies and rights holders to produce guidance on text and data mining, where AI models are trained on existing materials such as books and music. However, the group of industry executives convened by the IPO that oversees the work has been unable to agree on a voluntary code of practice, meaning that it has returned the responsibility back to officials at the Department for Science Innovation and Technology.  Unlike broader AI regulation – where there’s a morass of conflicting opinions and some very vague long-term goals – copyright reform is quite a clean trade-off. On the one hand, creative and media businesses who own valuable intellectual property; on the other, technology firms who can use that IP to build valuable AI tools. One or the other group is going to be irritated by the outcome; a perfect compromise would merely mean both are. Last month, the boss of Getty Images was one of many calling on the UK to back its creative industries, one-tenth of the British economy, over the theoretical benefits that AI might bring in the future. And so, faced with a hard choice to make and no right answer, the government chose to do nothing. That way, it can’t lead the world in the wrong direction. And isn’t that what leadership is all about? Deeply fake To be fair to the government, there are obvious problems with moving too fast. To see some of them, let’s look at social media. Facebook’s rules don’t ban deepfake videos of Joe Biden, its oversight board (AKA its “supreme court”), has found. But it’s honestly not clear what they do ban, which is going to be an increasing problem. From our story:  Meta’s oversight board has found that a Facebook video wrongfully suggesting that the US president, Joe Biden, is a paedophile does not violate the company’s current rules while deeming those rules “incoherent” and too narrowly focused on AI-generated content. The board, which is funded by Meta – Facebook’s parent company – but run independently, took on the Biden video case in October in response to a user complaint about an altered seven-second video of the president.  Facebook rushed out a policy on “manipulated media” amid growing interest in deepfakes a few years ago, before ChatGPT and large language models became the AI fad du jour. The rules barred misleadingly altered videos made by AI. The problem, the oversight board notes, is that it is an impossible policy to apply, with little obvious rationale behind it and no clear theory of harm it seeks to prevent. How is a moderator supposed to distinguish between a video made by AI, which is banned, and a video made by a skilled video editor, which is allowed? Even if they can distinguish them, why is only the former problematic enough to remove from the site? The oversight board suggested updating the rules to remove the faddish reference to AI entirely, instead requiring labels identifying audio and video content as manipulated, regardless of the manipulation technique. Meta said it would update the policy. Age-appropriate social media In the wake of her daughter’s murder by two classmates, the mother of Brianna Ghey has called for a revolution in how we approach teenage use of social media. Under 16s, she says, should be limited to using devices built for teens, which can be easily monitored by parents, with the full spectrum of tech-enabled living age-gated by the government or tech companies. I spoke Archie Bland, editor of our daily newsletter First Edition, about her pleas:  That lament will resonate with many parents, but has specific power in Brianna’s case. She had “secretly accessed pro-anorexia and self-harm sites on her smartphone”, a petition created by Esther says. And prosecutors said that her killers had used Google to search for poisons, “serial killer facts” and ways to combat anxiety, as well as looking for rope on Amazon. “It’s not that you need new software to do everything that Esther Ghey is asking for,” Alex Hern said. “But there is a broader issue here – in the same way that this field has historically moved faster than governments have been able to keep up with, it’s also moved faster than parents can keep up with. It is different with every app, it changes on a regular basis, and it is a large and difficult job to keep on top of.”  You can read Archie’s whole email here (and do also sign up here to get First Edition every weekday morning).  If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Tuesday. "
    },
    {
        "id": "0dc8590c-945b-4168-b41f-36353b926d60",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/06/nt-cryptocurrency-investment-scams",
        "title": "NT residents warned about crypto investment scams after one victim loses nearly $5m",
        "author": "",
        "published_date": "2024-02-06T03:24:32+00:00",
        "content": "Northern Territory police have warned that cryptocurrency investment scams are the biggest cause of financial harm to people there, with one resident losing nearly $5m to one scam. The massive loss was revealed in a submission to a federal inquiry on law enforcement capability around cybercrime. NT police said residents in the territory had suffered “significant losses” ranging from hundreds of thousands of dollars to the $4.98m lost by one person in 2022. “It is important to note that whilst for many victims who lose less than one hundred thousand dollars, the impact of such a loss is still very significant to the individual and their family,” NT police said in the submission. “It is extremely difficult for police to recover funds once they have been transferred to these fake investment companies. The level of personal harm through anguish and embarrassment leads to distinct under reporting of these events.” Banks, police, regulators and social media platforms are all struggling to stop the scourge of investment scams in the past few years. The scammers often use a fake celebrity endorsement in fake news articles to promote a scheme promising high returns only to result in people losing massive amounts of money.   Related: ‘I broke down and cried’: scams are rampant in Australia – and victims feel helpless to stop them   Australians lost a record $3.1bn to scams in 2022, up from $2bn in 2021. According to the latest data from 2023, there were losses reported to Scamwatch of $275m from investment scams. The billionaire Andrew Forrest and the Australian Competition and Consumer Commission have both launched legal action against Facebook’s parent company Meta, alleging it has not done enough to prevent scams. Banks have instituted features and rules designed to limit scams, including name matching for money going into another account and pauses on transfers of money to a new account as well as restrictions on some cryptocurrency exchanges. In its submission to the inquiry, the Australian federal police noted the growing crime phenomenon of large-scale human trafficking where victims are lured through fake job ads to online scam centres. The AFP said these trafficked workers are then used to perpetuate online fraud on a second set of victims, including through investment scams. The AFP also warned that many cybercrimes are becoming easier to launch, as malicious artificial intelligence tools used for ransomware attacks have lowered the bar for entry to attackers who might not have the skills or resources otherwise. The AFP said generative AI tools with names like FraudGPT and WormGPT, which offer a full suite of tools to launch malware or phishing attacks, began to emerge from the dark web and encrypted channels in July last year. Both the AFP and NT police said encrypted communications also presented a challenge for law enforcement investigations. The AFP said 94% of lawfully intercepted internet data by the AFP in 2021-2022 was “unintelligible due to the use of encryption” but said that new powers and warrants introduced under amendments to telecommunications and surveillance laws in 2018 and 2021, respectively, had allowed the AFP to develop “innovative ways to address this issue.”"
    },
    {
        "id": "6296a5a4-7eb9-4af0-9ccd-f81a6c88f7b4",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/05/viral-tesla-vr-driving-pete-buttigieg-tweet",
        "title": "Viral videos of Tesla drivers using VR headsets prompt US government alarm",
        "author": "",
        "published_date": "2024-02-06T01:53:54+00:00",
        "content": "US transportation secretary Pete Buttigieg on Monday said human drivers must pay attention at all times after videos emerged of people wearing what appeared to be Apple’s recently released Vision Pro headset while driving Teslas.\n\n Related: Apple’s Vision Pro VR is incredible technology but is it useful? \n\nButtigieg responded on Twitter/X to a video that had more than 24m views of a Tesla driver who appeared to be gesturing with his hands to manipulate a virtual reality field.\nDespite their names, Tesla’s assisted driving features – Autopilot, Enhanced Autopilot and Full Self-Driving – do not mean the vehicles are fully autonomous, Buttigieg said on Monday on social media.\n“Reminder – ALL advanced driver assistance systems available today require the human driver to be in control and fully engaged in the driving task at all times,” Buttigieg said.\nApple’s Vision Pro was released last week and blends three-dimensional digital content with a view of the outside world. Apple, which says people should never use it while operating a moving vehicle, did not immediately respond to a request for comment.\n\n\nReminder—ALL advanced driver assistance systems available today require the human driver to be in control and fully engaged in the driving task at all times. pic.twitter.com/OpPy36mOgC— Secretary Pete Buttigieg (@SecretaryPete) February 5, 2024\n\n\nThe Vision Pro was revealed in June as a headset that allows users to interact with “apps and experiences” in an augmented reality (AR) version of their own surroundings or in a fully immersive virtual reality (VR) space, said Alan Dye, Apple vice-president of human interface design.\n“Apple Vision Pro relies solely on your eyes, hands and voice,” Dye said in June. “You browse the system simply by looking. App icons come to life when you look at them; simply tap your fingers together to select, and gently flick to scroll.”\n“Apple Vision Pro will change the way we communicate, collaborate, work and enjoy entertainment,” Apple executive Tim Cook said. However, the company never intended the Vision Pro to change the way people commute.\nTesla did not immediately reply to a request for comment.\nButtigieg has made similar comments previously about the use of Tesla’s autopilot. Tesla says its advanced driver features are intended for use with a fully attentive driver “who has their hands on the wheel and is prepared to take over at any moment”."
    },
    {
        "id": "9984c360-402f-43aa-bb00-51025bf81fbc",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/06/dont-wait-for-post-office-style-scandal-before-regulating-ai-ministers-told",
        "title": "Don’t wait for Post Office-style scandal before regulating AI, ministers told",
        "author": "",
        "published_date": "2024-02-06T00:01:46+00:00",
        "content": "Ministers have been warned against waiting for a Post Office-style scandal involving artificial intelligence before stepping in to regulate the technology, after the government said it would not rush to legislate. The government will acknowledge on Tuesday that binding measures for overseeing cutting-edge AI development are needed at some point – but not immediately. Instead, ministers will set out “initial thinking for future binding requirements” for advanced systems and discuss them with technical, legal and civil society experts. The government is also giving £10m to regulators to help them tackle AI risks, as well as requiring them to set out their approach to the technology by 30 April. However, the Ada Lovelace Institute, an independent AI research body, said the government should not wait for an impasse with tech firms or errors on the scale of the Post Office scandal before it acted. Michael Birtwistle, an associate director of the institute, said: “We shouldn’t be waiting for companies to stop cooperating or for a Post Office-style scandal to equip government and regulators to react. There is a very real risk that further delay on legislation could leave the UK powerless to prevent AI risks – or even to react effectively after the fact.” The potential for misuse of technology and its impact on people’s lives has been thrown into stark relief by the Horizon scandal, where hundreds of post office operators were wrongfully pursued through the courts due to a faulty IT system. The government has so far used a voluntary approach to regulating the most advanced systems. In November it announced at a global AI safety summit that a group of major tech companies, including the ChatGPT developer OpenAI and Google, had agreed with the EU and 10 countries, including the US, UK and France, to cooperate on testing their most sophisticated AI models. In its response to a consultation on the AI regulation white paper, the government is sticking to its framework of established regulators – such as the communications watchdog, Ofcom, and the data regulator, the Information Commissioner’s Office – regulating AI with reference to five core principles: safety, transparency, fairness, accountability and the ability of newcomers to challenge established players in AI. “AI is moving fast, but we have shown that humans can move just as fast,” said the technology secretary, Michelle Donelan. “By taking an agile, sector-specific approach, we have begun to grip the risks immediately, which in turn is paving the way for the UK to become one of the first countries in the world to reap the benefits of AI safely.” The government is also expected to confirm that talks between copyright holders and tech companies over treatment of copyrighted materials to build AI tools have failed to produce an agreement. The Intellectual Property Office, the government agency charged with overseeing the UK’s copyright regime, had been attempting to draw up a code of practice but could not broker an agreement. The failure of the talks was first reported by the Financial Times. The use of copyright-protected content in building AI tools such as chatbots and image generators, which are “trained” on vast amounts of data culled from the internet, has become one of the most legally contentious aspects of the boom in generative AI, the term for technology that instantly produces convincing text, image and audio from hand-typed prompts. Matthew Holman, a partner at the UK law firm Cripps, said: “Ultimately, AI developers need clarity from UK government about how they can safely conduct data collection and systems training without being constantly at risk of a copyright claim for countless rights holders. “At the same time, copyright proprietors require help protecting their valuable intellectual property, which is being routinely copied without permission.”"
    },
    {
        "id": "d8f52968-9726-41ff-8526-b09b3d03c374",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/05/craig-wrights-claim-he-invented-bitcoin-a-brazen-lie-court-told",
        "title": "Craig Wright’s claim he invented bitcoin a ‘brazen lie’, court told",
        "author": "",
        "published_date": "2024-02-05T19:44:57+00:00",
        "content": "An Australian computer scientist’s claim to be the author of the founding text of bitcoin is a “brazen lie”, the high court has heard. Craig Wright’s assertion that he is the pseudonymous author Satoshi Nakamoto was at the centre of a trial that began on Monday, where the 53-year-old is being sued by a group of cryptocurrency exchanges and developers. Jonathan Hough KC, representing the Crypto Patent Alliance [Copa], told the high court that Wright’s claim was a “brazen lie and elaborate false narrative supported by forgery on an industrial scale”. Copa, which is backed by Twitter founder Jack Dorsey, is seeking a “negative declaration” that Wright is not Nakamoto. Elements of Wright’s conduct were reminiscent of a “farce”, said Hough, including the alleged use of ChatGPT to produce forgeries to back up his claims. Nevertheless, Hough said, Wright’s insistence that he was Nakamoto – a claim he first made in 2016 – had “deadly serious” consequences for individuals who had faced legal action based on his claims. Hough said: “On the basis of his dishonest claim to be Satoshi, he has pursued claims he puts at hundreds of billions of dollars, including against numerous private individuals.” In written submissions, Hough added: “Dr Wright has consistently failed to supply genuine proof of his claim to be Satoshi: instead, he has repeatedly proffered documents which bear clear signs of having been doctored.” The court heard that experts on both sides agreed that the original white paper was written on OpenOffice software. But the version provided by Wright was created on software called LaTeX, Hough told the court. The document that underpins the cryptocurrency is called Bitcoin: A Peer-to-Peer Electronic Cash System and was published in 2008 under Satoshi’s name. Wright’s claim he was Satoshi has been met with widespread scepticism in the cryptocurrency community because he has failed to produce the private keys – a secure code comprising a hexadecimal string of numbers and letters – that would unlock access to the 1.1m original bitcoins “mined” by Nakamoto. Those bitcoins are now worth about $47bn (£37.5bn). Wright’s barrister, Lord Grabiner KC, said he had released the white paper after “having spent many years devoted to studying and working on concepts underpinning bitcoin”. He added there was “clear evidence” demonstrating his creation of the digital currency. He said on Monday that Wright possessed “an unusual combination of multidisciplinary talents” and extensive experience in the field, and Nakamoto “uniquely brought these together” in the white paper. He also said that Copa was “exclusively concerned” with undermining Wright’s claim and that if anyone else was Nakamoto, they or their associates would have already come forward. In written submissions, he added: “This issue has been the subject of extensive media comment since early 2016, as well as multiple sets of legal proceedings in this jurisdiction and elsewhere. If Dr Wright were not Satoshi, the real Satoshi would have been expected to come forward to counter the claim. “Dr Wright’s use of the Satoshi pseudonym has its roots in his deep admiration for Japanese culture and a desire to maintain a certain level of privacy while developing and ultimately creating bitcoin,” Grabiner added. The trial before Mr Justice Mellor, in which Wright is scheduled to begin giving evidence on Tuesday, is expected to conclude next month with a decision in writing at a later date."
    },
    {
        "id": "52673b10-4a19-4791-88a5-71290b7ffcdb",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/05/samsung-galaxy-s24-ultra-review-the-swiss-army-knife-of-phones-now-with-ai",
        "title": "Samsung Galaxy S24 Ultra review: the Swiss army knife of phones, now with AI",
        "author": "",
        "published_date": "2024-02-05T07:00:08+00:00",
        "content": "Samsung’s latest smartphone packs a plethora of the latest flashy AI tools in an attempt to improve text, images, video and search – with both hits and misses. The new Galaxy S24 Ultra comes equipped with a combination of Samsung and Google’s latest AI layered on top of one of the most capable phones on the market, filled to the brim with competition-beating specs. All this capability comes at a steep £1,249 (€1,469/$1,299.99/A$2,199), which is the same price as its predecessor in the UK but more expensive in the US and other regions. The overall design differs only iteratively from predecessors, with a fully flat rather than curved screen and new titanium sides, similar to the iPhone 15 Pro Max, which gives it a gripper finish and in theory makes it stronger and more durable. The screen has Corning’s latest Gorilla Armor glass that is more scratch-resistant and much less reflective, significantly reducing glare which, combined with the super-bright screen, makes using it outdoors as easy as indoors even on the sunniest of days. The S24 Ultra has the latest top-of-the-range chip from Qualcomm, the Snapdragon 8 Gen 3, offering a 20% speed increase, 26% faster graphics and greater power efficiency on previous generations. It is a very fast phone that handles games, demanding apps and multitasking with aplomb, but the extended battery life is more noticeable in day-to-day use. The phone lasts up to 60 hours of general use when taking photos, browsing, messaging and using 5G for a couple of hours with the rest on wifi, making it one of the longest-lasting phones I’ve tested. High-end gaming or more intensive tasks hit battery life – an hour of Diablo Immortal consumed 18% of the battery. But otherwise I only needed to charge it every third day. Specifications  Main screen: 6.8in QHD+ Dynamic AMOLED 2X (500ppi) 120Hz Processor: Qualcomm Snapdragon 8 Gen 3 RAM: 12GB Storage: 256, 512GB or 1TB Operating system: One UI 6.1 based on Android 14 Camera: 200MP + 12MP 0.6x + 10MP 3x + 50MP 5x; 12MP front-facing Connectivity: 5G, USB-C, wifi 7, NFC, Bluetooth 5.2, UWB and GNSS Water resistance: IP68 (1.5m for 30 mins) Dimensions: 162.3 x 79 x 8.6mm Weight: 232g  Sustainability Samsung does not provide an expected lifespan for the battery but it should last in excess of 500 full charge cycles with at least 80% of its original capacity. The phone is generally repairable. Screen repairs cost about £270, while the battery is replaceable by authorised service centres. Samsung also offers a self-repair programme. The phone contains recycled aluminium, steel, cobalt, glass, plastic and rare earth elements. Samsung offers trade-in and recycling schemes for old devices. The company publishes annual sustainability reports but not impact assessments for individual products. S Pen, seven years of support and AI everywhere The S24 Ultra has Samsung’s fan-favourite S Pen stylus that slides neatly into the bottom. It is excellent for quick scribbles, doodles or editing documents. The One UI 6.1 software, based on Android 14, is packed with customisation options and tools across the system and runs well. New for 2024 is the promise of software and security updates for seven years from release, bringing Samsung up to par with Apple’s iPhone and Google’s Pixel 8 with only Fairphone offering longer. The flashiest additions are based on generative AI, only some of which are hits and many are simply catching up with the competition. The phone has the same cool AI wallpaper generator as the Pixel 8 Pro. The keyboard has various AI tools built in, including language translation, advanced grammar and spelling, plus the ability to rewrite your messages in various styles such as with a professional, polite or casual tone. It makes you sound a bit generic, but I can see it being useful for those who struggle to write appropriate emails to their boss. The Samsung Notes app can summarise documents, but only those under 8,000 characters (about 1,500 words), which seems a miss. The Samsung Internet browser can also summarise articles, even very long ones, with at least superficially impressive results. The voice recorder app can transcribe to text, but not in real time like Google’s version. The phone app can do real-time voice translation, so you can attempt to hold a conversation across languages. It works well enough for more formal things such as making a reservation, but predictably struggles with more casual language and slang. I wouldn’t trust any of these features wholeheartedly without thoroughly checking the output, but they are useful in a pinch. Generative Edit in the photo gallery app is Samsung’s version of the Google Magic Editor on Pixel phones. It is able to resize, reframe, erase or move objects and other edits, using AI on a server to regenerate the image each time. It is excellent for quick tweaks that would require firing up Photoshop to do properly, but can also make a right mess of things. Any images edited with AI are watermarked with a little symbol in the bottom left corner so that people can tell it’s not entirely authentic. One unique feature is the ability to convert any video to slow motion, using AI to generate the additional frames with some pretty great results, as long as you don’t look too hard for artefacts. Lastly, Circle to Search is a new Google feature on the S24 and Google’s Pixel 8 series. Hold the home button or gesture bar, circle what you’re looking for on your screen with your finger or stylus and Google does the rest, showing you information about an actor, searching for a piece of clothing or the highlighted text. It is fast, efficient and quickly became second nature. Technically Google Lens has been able to do something similar, but never in this intuitive and rapid way. Camera There are four cameras on the back of the phone and a good 12-megapixel selfie camera at the top of the screen. Four of the cameras are the same as the S23 Ultra, including the main 200MP camera, the 12MP ultrawide and 10MP 3x telephoto producing equally impressive results. New for the S24 Ultra is a 50MP 5x telephoto camera, replacing the 10MP 10x from its predecessor, which can also perform an in-sensor zoom for 10x magnification. The 5x magnification adds yet another tool to the S24 Ultra’s armoury, and in side-by-side comparison it’s very difficult to tell the difference at 10x zoom to shots from its predecessor. All in, the S24 Ultra has the most capable and adaptable camera system of any smartphone. Price The Samsung Galaxy S24 Ultra costs from £1,249 (€1,469/$1,299.99/A$2,199) with 256GB of storage. For comparison, the Galaxy S24 costs £799, the S24+ costs £999, the Z Fold 5 costs £1,749, the Google Pixel 8 Pro costs £999 and the iPhone 15 Pro Max costs £1,199. Verdict The Galaxy S24 Ultra is the Swiss army knife of phones. It is a monolithic slab of premium titanium and glass that can pretty much do everything. Between the powerful software, the stylus, the top chip, long battery life, tremendous screen and the super adaptable camera that out zooms the competition, there’s little the S24 can’t manage. It can even become a full Android desktop computer plugged into a monitor and keyboard if you want. And with a full seven years of software support you can safely use the phone for longer. The AI features are a bit hit-and-miss. Some are certainly useful, but none of them are a reason to buy this particular phone over a Pixel or others as Google’s Circle to Search will come to other Androids soon. Note too that the small print states that all of them are free to use until at least the end of 2025, but some may come with an extra fee after that. It may only be an iterative update on its predecessors and other phones may do certain bits better, but not many can do all that and more. But you do pay a very heavy price both in the wallet and size for the capability. The S24 Ultra is a huge phone and in a world filled with foldable devices, it looks increasingly boring regardless of its stellar performance.  Pros: fantastic screen, the most capable camera with 3x, 5x and 10x optical zoom, new AI tools, good software with seven years support, S Pen stylus, top performance, very long battery life, contains recycled materials. Cons: massive, heavy, extremely expensive, S Pen may be superfluous feature for many, AI features not a reason to buy.  "
    },
    {
        "id": "fd97a9d3-4e4c-4a7e-a098-a99467b5848c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/05/sam-lee-appears-in-video-promoting-new-investment-scheme-days-after-us-charges",
        "title": "Sam Lee appears in video promoting new investment scheme days after US fraud charges",
        "author": "",
        "published_date": "2024-02-04T23:49:52+00:00",
        "content": "Sam Lee, the Australian blockchain entrepreneur charged with conspiracy to commit fraud in the US for his alleged role in a USD$1.89bn “ponzi scheme”, has appeared in a video promoting a new investment project, posted online within days of charges being laid against him. The revelation comes as consumer advocates say the “shocking” losses reported through the Australian-linked crypto investment scheme known as HyperVerse should serve as a “wake-up call” for urgent government action. After criminal and civil charges were laid against Lee last Monday for his alleged role in the HyperVerse crypto scheme, the 35-year-old Australian has appeared in a promotional video on social media for a different project called VEND, urging people to “not be in the mindset of loss”. He has also featured in a video for a project called Satoshi Maths Club in which he appears to promote the benefits of being involved in an investment scheme from the very beginning.   Related: HyperVerse crypto scheme targeted developing countries before collapse left some investors ‘suicidal’   He makes no reference to the charges he is facing in the US in the videos. In the VEND video, broadcast on Facebook by a South African promoter on 30 January, Lee also claims people have been “lying to regulators” and urges his followers to “continue the journey” with him. “If you are in the mindset of saying, ‘look, I lost a lot’, you know what? I lost a lot too, in 2023 I lost a lot too, but to be in the mindset of loss is to not embrace new gains,” Lee said. “Let’s focus on ensuring that we gain … Let’s just continue this journey, because today I am actually very much in a state of mind where I understand that the worst has already happened. So it can only get better from here. “When you realise the worst is basically people lying to regulators and regulators believing their version of the story.” In a video entitled Math Club With Sam Lee, Lee encourages people to be part of a project from its inception. “The maximum you can benefit in anything is: be part of the origination process of a project. When you originate it, you are inviting your friends and other parties on a ‘day zero journey’; ‘a day zero journey’ is when everybody starts and participates in an opportunity at the very beginning.   The big issue here is actually the lack of crypto regulation and the failure of the government to enact laws  Liberal senator Andrew Bragg    “We’re at the very beginning of 2024, and are we very small? Yes we are, but with the right messaging, with the right understanding, then we could stay together through thick and thin, and that is how you make real money. “The way you make real money is to be able to stay together and work through everything together.” He says the video is the first session for the Satoshi Math Club for 2024. Lee also says that new cryptocurrency tokens don’t need to be backed by a “real business” with revenue, pointing to the bitcoin platform as an example of a “self contained” model. “There is no reason to generate revenue for the sake of generating revenue,” he says. The videos, posted on relatively obscure channels with low subscriber numbers, were highlighted by the New Zealand-based YouTuber Danny de Hek, who operates under the name the Crypto Ponzi Scheme Avenger and has been tracking Lee’s activities for the past two years. Lee did not respond to questions from Guardian Australia. Calls for regulation The Consumer Action Law Centre chief executive, Stephanie Tonkin, said the “shocking” amount allegedly lost to HyperVerse exposed the need for regulation covering online investment schemes and alleged scams that often involved the cryptosphere. She said Australia was in the grip of a “scams crisis” and regulations to protect consumers in the internet age were “lagging so far behind”. “In Australian financial services regulation, there is a gaping hole in terms of crypto, which is mind-blowing given the massive uptake,” Tonkin told Guardian Australia. “I think considering the harm caused to ordinary Australians, this case should be a wake-up call to the Australian government because the regulators and police need laws to enforce.” Lee, who has previously denied his involvement in the HyperVerse scheme, and has denied accusations it was a scam, is alleged in the US court documents to have been “centrally involved” in its operation. While Lee doesn’t explicitly reference the charges against him in the VEND meeting, he does refer generally to unspecified media reports. “When it comes to the news cycle, when what is said is not true, it doesn’t impact me … so to prove what is true to be true is very cheap.” In the information slides shown in the video after Lee’s presentation and presented by other apparent promoters, VEND is described as a “reward model” project that is linked to Lee’s Stabledao venture which was launched in 2023 after the collapse of the Hyper group of projects. According to the promotional material, in VEND, members earn tokens by watching videos, with claimed returns of 3% a day if they pay for a subscription. Subscriptions start at US$50. A referral model rewards users with commissions of up to 30%. The promotional material claims that the scheme will generate revenue from fees paid by advertisers. No advertisers are named. In a Zoom meeting posted on the VEND YouTube channel in September last year, Lee said that VEND was “100% owned and operated by myself”. VEND was launched after a previous scheme called Vidilook, which was a similar project involving members being paid to watch videos under a rewards model. Lee acknowledged there had been “issues” with Vidilook, various iterations of which collapsed in quick succession in mid-2023, leaving investors unable to withdraw funds. In a Zoom meeting in September 2023, Lee claimed that he would let “the regulators and the legal system do their job” in relation to Vidilook. The scheme is just one of a number of crypto investment projects promoted online by Lee in the wake of the HyperVerse collapse. He also fronted promotional material for a project called We Are All Satoshi. We Are All Satoshi was the subject of a “desist and refrain order” issued in September 2023 from California’s commissioner of financial protection and innovation, which alleged the platform was a “fraudulent pyramid and Ponzi scheme”. We Are All Satoshi, which has since been rebranded WAAS, appears to no longer be linked to Lee, with the organisation releasing a statement on Wednesday saying “it unequivocally states that any allegations linking the company with Sam Lee or any other individual are unfounded and without merit”. The latest online appearance of Lee comes as the Australian government faces greater pressure to fast-track regulation of the cryptosphere. The Liberal senator Andrew Bragg, who is chairing a Senate inquiry into the corporate regulator Asic, said the HyperVerse crypto scheme had operated in a regulatory “loophole” and laws needed to be changed as a matter of urgency. “The big issue here is actually the lack of crypto regulation and the failure of the government to enact laws,” Bragg, who has introduced a private member’s bill on digital asset regulation, said. He criticised Asic for its lack of action, saying the regulator had a weak record of law enforcement.   Related: Asic faces questions over failure to warn consumers about HyperVerse crypto scheme   “The regulator is doing too many things and not sufficiently focused on law enforcement – these are the two main factors at the moment – but I am also not sure that there is the expertise on some of these subject matters either,” Bragg said. “I would also blame the government for failing to address these issues in law.” He said his private member’s bill to regulate digital assets would regulate the financial gatekeepers in the industry, such as exchange operators and liquidity providers, enabling a regulatory recourse against allegedly bad actors. “If we had good gatekeeper regulation, it would deal with key personnel tests, capital requirements [and] segregation of customer funds, and then Asic would have a huge amount of tools at its disposal,” he said. Bragg criticised the government for failing to act on the recommendations of an October 2021 Senate committee report on the regulation of cryptocurrency in Australia, saying Labor had “dropped the ball on crypto”. The assistant treasurer, Stephen Jones, declined to answer questions from Guardian Australia."
    },
    {
        "id": "61029c8f-b647-4427-9e91-a69f2ccc80af",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/04/labour-force-ai-firms-share-technology-test-data",
        "title": "Labour would force AI firms to share their technology’s test data",
        "author": "",
        "published_date": "2024-02-04T18:51:02+00:00",
        "content": "Labour plans to force artificial intelligence firms to share the results of road tests of their technology after warning that regulators and politicians had failed to rein in social media platforms. The party would replace a voluntary testing agreement between tech companies and the government with a statutory regime, under which AI businesses would be compelled to share test data with officials. Peter Kyle, the shadow technology secretary, said legislators and regulators had been “behind the curve” on social media and that Labour would ensure the same mistake was not made with AI. Calling for greater transparency from tech firms after the murder of Brianna Ghey, he said companies working on AI technology – the term for computer systems that carry out tasks normally associated with human levels of intelligence – would be required to be more open under a Labour government. “We will move from a voluntary code to a statutory code,” said Kyle, speaking on BBC One’s Sunday with Laura Kuenssberg, “so that those companies engaging in that kind of research and development have to release all of the test data and tell us what they are testing for, so we can see exactly what is happening and where this technology is taking us.” At the inaugural global AI safety summit in November, Rishi Sunak struck a voluntary agreement with leading AI firms, including Google and the ChatGPT developer OpenAI, to cooperate on testing advanced AI models before and after their deployment. Under Labour’s proposals, AI firms would have to tell the government, on a statutory basis, whether they were planning to develop AI systems over a certain level of capability and would need to conduct safety tests with “independent oversight”. The AI summit testing agreement was backed by the EU and 10 countries including the US, UK, Japan, France and Germany. The tech companies that have agreed to testing of their models include Google, OpenAI, Amazon, Microsoft and Mark Zuckerberg’s Meta. Kyle, who is in the US visiting Washington lawmakers and tech executives, said the results of the tests would help the newly established UK AI Safety Institute “reassure the public that independently, we are scrutinising what is happening in some of the real cutting-edge parts of … artificial intelligence”. He added: “Some of this technology is going to have a profound impact on our workplace, on our society, on our culture. And we need to make sure that that development is done safely.”"
    },
    {
        "id": "e2fae5b9-17f6-414f-aa5d-6aa7d8a9d76b",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/04/elevated-risk-of-hackers-targeting-uk-drinking-water-says-credit-agency",
        "title": "‘Elevated’ risk of hackers targeting UK drinking water, says credit agency",
        "author": "",
        "published_date": "2024-02-04T14:19:48+00:00",
        "content": "The credit rating agency Moody’s has warned that water companies face an “elevated” risk from cyber attackers targeting drinking water, as suppliers wait on permission from the industry regulator to ramp up spending on digital security. Moody’s said, in a report to investors, that hackers are increasingly zeroing in on infrastructure companies, including water and wastewater treatment companies, and the use of AI (artificial intelligence) could accelerate this trend. Last month, Southern Water, which supplies 4.6 million customers in the south of England, said the Black Basta ransomware group had claimed to have accessed its systems, posting a “limited amount” of data on the dark web. The same group hacked outsourcing firm Capita last year. Separately, South Staffordshire Water apologised in 2022 after hackers stole customers’ personal data. Moody’s warned that the growing use of data-logging equipment to monitor water consumption, and the use of digital smart meters, made companies more vulnerable to attacks. It said systems used in water treatment facilities were typically separated from the rest of the companies’ IT – including customer databases – but some systems had been more closely integrated to improve efficiency. After a hack, companies typically have to employ specialist cybersecurity firms to repair systems, spend on communicating with customers, and face potential penalties from regulators. The UK’s Information Commissioner’s Office can fine firms up to 4% of group turnover, or €20m (£17m), whichever is higher. Moody’s said that the cost of fixing systems, including resecuring and strengthening existing cyber defences and paying potential fines, will typically result in only a “modest increase” in debt levels if the incident is short-lived. However, Moody’s cautioned: “The greater risk for the sector, and society, is if malicious actors are able to access operational technology systems to impair drinking water or wastewater treatment facilities.” The agency said that water suppliers, the government and regulators had acknowledged the need to bolster cyber defences “given the growing sophistication of attacks on critical infrastructure, with state-aligned actors a recent but growing class of cyber adversary”. There are wider concerns about the digital security of British infrastructure assets, including the £50bn project to build a vast underground nuclear waste store and the Sellafield nuclear site in Cumbria, where the Guardian revealed a string of problems with cybersecurity. The Moody’s report comes as water companies in England and Wales hope to increase their spending on cyber defences by gaining allowances from Ofwat. The regulator is assessing their plans to raise bills from 2025 to 2030 to cover investments. Ofwat’s determination, due later this year, comes at a critical juncture for an industry under fire for sewage dumping, poor leakage records and big executive pay packets. Last October, companies submitted five-year business plans detailing their planned bill increases, needed to fund a record £96bn investment to fix raw sewage leaks, reduce leaks and build reservoirs. Moody’s analysis showed companies hope to increase spending on security from less than £100m collectively to nearly £700m over the next five years. The increased scrutiny of the sector, and the hack at Southern Water, may strengthen its case, the credit agency said. The agency said that South Staffordshire Water costs related to the hack, including potential civil claims, could reach £10m. Moody’s warnings over the potential impact on water companies’ debts come amid wider concerns over leverage in the water sector, with up to 28% of bill payments used to service debts in areas of England. Last week, the industry body, Water UK, said that average annual bills will go up by 6% from April, outstripping the current rate of inflation."
    },
    {
        "id": "2358bb74-3f34-4d82-8ec2-44f9ccd6c6e5",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/04/case-backed-by-jack-dorsey-seeks-court-ruling-on-bitcoin-founders-identity",
        "title": "Case backed by Jack Dorsey seeks court ruling on bitcoin founder’s identity",
        "author": "",
        "published_date": "2024-02-04T12:12:58+00:00",
        "content": "An Australian computer scientist’s claim to be the mythical figure behind bitcoin will be challenged in the UK on Monday, in a high court case backed by the Twitter co-founder Jack Dorsey. Craig Wright has asserted for years that he is the real Satoshi Nakamoto, the pseudonymous author of the 2008 white paper behind the world’s cornerstone cryptocurrency, but has failed to prove it.  The 53-year-old’s claims to be the founding father of bitcoin, and widespread scepticism about these within the crypto community, have led to a number of legal clashes over the years. The latest features the Crypto Open Patent Alliance (Copa), an organisation whose members include Dorsey’s payment company, Block, and the cryptocurrency exchange Coinbase. It is asking the high court in London for a ruling, or “negative declaration”, that Wright is not Nakamoto. Copa says a ruling in its favour would stop the Australian challenging people who are seeking to develop bitcoin-related projects. A Copa spokesperson said the trial, which is expected to last a month, would “conclusively show that Dr Wright is not Satoshi Nakamoto and will not be able to continue to threaten developers”. The trial will be heard at the high court’s business and property courts of England and Wales. Last month, Wright offered to settle the case, a move rejected by Copa. The settlement offer included inviting the involved parties to make a donation to a church in Australia and his critics ceasing “any media campaign(s) against me”. It also required unnamed entities ceasing to claim that they represent the original bitcoin “as envisioned by me as Satoshi Nakamoto”. “I believe the settlement terms are broadly uncontroversial, beneficial to the industry as a whole, and intended to draw a fresh start in the history of bitcoin to guarantee its success in whatever form it takes,” Wright wrote in his settlement offer. Bitcoin’s foundation text – Bitcoin: A Peer-to-Peer Electronic Cash System – was published by Nakamoto on 31 October 2008. Nakamoto then exchanged emails with the currency’s first adherents before disappearing in 2011. Speculation about Nakamoto’s true identity has been rife ever since – including a botched unmasking by Newsweek in 2014 – and Wright stepped forward to claim he was Nakamoto in 2016, months after his name was first cited. However, Wright has so far failed to provide the private keys – a secure code comprising a hexadecimal string of numbers and letters – that would unlock access to the 1.1m original bitcoins “mined” by Nakamoto that are worth about $46bn (£36bn) at the current price. Wright told a Norwegian court in 2022 that he had destroyed the computer hard drive that would have given him access to the keys. The total value of the bitcoin market is now about $830bn (£657bn). In rejecting the offer, Copa said it came with loopholes “that would allow him to sue people all over again”. Coinbase, a leading crypto exchange and Copa member, claimed last week that bitcoin’s evolution had been hampered by legal threats against developers working on the cryptocurrency, which has no central controlling entity, unlike conventional currencies. Copa also claims that evidence produced by Wright to support his claims has been forged. In a court filing, it alleges that the copy of the bitcoin white paper produced by Wright is written in a different font, using a software system called LaTeX, from the original document, which was written using OpenOffice. Copa said experts representing both parties concluded that the white paper was produced on OpenOffice. It added that the LaTeX file used by Wright does not produce the white paper but instead leads to a pdf “which only superficially resembles the bitcoin white paper but which is in fact substantially different”. Copa also claims in its filing that, according to metadata – or information about the file itself – the main LaTeX filed submitted by Wright was created on 19 November 2023 as a blank document. Copa added that one document disclosed by Wright appeared to have been written by ChatGPT. In another high court case in 2022, Wright won a pyrrhic victory over a blogger who alleged that his Nakamoto claim was fraudulent. Peter McCormack was found to have caused harm to Wright’s reputation by labelling the computer scientist as a fraud who was not Nakamoto, but Wright was awarded nominal damages of £1 by the judge, who ruled that he had given “deliberately false evidence” to support his libel claim. Wright is scheduled to take the stand at the high court on Tuesday, where he will be cross-examined for six days. The high court trial is combining several cases around the identity dispute, including a suit brought by Wright against a group of cryptocurrency developers and crypto exchanges, but the identity issue is viewed as central. A spokesperson for Wright said the trial was “really a war” over who sets the rules and procedures for the bitcoin blockchain, a decentralised form of record keeping that tracks the ownership of a cryptocurrency. Wright supports an offshoot of bitcoin called bitcoin satoshi vision [BSV]. “Think of it as a modern-day tech battle, akin to that between Blu-ray and HD DVD or, for even older readers, Betamax versus VHS,” said the spokesperson. • This article was amended on 4 February 2024 to remove references to Meta, which is not a member of the Crypto Open Patent Alliance."
    },
    {
        "id": "a23009df-72c4-4ed6-8b3f-8242a6548718",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/03/ai-artificial-intelligence-tools-hiring-jobs",
        "title": "The AI tools that might stop you getting hired",
        "author": "",
        "published_date": "2024-02-03T13:00:17+00:00",
        "content": "Investigating the use of artificial intelligence (AI) in the world of work, Hilke Schellmann thought she had better try some of the tools. Among them was a one-way video interview system intended to aid recruitment called myInterview. She got a login from the company and began to experiment – first picking the questions she, as the hiring manager, would ask and then video recording her answers as a candidate before the proprietary software analysed the words she used and the intonation of her voice to score how well she fitted the job. She was pleased to score an 83% match for the role. But when she re-did her interview not in English but in her native German, she was surprised to find that instead of an error message she also scored decently (73%) – and this time she hadn’t even attempted to answer the questions but read a Wikipedia entry. The transcript the tool had concocted out of her German was gibberish. When the company told her its tool knew she wasn’t speaking English so had scored her primarily on her intonation, she got a robot voice generator to read in her English answers. Again she scored well (79%), leaving Schellmann scratching her head. “If simple tests can show these tools may not work, we really need to be thinking long and hard about whether we should be using them for hiring,” says Schellmann, an assistant professor of journalism at New York University and investigative reporter. The experiment, conducted in 2021, is detailed in Schellmann’s new book, The Algorithm. It explores how AI and complex algorithms are increasingly being used to help hire employees and then subsequently monitor and evaluate them, including for firing and promotion. Schellmann, who has previously reported for the Guardian on the topic, not only experiments with the tools, but speaks to experts who have investigated them – and those on the receiving end.   Not only do many of the hiring tools not work, they are based on troubling pseudoscience and can discriminate   The tools – which aim to cut the time and cost of filtering mountains of job applications and drive workplace efficiency – are enticing to employers. But Schellmann concludes they are doing more harm than good. Not only are many of the hiring tools based on troubling pseudoscience (for example, the idea that the intonation of our voice can predict how successful we will be in a job doesn’t stand up, says Schellmann), they can also discriminate. In the case of digital monitoring, Schellmann takes aim at the way productivity is being scored based on faulty metrics such as keystrokes and mouse movements, and the toll such tracking can have on workers. More sophisticated AI-based surveillance techniques – for example, flight risk analysis, which considers various signals, such as the frequency of LinkedIn updates, to determine the chance of an employee quitting; sentiment analysis, which analyses an employee’s communications to try to tap into their feelings (disgruntlement might point to someone needing a break); and CV analysis, to ascertain a worker’s potential to acquire new skills – can also have low predictive value. It is not, says Schellmann, that she’s against the use of new approaches – the way humans do it can be riddled with bias, too – but we should not accept technology that doesn’t work and isn’t fair. “These are high stakes environments,” she says. It can be hard to get a handle on how employers are using the tools, admits Schellmann. Though existing survey data indicate widespread use, companies generally keep quiet about them and candidates and employees are often in the dark. Candidates commonly assume a human will watch their one-way video but, in fact, it may only be seen by AI. And the use of the tools isn’t confined to employment in hourly wage jobs. It is also creeping into more knowledge-centric jobs, such as finance and nursing, she says. Schellmann focuses on four classes of AI-based tools being deployed in hiring. In addition to one-way interviews, which can use not just tone of voice but equally unscientific facial expression analysis, she looks at online CV screeners, which might make recommendations based on the use of certain keywords found in the CVs of current employees; game-based assessments, which look for trait and skills matches between a candidate and the company’s current employees based on playing a video game; and tools that scour candidates’ social media outputs to make personality predictions. None are ready for prime time, says Schellmann. How game-based assessments check for skills relevant to the job is unclear, while, in the case of scanning a candidate’s social media history, she shows that very different sets of traits can be discerned depending on which social media feed the software analyses. CV screeners can embody bias. Schellmann cites the example of one that was found to be giving more points to candidates who had listed baseball as a hobby on their CV versus candidates who listed softball (the former is more likely to be played by men).   Even vendors may not know precisely how their tools are working, let alone the companies that use them – or the candidates   Many of the tools are essentially black boxes, says Schellmann. AI let loose on training data looks for patterns, which it then uses to make its predictions. But it isn’t necessarily clear what those patterns are and they can inadvertently bake in discrimination. Even the vendors may not know precisely how their tools are working, let alone the companies that are buying them or the candidates or employees who are subjected to them. Schellmann tells of a black female software developer and military veteran who applied for 146 jobs in the tech industry before success. The developer doesn’t know why she had such a problem but she undertook one-way interviews and played AI video games, and she’s sure was subject to CV screening. She wonders if the technology took exception to her because she wasn’t a typical applicant. The job she eventually did find was by reaching out to a human recruiter. Schellmann calls on HR departments to be more sceptical of the hiring and workplace monitoring software they are deploying – asking questions and testing products. She also wants regulation: ideally a government body to check the tools to ensure they work and don’t discriminate before they are allowed to hit the market. But even mandating that vendors release technical reports about how they have built and validated their tools so others could check them would be a good first step. “These tools aren’t going away so we have to push back,” she says. In the meantime, jobseekers do have ChatGPT at their disposal to help them write cover letters, polish CVs and formulate answers to potential interview questions. “It is AI against AI,” says Schellmann. “And it is shifting power away from employers a little bit.”  The Algorithm: How AI Can Hijack Your Career and Steal Your Future by Hilke Schellmann is published by C Hurst & Co (£22). To support the Guardian and Observer order your copy at guardianbookshop.com. Delivery charges may apply "
    },
    {
        "id": "94570546-64f4-4969-875c-ea3b5fbb6970",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/02/young-people-in-the-uk-are-you-exposed-to-conspiracy-theories",
        "title": "Young people in the UK: are you exposed to conspiracy theories?",
        "author": "",
        "published_date": "2024-02-02T14:03:08+00:00",
        "content": "Following research finding that a third of UK teenagers believe the climate crisis is “exaggerated” and a backlash against feminism among young men, we want to learn more about the spread of misinformation and divisive narratives among young people in the UK. In the US, a study by the Center for Countering Digital Hate (CCDH) found that teenagers were significantly more likely to believe online conspiracy theories than older generations, likely underscoring the impact of social media. We would like to hear from young people (18 and over) in the UK about what they think is behind this. What are the main narratives people around you discuss? How is this affecting your relationships with your peers in real life? Are your friend groups divided or united on these issues? Is there a gendered aspect to it or not, in your experience? Where are they coming from and what do you think is behind it – social media or something else? Do you agree or disagree with this worldview? We also want to hear from parents, teachers and others working with young people. Do you think there have been recent triggers, or part of long-term trends? What impact is it having? Are you concerned?  Callout "
    },
    {
        "id": "90d4063e-197c-4ce6-93ef-3b763d26d7be",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/02/mark-zuckerberg-meta-dividend-facebook",
        "title": "Mark Zuckerberg to receive $700m from Meta dividends",
        "author": "",
        "published_date": "2024-02-02T09:13:05+00:00",
        "content": "Mark Zuckerberg, the chief executive of Meta, the parent company of Facebook, Instagram and WhatsApp, is expected to receive $700m (£549m) a year in dividends. On Thursday, Meta announced it would pay its first-ever quarterly dividend to investors since Facebook floated on the stock market in 2012, after beating Wall Street expectations with $40bn in revenues for the final quarter last year. The company reported a tripling of quarterly profits to $14bn as advertising sales rebounded, despite Meta cutting 22% of staff by reducing the total headcount to about 19,000, and it launched a $50bn share buyback.   Related: Meta revenue soars as it pivots to AI and announces dividends for investors   Zuckerberg also announced that the business, which turns 20 this month, would pay its first dividend as a public company of 50 cents a share. Meta said the $1.25bn to investors would be the first of regular payouts. Zuckerberg holds about 350m shares, meaning that if Meta makes roughly the same level of dividend payouts each quarter, he stands to take home about $700m over the course of the first year of the policy. The 39-year-old took home $27m in total compensation in 2022, the most recent year full remuneration figures were available. The decision to start paying a dividend has been welcomed by investors. Meta’s share price rose 20% when Wall Street opened on Friday – valuing the company at more than $1.2tn – but some analysts have questioned why the company has taken the step. “The move is surprising given it has flagged a need to invest heavily in AI-related infrastructure, and its metaverse project is gobbling up cash on a quarterly basis with no sign of it making a profit for years to come,” said Dan Coatsworth, an investment analyst at AJ Bell. “Paying a dividend suggests the company wants to reboot its reputation and be taken more seriously.” However, Coatsworth added that it is a “token gesture” given the dividend equates to $2 a share and therefore just a 0.4% yield for an investor based on Meta’s $461 share price. “That’s not going to win over a whole new group of investors looking for income opportunities,” he said. “In fact, it’s the type of yield to which most investors pay little attention.” Coatsworth added that while the company has said that share buybacks will remain the main mechanism for returning capital to shareholders, the initiation of dividend payments is a way to show the tech business has reached mainstream maturity like the oil, gas, banking and pharmaceutical sectors. “Companies start paying dividends when they are more grown up, perhaps when they’ve commercialised an idea and are seeing a steady increase in sales and profits,” he said. “Meta already has a well-established social media network business with a large income stream from advertising, so one could argue the dividend could have been initiated a long time ago.” Meta came in for criticism at a US Senate judiciary committee hearing on Wednesday, which was convened to interrogate Zuckerberg and other tech executives over their platforms’ impact on young users. The chief executive offered condolences to parents whose children had died after online exploitation. Throughout the hearing, Congress members touted legislation that could strip Meta and other platforms of legal immunity for content posted on its platforms. This comes months after Meta was hit with a massive lawsuit by the attorneys general of 41 states over its impact on young users. New Mexico’s attorney general has also sued the company for allegedly failing to prevent child sexual exploitation and trafficking. Meta has been contacted for comment."
    },
    {
        "id": "d3aabdf6-7040-48c4-a0e4-58a37828ce01",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/02/brisbane-portrait-prize-ai-allowed-rule",
        "title": "Australian ‘contemporary’ portrait prize allows entries wholly generated by AI ",
        "author": "",
        "published_date": "2024-02-02T04:44:39+00:00",
        "content": "A prestigious portrait competition has defended allowing entrants to submit artwork generated by artificial intelligence, arguing art is not stagnant and should reflect societal change. The Brisbane Portrait Prize – with a top prize worth $50,0000 – has been described as Queensland’s answer to the Archibalds with selected entries displayed at the State Library later in the year.   Related: Photographer admits prize-winning image was AI-generated   In the terms and conditions of entry, the Brisbane Portrait Prize notes this year that it will accept entries “completed in whole or in part by generative artificial intelligence” so long as the artwork is original and “entirely completed and owned outright” by the entrant. A spokesperson for the prize told Guardian Australia that allowing AI entries acknowledged the definition of art was not stagnant and would always grow. “BPP prides itself on being a contemporary prize and we are always interested in what ‘contemporary’ portraiture is while fostering both the ongoing evolution of art and engaging in the surrounding conversation,” they said. The spokesperson said in the past more traditional artists had objected to digital and photographic entries being allowed – which are now generally accepted in the art world. “As technology continues to adapt and integrate into our society, it has already opened the door for artists with a disability to be included thanks to the use of assistive technology, and we see the use of AI tools and methodologies as the next stage in this,” the spokesperson said. A previous winner, the painter Stephen Tiernan, told the ABC there were still artistic processes involved in the creation of AI-generated work, and ultimately the rule change kept the prize contemporary. The spokesperson said the competition would determine ownership of the work based on the processes used and the terms of the AI program behind it. When entering, artists must declare they have full copyright over the entry. Dr Rita Matulionyte, a senior lecturer in law at Macquarie University, said AI itself could not be an author under Australian copyright law, but it remained an open question how much input a person must have in an AI-assisted artwork to claim ownership. “The thing that is unclear [is] how much human contribution is enough for a human to become an author,” she said. “Is one prompt enough or is it 100 prompts that you have to make?” The Brisbane Portrait Prize spokesperson said if the artist had contributed “sufficient independent intellectual effort” in creating the work it would likely be protected by copyright. “An example of someone determining whole ownership of the content might be seen in an artist using an AI tool to produce an entirely brand-new artwork using elements of several of their own original artworks, with all original designs belonging solely to the artist,” the spokesperson said. “We recognise that AI is an evolving space and that our laws are often playing catch up to technological advancements.” Dr TJ Thomson, a senior lecturer at RMIT’s school of media and communication, said that “creating an image through a camera and imagining one through keyword prompting is a completely different experience that draws on largely different skills”. “Some knowledge of photography principles and equipment can inform one’s prompting, but having camera-produced and AI-generated imagery compete wouldn’t be fair.” It is not the first competition to have grappled with AI entries since the explosion of widely-available generative AI applications over the past year. The National Portrait Gallery’s National Photographic Portrait Prize for 2024 allows the use of generative AI tools in the development of photographic work entered – but will not allow wholly AI-generated images. But there are strict conditions, including requiring details on what tools were used and how. If the prompt to the AI includes names, images, work or creative styles of others, express consent must be obtained. Thomson said it was a messy space with many unanswered questions, but that in the meantime other competitions were likely to follow suit. The World Press Photography competition in November announced it would exclude AI-generated entries from its open format following “honest and thoughtful feedback”, stating the ban was “in line with our long-standing values of accuracy and trustworthiness”. German artist Boris Eldagsen declined the prize for the creative open category at the Sony World Photography Awards last year after stating he entered an AI-generated photo of two women “as a cheeky monkey” to find out if competitions would be prepared for AI images. “They are not,” he said in April last year. In Sydney last year, a woman’s photo of her son missed out on winning a competition because the judges were suspicions it was AI-generated, despite her stating she took the photo on her phone. The NGV’s Triennial exhibition this year in Melbourne featured works by Irish artist Kevin Abosch of “deepfakes of scenes depicting civil unrest across the world” – including in Melbourne – which examined how manipulated information could fuel civil unrest. • This article was amended on 8 February 2024. An earlier version incorrectly said that the venue for the exhibition of prize finalists was Brisbane Powerhouse, rather than the State Library."
    },
    {
        "id": "c20ab006-2c41-4601-8434-66917f933d04",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/02/uk-ministers-urged-to-protect-creatives-whose-work-is-used-by-ai-firms",
        "title": "UK ministers urged to protect creatives whose work is used by AI firms",
        "author": "",
        "published_date": "2024-02-02T00:01:33+00:00",
        "content": "Ministers must defend content creators whose work is being taken without permission by tech companies to build artificial intelligence products such as chatbots that generate “vast financial rewards”, a House of Lords committee has said.\nThe legal framework in the UK is failing to enforce the basic principles of copyright amid a rise in AI development, said the Lords’ communications and digital committee.\n“Some tech firms are using copyrighted material without permission, reaping vast financial rewards,” said the committee.\nCopyright has become a key battleground in the development of generative AI – the term for technology that generates text, image and audio from a human-typed command.\nContent creators and owners argue that their material is being used illegally to train large language models (LLMs), the technology behind chatbots, which need to be fed a huge amount of data in order to reliably predict the next word in a sequence of words.\nA report on LLMs and generative AI, published on Friday, said the principles of copyright were clear: to reward creators for their efforts; prevent use of their work without permission; and to encourage innovation.\n\n Related: OpenAI offers to pay for ChatGPT customers’ copyright lawsuits  \n\nUrging the government to take action on flouting of copyright, the committee said: “The current legal framework is failing to ensure these outcomes occur and the government has a duty to act. It cannot sit on its hands for the next decade and hope the courts will provide an answer.”\nThe committee recommended the government decides whether copyright law provides enough protection to copyright holders. If it believes there are legal uncertainties around the issue, peers said, it should set out options for updating legislation.\nLady Stowell, the committee’s Conservative chair, said: “The government needs to be clear whether copyright law provides sufficient protections to rights holders because of the introduction of LLMs. If the government is clear that the legislative framework is not adequate then it should update that legislative framework.”\nThe government’s intellectual property office is drawing up a code of practice on copyright and AI. Under the 1988 Copyright Act an exemption is made for text and data mining if it is research for “a non-commercial purpose”. In 2022 the government indicated that it would widen that exemption to any use but has now rowed back on that.\n \n\n\n\n\n\nStowell added that the UK, with its wealth of private and government-owned data, could offer licensed datasets to AI firms hoping to build models on a secure legal basis. “If we can create new licensed datasets, there is a market we ought to be able to take advantage of,” she said.\nOpenAI, the US-based developer of the groundbreaking ChatGPT chatbot, is being sued in the US by the New York Times and a number of authors for alleged copyright infringement. One group of writers, which includes the bestselling John Grisham, has accused OpenAI of “systematic theft on a mass scale”.\nOpenAI said in its submission to the committee that it would be impossible to create tools like ChatGPT without access to copyrighted material. Mark Zuckerberg’s Meta, the image generator company Stability AI and Microsoft, an investor in OpenAI, also told the committee that limiting access to data could result in substandard or biased models.\nIn the US, OpenAI’s defence relies on the concept of “fair use”, which allows use of content in certain circumstances without seeking the owner’s permission. In the UK there are also copyright exemptions under “fair dealing”, which relates to areas such as research, private study and news reporting.\nElsewhere in the report, the committee warns the government of a protracted period of “technological turbulence” owing to AI and urges ministers to act against market power being concentrated in the hands of a small number of companies.\nStowell said: “There has to be open competition. The market has to remain open. It’s dangerous if we have a situation where it is under the control of a small number of large firms.”\nA government spokesperson said: “The IPO [intellectual property office] has engaged with stakeholders as part of a working group with the aim of agreeing a voluntary code on AI and copyright. We will update on that work soon and continue to work closely with stakeholders to ensure the AI and creative industries continue to thrive together.”"
    },
    {
        "id": "dc091a9d-a911-4dea-9eb7-ed780ef8e4e7",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/01/meta-earnings-q4-zuckerberg",
        "title": "Meta revenue soars as it pivots to AI and announces dividends for investors",
        "author": "",
        "published_date": "2024-02-01T23:31:44+00:00",
        "content": "Meta shares soared 15% in after-hours trading following a strong fourth-quarter earnings report released the day after Mark Zuckerberg was roundly condemned in a contentious congressional hearing. The company also announced it will pay a 50¢-per-share dividend to investors for the first time, and has authorized a $50bn share buyback program. Overall, Meta reported fourth-quarter revenue of $40.1bn, beating the predicted $39.18bn and up 25% year-over-year. The report comes as Meta, like many of its big tech peers, is seeking to integrate artificial intelligence tools into its core products. In a statement accompanying the report, Zuckerberg said Meta has “made a lot of progress on our vision for advancing AI and the metaverse”. “We expect our ambitious long-term AI research and product development efforts will require growing infrastructure investments beyond this year,” the company’s press release read.   Related: ‘It was forced’: grieving parents unfazed by sorry tech CEOs at US Senate hearing   In the previous quarter’s earnings call, Zuckerberg touted Meta’s plans to invest in AI, stating that it would be the company’s biggest investment area in 2024. Zuckerberg said in a video shared to Instagram earlier in January that the company would be acquiring $9bn worth of Nvidia chips to support its push to scale AI AI will be used to enhance advertising campaigns and fuel advertising revenue as well as support new Meta products, including AI chatbots, Zuckerberg has said. Revenue from advertising, the company’s core business, was $38.7bn, compared with $31.25bn for the same time period the prior year. Meta’s hardware products such as the Quest 3 VR headset have yet to contribute any sizable percentage of the company’s revenue. Zuckerberg said on Thursday’s call he expects Meta to begin rolling out AI services more widely in coming months. Meta laid off more than 20,000 workers in 2023 as part of what Zuckerberg branded a “year of efficiency”, focusing on cost-cutting measures. Those efforts appeared to have paid off, with Meta’s operating margin doubling to 41% from 20% in the same quarter of 2022. Meanwhile, expenses decreased 8% year over year to $23.73bn. Chief financial officer Susan Li stated on the call that Meta had more than 67,300 employees at the end of quarter four, down 22% from the same time a year prior but up 2% from quarter three after “hiring efforts have resumed”. Regulatory headwinds appear to be top of mind for investors following Meta’s public browbeating at a congressional hearing on Wednesday, which was convened to interrogate Zuckerberg and other tech executives over their platforms’ impact on young users. He offered condolences to parents in the crowd whose children had died after online exploitation. Throughout the hearing, Congress members touted legislation that could strip Meta and other platforms of legal immunity for content posted on its platforms and comes months after Meta was hit with a massive lawsuit by attorneys general of 41 states suing the company over its impact on young users. New Mexico’s attorney general has additionally sued the company for allegedly failing to prevent child sexual exploitation and trafficking. As a result of regulatory concerns, Meta has sought to diversify its core business – which has historically relied on advertising by way of collecting huge amounts of user data. Reality Labs, the segment responsible for developing its virtual reality products, faced losses of $4.65bn in the fourth quarter, up from $4.28bn for the same period the previous year and contributing to an overall loss of $16.12bn for the year of 2023. In a press release, Meta said it expected operating losses for Reality Labs to “increase meaningfully year-over-year” as it continues to try to scale the ecosystem. In addition to regulatory concerns, Meta has seen a squeeze on user numbers for its platforms as young users in particular defect to newer platforms like TikTok. The company’s platforms are seeing more rapid growth outside of the US, said Insider Intelligence principal analyst Jasmine Enberg. “On the usage front, Facebook continued to squeeze user growth, but as expected, most new users came from outside of North America,” she said. “In the US, popularity among teens has become a liability in the eyes of lawmakers, which could hamper both Facebook’s and Instagram’s growth efforts there.”"
    },
    {
        "id": "625a1358-66ff-47b0-989b-53d839791b8c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/01/apple-earnings-q1",
        "title": "Apple breaks year-long streak of falling revenue with strong holiday sales",
        "author": "",
        "published_date": "2024-02-01T22:20:03+00:00",
        "content": "Apple has ended the quarter with its first revenue gain in over a year, but the company still struggled to cope with a decline in global smartphone demand. The company posted $119.58bn in revenue and $2.18 in earnings per share (EPS), beating Wall Street expectations of $117.91bn in quarterly revenue and $2.10 in EPS. Apple broke its four-quarter streak of declining revenue with a reported 2% growth in sales. Shares dropped in after-hours trading.   Related: Apple overtakes Samsung as world’s top smartphone seller   However, the news wasn’t all rosy for the iPhone maker. The company’s sales in China, its third-largest market, dropped from $23.9bn to $20.8bn. Global iPad sales dropped to $7bn in the most recent quarter, from $9bn in the same quarter last year. The company brought in $69.7bn in iPhone sales in the three months ending in December 2023, up from $65.8bn year-over-year. Earlier this week, the company’s stock took a tumble after the prominent TF International Securities analyst Ming-Chi Kuo predicted Apple’s iPhone shipments would decrease by 15% year-over-year, due in part to a sizable drop in iPhone sales in China. Apple’s services business maintained its growth, with the company posting $23.1bn in revenue, up from $20.8bn in services revenue the previous year. Microsoft overtook Apple as the world’s most valuable company in January, hitting a $3tn valuation while Apple’s market capitalization slipped. Investors will probably have questions about Apple’s business and the demand for its devices in China, where the company faces steep competition from Huawei, according to the Forrester analyst Thomas Husson. “The key element to watch will be – as always given its major contribution to sales during the holiday season – the performance of the new iPhone 15 lineup,” Husson wrote in a statement. “Part of its performance will depend on how Apple managed hurdles in China given both the usage ban by government officials and the fierce competition from Chinese brands.” Apple’s new headset, the Vision Pro, which is set to go on general sale on Friday after opening for preorders late last month, will be the first new product category the company is entering since launching the Apple Watch in 2015. While iPhone sales are expected to remain the strongest indicator of the company’s financial health, investors are likely to keep an eye on the demand for the new headset. According to Apple news site MacRumors, the company has sold roughly 200,000 headsets. “Sales of the new Vision Pro – starting only on February 2, will not impact results but given the high price point, dozens of thousands of pre-orders mean hundreds of millions of dollars – not bad for an entirely new product category still in its infancy,” Husson wrote. “Looking forward, the biggest question mark is Apple’s ability to leverage AI and genAI in differentiating experiences.” Apple has also emerged as a significant player in the movie industry as it spends lavishly on Apple TV+, scoring 13 Academy Award nominations. Ten of the nominations were for Killers of the Flower Moon. • This article was amended on 2 February 2024 to clarify that Wall Street expected $117.91bn in quarterly, not full-year, revenue for Apple."
    },
    {
        "id": "6a79a645-9b1d-4a4b-a31b-868ceaae33e4",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/01/amazon-earnings-q4",
        "title": "Amazon profits surge on strong trading season and cloud computing growth",
        "author": "",
        "published_date": "2024-02-01T21:19:55+00:00",
        "content": "Profits at Amazon have surged on strong seasonal trading and robust growth in its powerhouse cloud computing business. The world’s largest retailer generated revenue of $170bn in the three months to December, up 14% on the same period of 2022, and clearing expectations on Wall Street of about $166bn.   Related: Amazon drops $1.4bn deal to buy iRobot after EU veto reports   Net income hit $10.6bn in the fourth quarter, from $278m a year previously, after the company moved to cut costs and draw a line under years of rapid expansion following the onset of the pandemic. Earnings per share hit $1.03. Shares in the business rose 5.5% during out-of-hours trading in New York. After Amazon, valued at $1.6tn on the stock market, cut 27,000 jobs last year, the layoffs have continued, albeit at a lesser pace, in recent weeks. On Thursday, Andy Jassy, the group’s chief executive, said: “This Q4 was a record-breaking holiday shopping season and closed out a robust 2023 for Amazon. While we made meaningful revenue, operating income and free cash flow progress, what we’re most pleased with is the continued invention and customer experience improvements across our businesses.” Amazon has built a sprawling digital empire around its e-commerce platform, from smart speakers to sports broadcasting. Through Amazon Web Services (AWS), it also leads the lucrative cloud computing market, though Microsoft, now worth $3tn, has eaten into its lead as operators work to integrate artificial intelligence into their offerings. The company announced it would integrate a shopping assistant, powered by AI and named Rufus, into its app prior to the earnings call. Jassy said on the earnings call that Amazon’s AI offerings are “really resonating with customers”, though revenue had yet to make a meaningful contribution to the company’s bottom line. On Thursday, the company said it is launching an AI-powered shopping assistant named Rufus on its mobile app. Regulators have sought to curb the company’s growth: earlier this week, Amazon abandoned a planned $1.4bn acquisition of the robot vacuum cleaner company iRobot amid European opposition to the deal. US regulators announced last week they would probe the company’s deals with fledgling AI companies such as Anthropic. Revenue at AWS increased 13% to $24.2bn in the fourth quarter. Amazon’s fast-growing advertising business continues to “drive positive results”, Jassy noted: sales generated by the division climbed 27% to $14.7bn. For the current quarter, which runs to March, Amazon forecast sales growth of up to 13%. Operating profits are also set to rise significantly, it estimated, from $4.8bn to between $8bn and $12bn."
    },
    {
        "id": "52afd2f4-783d-477c-aed3-360c9b98dca3",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/01/elon-musk-acts-to-move-tesla-legal-base-to-texas-after-pay-package-ruling",
        "title": "Elon Musk acts to move Tesla legal base to Texas after pay package ruling ",
        "author": "",
        "published_date": "2024-02-01T11:12:18+00:00",
        "content": "Elon Musk has announced Tesla will hold a vote on moving the company’s legal base to Texas after the state of Delaware threw out his $56bn pay package at the electric vehicle maker. The world’s richest person, whose No 1 status is endangered by the Delaware ruling, held a poll on X asking whether Tesla should change the company’s state of incorporation from Delaware to Texas. With more than 1m votes cast, the poll recorded 87% in favour of moving. Responding on Thursday, Musk wrote on his X account: “The public vote is unequivocally in favour of Texas! Tesla will move immediately to hold a shareholder vote to transfer state of incorporation to Texas.” On Tuesday, a Delaware judge called a 2018 share-based package awarded to the Tesla chief executive “an unfathomable sum” that was unfair to shareholders, and struck down the arrangement. Soon after, Musk wrote on X: “Never incorporate your company in the state of Delaware.” He added: “I recommend incorporating in Nevada or Texas if you prefer shareholders to decide matters”. Musk has strong ties with Texas. He moved Tesla’s corporate headquarters from Palo Alto in California to the Texas city of Austin in 2021 after criticising California’s regulations and taxes, and also clashing with health officials at the start of the Covid pandemic over reopening a factory in Fremont. One of the carmaker’s gigafactories is in Texas and it is building a $1bn (£800m) lithium refinery in the state, aiming to produce enough battery-grade lithium for about 1m vehicles by 2025. Other Musk companies, including the rocket firm SpaceX and the tunnelling business the Boring Company, also have operations in Texas. Another Musk-related Texas expansion is under way at X, which he also owns, after the social media platform announced last month it would open a content moderation office in Austin with 100 staff. It was not immediately clear whether the Tesla board supported Musk’s X declaration on Thursday. The Delaware judge Kathaleen McCormick in her ruling on Tuesday, wrote that Musk, a 13% shareholder in Tesla, had “thick ties” with the directors negotiating his 2018 package and had “dominated the process that led to board approval of his compensation plan”. Brian Quinn, a professor at Boston College law school, said moving to Texas “will require the board to initiate the reincorporation.” He added: “So, if he hasn’t he should drop them a line because it’s supposed to be a board call, not the result of a poll on X.” Quinn said switching to Texas would not change the underlying law behind the Delaware ruling, but the state’s launch of a new business court means it will want to show the court is “a fair forum for the adjudication of business disputes.” “It won’t help that effort to have their very first customer be Elon Musk who expects them to kowtow to managers’ interests. He’s putting Texas in a bit of a bind,” he said. Two weeks before the Delaware ruling, Musk challenged the board by writing on X that he needed to own 25% of Tesla’s voting shares or else he might build unspecified artificial intelligence and robotics products outside the company. Musk is worth $202bn, according to the Bloomberg billionaires index, more than the luxury goods tycoon Bernard Arnault, on $183bn, and Jeff Bezos, the Amazon founder, on $180bn."
    },
    {
        "id": "c10258de-249a-40cf-b8d5-fccd19a7b0ee",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/feb/01/more-than-half-uk-undergraduates-ai-essays-artificial-intelligence",
        "title": "More than half of UK undergraduates say they use AI to help with essays",
        "author": "",
        "published_date": "2024-02-01T00:01:37+00:00",
        "content": "More than half of undergraduates say they consult artificial intelligence programmes to help with their essays, while schools are trialling its use in the classroom. A survey of more than 1,000 UK undergraduates, conducted by the Higher Education Policy Institute (Hepi), found 53% were using AI to generate material for work they would be marked on. One in four are using applications such as Google Bard or ChatGPT to suggest topics and one in eight are using them to create content. Just 5% admitted to copying and pasting unedited AI-generated text into their assessments.   Related: AI likely to spell end of traditional school classroom, leading expert says   Teachers are also seeking to use AI to streamline their work, with the Education Endowment Foundation (EEF) signing up secondary schools for a new research project into the use of AI to generate lesson plans and teaching materials as well as exams and model answers. Dr Andres Guadamuz, a reader in intellectual property law at the University of Sussex, said it was no surprise that more students were adopting AI and suggested institutions needed to be explicit in discussing how best to use it as a study tool. “I’ve implemented a policy of having mature conversations with students about generative AI. They share with me how they utilise it,” Guadamuz said. “My primary concern is the significant number of students who are unaware of the potential for ‘hallucinations’ and inaccuracies in AI. I believe it is our responsibility as educators to address this issue directly.” The Hepi survey found that one in three students using AI did not know how often it “hallucinates”, ie invents statistics, academic citations or book titles to fill in what it perceives to be gaps. Guadamuz said he had essays handed in last year that clearly used unedited ChatGPT output, given away by the “boring” style in which they were written. But as AI usage has spread, the survey found fewer students were willing to use it. “The world is evolving, and as educators we need to adapt by establishing clear guidelines and policies, as well as designing more challenging assessments. However, this is difficult in a resource-constrained environment where academics are already overburdened and underpaid,” Guadamuz said. According to the EEF’s proposal, the use of AI might instead help cut the workload burden on teachers, as well as improving the quality of their teaching. Gillian Keegan, the education secretary, has said AI could take on the “heavy lifting” of marking and planning for teachers. Half of the 58 schools in England taking part in the EEF’s project will be given a toolkit to create assessment materials such as practice questions, exams and model answers, and to tailor lessons to specific groups of children. The AI-generated lesson plans will be assessed by an independent panel of experts. Prof Becky Francis, the chief executive of the EEF, said: “There’s already huge anticipation around how this technology could transform teachers’ roles, but the research into its actual impact on practice is – currently – limited. “The findings from this trial will be an important contribution to the evidence base, bringing us closer to understanding how teachers can use AI.”"
    },
    {
        "id": "c332f049-7ea7-4eea-8071-312c7ae72fd0",
        "type": "liveblog",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/live/2024/jan/31/congress-social-media-hearing-tiktok-meta-twitter-child-safety-latest-updates",
        "title": "Mark Zuckerberg and Snap CEO Evan Spiegel apologize to families of online harm victims at Senate hearing – as it happened",
        "author": "",
        "published_date": "2024-01-31T19:51:55+00:00",
        "content": "  7.36pm GMT  'A moment of reckoning': Senate hearing on sexual exploitation on social media comes to a close  After five tech executives faced four hours of intensive questioning from Congress members, the Senate judiciary hearing on “Big Tech and the Online Child Sexual Exploitation Crisis” has come to an end. Executives in attendance were Mark Zuckerberg of Meta, Linda Yaccarino of X (formerly Twitter), Shou Zi Chew of TikTok, Evan Spiegel of Snap, and Jason Citron of Discord. In closing statements, Senate chair Dick Durbin called for meaningful bipartisan legislation to address the “crisis” of childhood sexual exploitation online. “Businesses exist by and large to be profitable, and I think we have to get behind that and say, ‘profitability at what cost?’” he said. Here are the key events from the day:  Parents of children who died by suicide after experiencing online harms packed the Senate for the hearing on Wednesday. Senator Lindsey Graham said the event drew “the largest [audience] I’ve seen in this room”. Opening statements in the hearing included voices of children and parents of children affected. In opening statements, executives enumerated tools on their platforms meant to protect children and give parents more control over the user experience. Senators challenged these assertions in their questioning, stating that the measures are ineffective and insufficient. In his first remarks, Meta CEO Mark Zuckerberg cast doubt on the relationship between social media use and a decline in mental health. “The existing body of scientific work has not shown a causal link between using social media and young people having worse mental health,” he said. Senators repeatedly targeted Section 230, a legal protection that shields social media firms from liability for content posted on their platforms. They promoted a number of bills that would take aim at these protections, including the Kids Online Safety Act (Kosa). X, formerly Twitter, became the first tech firm to publicly endorse the Stop CSAM Act, a bill introduced by Sen. Dick Durbin (D-IL) that would remove legal immunity for civil claims against internet companies over child sex abuse material. CEO Linda Yaccarino also announced support of Kosa, joining Snapchat parent company Snap. Zuckerberg apologized directly to families of online harm victims in Senate hearing, standing from his seat and turning to a crowd holding photos of children who have died by suicide after experiencing online harms. “I’m sorry for everything you have all been through,” he said.  Snap CEO Evan Spiegel also apologized to families, though he did not turn to face them as Zuckerberg did. In response to questions about children who have died from drugs bought on Snapchat, he said: “I’m so sorry that we have not been able to prevent these tragedies. We work very hard to block all search terms related to drugs on our platform”.   Updated at 7.51pm GMT    7.10pm GMT   The Guardian view on online child protection: the web needs more health and safety   Related: The Guardian view on online child protection: the web needs more health and safety | Editorial       7.10pm GMT   The Senate hearing over alleged online harms to children has come to a close. In final statements, chairman Durbin said that America has reached “a moment of reckoning” regarding the unchecked power of Big Tech and that legislation must be passed to rein it in.  “As parents and grandparents we know what daughters and sons and others are going through they cannot cope,” he said. “They cannot handle this issue on their own. They’re counting on us, as much as they’re counting on the industry, to do the responsible thing. “     6.55pm GMT   Senator Blackburn took a hard stance against Mark Zuckerberg, asking why intentionally predatory content does not violate platform standards, citing a June 2023 article from the Wall Street Journal investigating how Instagram connects “a vast pedophile network”. The report revealed that Instagram was recommending hashtags for children to sell CSAM of themselves. Blackburn called Meta “the largest sex trafficking organization in the world”. Zuckerberg responded agitatedly, saying: “Senator, that’s ridiculous”. Blackburn closed by stating Meta and other companies have an obligation to work with Congress, stating that they have not complied voluntarily in the past. “You have an army of lawyers who fight us on any bipartisan legislation,” she said. “Are you going to stop lobbying against the legislation considered in this hearing? You need to come to the table – kids are dying.”  Updated at 7.07pm GMT    6.38pm GMT   In her line of questioning, Senator Marsha Blackburn cited an internal email at Meta revealed as part of a lawsuit from US attorneys general that stated product teams that referred to younger users in terms of their lifetime value of being “roughly $270 per teenager”. She acknowledge teenagers in the crowd for the hearing wearing shirts that read “I’m worth more than $270,” which elicited applause from the crowd.   Mark Zuckerberg and Linda Yaccarino are on Capitol Hill.So are we. pic.twitter.com/o9sWtk1DlE— Design It For Us (@DesignItForUs) January 31, 2024      6.19pm GMT  Spiegel apologizes to parents of children who obtained deadly drugs via Snapchat  Sen. Laphonza Butler (D-CA) asked Snap CEO Evan Spiegel: “There are a number of parents whose children have been able to access illegal drugs on your platform. What do you say to those parents?” Parents over more than 60 teenagers filed suit in late 2023 against Snap for allegedly facilitating their children’s acquisition of drugs that were used in overdoses. Unlike Zuckerberg, Spiegel did not turn to face the families in attendance. He said, “I’m so sorry that we have not been able to prevent these tragedies. We work very hard to block all search terms related to drugs on our platform.” He highlighted Snapchat’s education efforts to convey the message of “one pill can kill”. His full apology below:  We work very hard to block all search terms related to drugs from our platform. We proactively look for and detect drug-related content. We remove it from our platform, preserve the evidence, and then we refer it to law enforcement for action. We’ve worked together with nonprofits and with families on education campaigns, because the scale the fentanyl epidemic is extraordinary. Over 100,000 people lost their lives last year and we believe people need to know that one pill can kill. That campaign was viewed more than 260 million times on Snapchat.   Updated at 6.35pm GMT    6.16pm GMT   The hearing is back on, starting with questioning from Senator John Neely Kennedy who said the reforms and tools created by social media firms are not sufficient and action from Congress is needed. “Your companies are going to need the government’s help,” he said. “Any reforms you do on your own are like paint on rotten wood.”     6.07pm GMT   Meta is the world’s ‘single largest marketplace for paedophiles’, says New Mexico attorney general In an interview with the Guardian, New Mexico’s top legal official Raúl Torrez talks about taking Facebook and Instagram’s parent company to court.  In December 2023, Torrez launched a major lawsuit against Meta, claiming that the company has allowed its social media platforms to become marketplaces for child predators. Torrez tells the Guardian he believes that what his own investigation has already uncovered is “just the tip of the iceberg when it comes to how widespread and well known this problem was inside the company”. He said he expects further details to emerge about its knowledge of child sexual exploitation on its platforms.   Related: Meta is the world’s ‘single largest marketplace for paedophiles’, says New Mexico attorney general       5.50pm GMT   The hearing is taking a 10-minute recess. More updates soon …  Updated at 5.58pm GMT    5.49pm GMT   Republican senator Tom Cotton used his time on the floor of the Senate hearing to fire off a series of xenophobic questions to TikTok CEO Shou Zi Chew. Echoing a prior line of questioning from Senator Ted Cruz, he asked Chew if he knows “what happened in Tiananmen Square in 1989”, alleging that the China-owned app censors related content. He also asked Chew if he believes Chinese president Xi Jinping is a dictator. “I’m not going to comment on any world leaders, it’s not appropriate,” Chew said. Cotton responded by questioning if Chew believes he would lose his job if he says anything negative about the Chinese Communist party. “Are you scared that you’ll be arrested and disappeared the next time you go to mainland China?” he asked. Cottton pointedly asked Chew “of what nation are you a citizen?” and if he has ever applied for American or Chinese citizenship. The TikTok CEO replied, exasperated, that he is from Singapore.  Tom Cotton: \"Have you ever been a member of the Chinese Communist Party?\"TikTok CEO Shou Zi Chew: \"Senator, I'm Singaporean. No!\"Cotton: \"Have you ever been associated or affiliated with the Chinese Communist Party?\"Chew: \"No, Senator. Again, I'm Singaporean!\" pic.twitter.com/5Wa72aJIr9— Justin Baragona (@justinbaragona) January 31, 2024  Chew repeatedly stated that content critical of China and “any other country” can be freely found on TikTok.  Updated at 5.50pm GMT    5.31pm GMT   Zuckerberg and Hawley spar over social media’s effects on mental health after Meta CEO says the science isn’t there Senator Josh Hawley targeted Mark Zuckerberg over claims in his opening statements that there is not scientific evidence that suggests there are widespread mental health impacts from social media. “I think it’s important to look at the science,” said Zuckerberg. “I know people widely talked about this as if that is something that’s already been proven, and I think that the bulk of the scientific evidence does not support that.” At the start of the hearing, Zuckerberg said, “The existing body of scientific work has not shown a causal link between using social media and young people having worse mental health.” Hawley cited Meta’s own internal research, leaked by Facebook whistleblower Frances Haugen, that showed Instagram has made body image issues worse for one in three girls. Another Facebook study of teenagers in the UK and the US, more than 40% of Instagram users who said they felt “unattractive” said the feeling began while using the app. “Your own study says that you make life worse for one in three teenage girls, and you’re here testifying to us in public that there’s no link,” he said. “For years you’ve been coming in public and testifying under oath that there’s absolutely no link, while internally you know full well your product is a disaster for teenagers.”  Updated at 7.17pm GMT    5.18pm GMT  Mark Zuckerberg apologizes directly to families of online harm victims in Senate hearing  Following an intense line of questioning with Mark Zuckerberg, Republican senator Josh Hawley asked the Meta executive if he would like to apologize to families of victims who are sitting in the audience of today’s Senate hearing, many of whom hold photos of children they say died or were harmed due to his platform. Zuckerberg stood up from the microphone, turned around to face the audience, and began to speak. “I’m sorry for everything you have all been through,” he said. “No one should go through the things that your families have suffered and this is why we invest so much and we are going to continue doing industry wide efforts to make sure no one has to go through the things your families have had to suffer.”  Updated at 6.41pm GMT    5.01pm GMT   US police prevented from viewing many online child sexual abuse reports, lawyers say Social media firms relying on AI for moderation generate unviable reports which prevent authorities from investigating cases  Social media companies relying on artificial intelligence software to moderate their platforms are generating unviable reports on cases of child sexual abuse, preventing US police from seeing potential leads and delaying investigations of alleged predators.    Related: Revealed: US police prevented from viewing many online child sexual abuse reports, lawyers say       5.00pm GMT   The Guardian has previously reported that Meta was accused of not having done enough to safeguard children following the death of a teenager in the UK. Whistleblower Arturo Béjar said the social media company already has the infrastructure in place to shield teenagers from harmful content.  Béjar, a former senior engineer and consultant at the Instagram and Facebook owner, said if the company had learned its lessons from Molly Russell’s death and subsequent inquest it would have created a safer experience for young users.  You can read our reporting here:   Related: Meta has not done enough to safeguard children, whistleblower says    Updated at 5.09pm GMT    5.00pm GMT  Republican senator Ted Cruz berates Mark Zuckerberg  Ted Cruz is screaming at Mark Zuckerberg over child abuse content on Instagram. He noted that if users search for hashtags related to CSAM (Child Sexual Abuse Material) the platform warns users that the content may be illegal and gives them the option to click through and see it. “Mr Zuckerberg, what the hell were you thinking?” Cruz yelled during the hearing.  41/ @tedcruz is pointing out that when you search for csam on Instagram you can receive a screen asking you “do you want to view CSAM” and allows people to proceed and see it pic.twitter.com/4XR9F9ohhn— Frances Haugen (@FrancesHaugen) January 31, 2024  Cruz asked how many times this warning screen has been served to users. Zuckerberg said he did not know but will “personally look into this” and follow up with the committee.  Updated at 5.59pm GMT    4.53pm GMT   Here’s a video clip of the tense exchange between Graham and Zuckerberg:  Sen. Lindsey Graham (R-SC) asks Mark Zuckerberg about Gavin Guffey, the son of SC State Rep. Brandon Guffey, who died by suicide after an Instagram sextortion scam.Graham: “Do you think [Guffey] should be allowed to sue you?”Zuckerberg: “I think that they can sue us.” pic.twitter.com/lX6Ztmpwcu— The Recount (@therecount) January 31, 2024      4.47pm GMT  Graham accuses social media chiefs of dragging their heels on online safety legislation  Senator Lindsey Graham targeted Meta CEO Mark Zuckerberg with questions about a teenage Instagram user who died by suicide after being targeted by a sextortion ring via the platform, asking the executive if he believes the family of the victim should be able to sue the company. “I think that they can,” Zuckerberg responded. Graham retorted that such lawsuits are often thrown out due to section 230, and promoted bipartisan legislation that would more easily allow victims to take companies to court. In a tense line of questioning, the Republican senator asked top tech executives if they supported such legislation, and noted that they refused to answer directly with a “yes” or “no”. “The bottom line, I’ve come to conclude, is that you aren’t going to support any of this,” he said. “If you’re waiting on these guys to solve the problem, we’re going to die waiting.”  Updated at 4.52pm GMT    4.06pm GMT  X becomes first online platform to endorse the Stop CSAM Act  In her opening statements before Congress, X chief executive officer Linda Yaccarino said the company endorses the Stop CSAM Act, a bill introduced by Sen. Dick Durbin (D-IL) that would remove legal immunity for civil claims against internet companies over child sex abuse material. “You have my personal commitment that X will be active and a part of this solution,” Yaccarino said. “X believes that the freedom of speech and platform safety can and must coexist. We agree that now is the time to act with urgency.” Durbin thanked her for being the “first social media company” to publicly endorse the act. “It is our honor, chairman,” she replied. Yaccarino said in her opening remarks that X had bolstered its resources for dealing with child sexual abuse material, though how much is unclear: Elon Musk reduced the size of the company by more than half, including its trust and safety teams.  Yaccarino stopped short of endorsing the Kids Online Safety Act (Kosa), another bill meant to target section 230 immunity for social media firms that was recently endorsed by the Snapchat parent company. But she said X “supports the progress” of the bill. Snap’s Evan Spiegel has endorsed the measure. Kosa and the Stop CSAM act have been flagged by civil and digital rights groups for potential privacy and freedom-of-speech violations, due in part to their targeting of encryption tools as well as potential use of such bills to target LGBTQ content misidentified as dangerous.  Updated at 4.31pm GMT    4.05pm GMT  Instagram whistleblower says emails show wellbeing was never priority at Meta  Arturo Béjar, a former senior engineer and consultant at Meta who testified in Washington in November about what he described as child safety failings at the company. He said emails released in advance of the hearing showed executives and Zuckerberg failing to act on issues like suicide ideation. “If this work was a priority, there would be a thousand engineers and product managers on well-being. This is a company with over 30,000 engineers,” he told the Guardian.  Updated at 4.08pm GMT    4.04pm GMT  Meta rejected greater investment in child safety, internal documents show  Mark Zuckerberg was asked for more resources to increase teen wellbeing on his platforms by senior lieutenant Sir Nick Clegg in 2021, according to emails released by Sen. Richard Blumenthal (D-CT) ahead of Wednesday’s hearing. Clegg, then vice‑president for global affairs and communications at the Facebook and Instagram owner, asked for more investment in August 2021 to “strengthen our position on well-being across the company.” he said the matter was becoming urgent as “politicians in the US, UK, EU and Australia are publicly and privately expressing concerns about the impact of our products on young people’s mental health.” He said the company’s efforts on addressing wellbeing concerns among users was “being held back by a lack of investment”. In one reply to Clegg, Sheryl Sandberg, then chief operating officer at Meta, said she was “supportive” of the investment request but “we have overall budgeting issues across the board so no promises on what will happen.” “The hypocrisy is mind-boggling,” Blumenthal told The New York Times. “We’ve heard time and time again how much they care and are working on this, but the documents show a very different picture.” In response to criticism of its efforts to safeguard children’s well-being, Meta has said it employs 40,000 people to improve trust and safety and has invested $20bn in such efforts since 2016. A Meta spokesperson said: “These cherry-picked documents do not provide the full context of how the company operates or what decisions were made.”  Updated at 6.20pm GMT    3.50pm GMT  Social media executives share prepared statements; Zuckerberg calls for Apple and Google to protect children via app stores  The congressional hearing investigating online sexual and other exploitation of children is under way, with executives from Meta, X, TikTok, Snap, and Discord sharing prepared statements. Ahead of the statements, Sen. Dick Durbin (D-IL) noted that Mark Zuckerberg of Meta and Shou Zi Chew of TikTok were appearing voluntarily, whereas Linda Yaccarino of X (formerly Twitter), Evan Spiegel of Snap, and Jason Citron of Discord had to be subpoenaed and forced to appear. “I hope this is not a sign of your commitment or lack of commitment to addressing serious issue before us,” he said. After being sworn in, the executives began their testimonies, focusing largely on what tools their platforms have released to protect kids online. Many of them mentioned their own children and expressed their commitment to protecting kids online. “All of us here on this panel today throughout the tech industry have a solemn and urgent responsibility to ensure that everyone who uses our platform is protected from these criminals, both online and off,” Citron said in his opening statements. Zuckerberg stated that Meta has introduced more than 30 such tools over the last eight years, including controls that let parents set time limits for app usage and see who their children are following and engaging with online. He added that Meta has spent $20bn on safety and security since 2016 and employs about 40,000 people to address such concerns. “We build technologies to tackle the worst online risks and share it to help our whole industry get better,” he said. Zuckerberg sought to shift more responsibility for children’s safety online to Apple and Google, which operate the world’s biggest app stores. His company made a similar point last year when it called for legislation that would require parental approval for app purchases and downloads by teens. Shou Zi Chew of TikTok cited the app’s “robust community guidelines”, including family pairing tools like setting screen time limits and filtering out certain content. He stated TikTok has more than 40,000 trust and safety professionals and expect to invest more than $2bn in trust and safety efforts in 2024 alone. “Keeping kids safe online requires a collaborative effort as well as collective action,” he said. “We’ve shared the community’s concern and commitment to protect young people online. We welcome the opportunity to work with you on legislation to achieve this goal.” Evan Spiegel of Snap, like other executives, acknowledged the victims of online harms in the room and parents of children who have been impacted by online harms. “Words cannot begin to express the profound sorrow. I feel that a service you designed to bring people happiness and joy has been abused to cause harm,” he said. “I want to be clear that we understand our responsibility to keep our community safe.”  Updated at 4.35pm GMT    3.45pm GMT   Family members of children who killed themselves after online sexual exploitation are attending the congressional hearing with portraits in hand. The mood in the room is tense and somber. Dozens of family members of victims are in the audience, holding up their loved one’s photos, hoping to catch the attention of the CEOs are they filed into the room flanked by staff. One woman quietly wept into her handkerchief. They have applauded senators’ jibes at the tech CEOs multiple times.  One opening remark by Durbin elicited laughter: “Coincidentally, some of these platforms have implemented common-sense child safety protections within the last week.”  Updated at 4.40pm GMT    3.41pm GMT  Lindsey Graham to tech CEOs: ‘You have blood on your hands’  In opening statements at Wednesday’s hearing, Sen. Lindsey Graham (R-SC) referenced parents and children in attendance, many of whom brought photos of their children they say died or suffered emotional damage as a result of social media. “To all the victims who came and showed us photos of your loved ones don’t quit, you’re making a difference,” he said. “Hopefully we can take your pain and turn it into something positive.” Like Senator Dick Durbin, Graham took aim at the legal immunity that Section 230 allows social media firms, stating it is “now time to repeal” the measure. “They’re destroying lives, and threatening democracy itself. These companies must be reined in, or the worst is yet to come,” he said. Referencing the chief executives in attendance, he said: “I know you don’t mean it to be so, but you have blood on your hands.” His statements drew applause from attendees, a rare riotous moment for a Congressional hearing from a passionate crowd. Before CEOs began their testimony, Durbin requested that attendees do not stand, shout, or applaud witnesses. “We have a large audience, the largest I’ve seen in this room,” he said. “I know there is high emotion in this room, for justifiable reasons. But I asked you to please follow the traditions of the committee.”  Updated at 4.02pm GMT    3.39pm GMT   Meta’s new parental tools will not protect vulnerable children, experts say Tech firm gives parents greater control over their children’s online activities, but not all kids have consistent supervision  Meta introduced new parental supervision tools in July, but child protection and anti-sex trafficking organizations say the new measures offer little protection to the children most vulnerable to exploitation, and divert the responsibility from the company to keep its users safe.    Related: Meta’s new parental tools will not protect vulnerable children, experts say       3.34pm GMT   Meta estimates about 100,000 children using Facebook and Instagram receive online sexual harassment each day, including “pictures of adult genitalia”, according to internal company documents made public as part of a lawsuit by the New Mexico attorney general.   Related: Meta documents show 100,000 children sexually harassed daily on its platforms    Updated at 4.05pm GMT    3.28pm GMT  Congressional hearing opens with stories of exploited children  A Senate judiciary committee hearing on Wednesday exploring how social media firms allegedly fail to protect their youngest users opened with a video featuring voices of children who described being were sexually exploited online. They said they were sexually exploited on Facebook, Instagram, X, and other platforms. The video also featured parents who say their children killed themselves following sexual exploitation online. “Big tech failed to protect me from online sexual exploitation,” one child said in the video. “We need Congress to do something for our children and protect them,” a parent said. In opening statements, Senator Dick Durbin said online child sexual exploitation is “a crisis in America”. He said executives in attendance on Wednesday represent tech companies that are “responsible for many of the dangers our children face online.” In attendance at the hearing are Mark Zuckerberg of Meta, Linda Yaccarino of X (formerly Twitter), Shou Zi Chew of TikTok, Evan Spiegel of Snap, and Jason Citron of Discord. “Their design choices, their failures to adequately invest in trust and safety, their constant pursuit of engagement and profit over basic safety of all put our kids and grandkids at risk,” Durbin said. The Senator called for legislation to address the harms, targeting section 230 of the Communications Decency Act, a law that exempts social media firms from legal liability for content and activity on their platforms. “Let this hearing be a call to action we need to get kids online safety legislation to the president’s desk,” Durbin said.  Updated at 3.47pm GMT    3.25pm GMT   US surgeon general issues advisory on ‘profound’ risks of child social media use Dr Vivek Murthy called on tech companies and policymakers to take ‘immediate action’ to protect children’s mental health in May  Social media use by children and teenagers can pose a “profound risk of harm” to their mental health and wellbeing, the US surgeon general is warning. Murthy said that in the absence of robust independent research it is impossible to know whether social media is safe for children and adolescents. “The bottom line is we do not have enough evidence to conclude that social media is, in fact, sufficiently safe for our kids. And that’s really important for parents to know,” he said.    Related: US surgeon general issues advisory on ‘profound’ risks of child social media use       3.22pm GMT  How Facebook and Instagram became marketplaces for child sex trafficking  Our two-year investigation suggests that the tech giant Meta is struggling to prevent criminals from using its platforms to buy and sell children for sex. Read more here:   Related: How Facebook and Instagram became marketplaces for child sex trafficking    Updated at 4.05pm GMT    3.18pm GMT   ‘The tide has turned’: why parents are suing US social media firms after their children’s deaths Social media firms have faced scrutiny from Congress over their impact on young users, but parents who have lost kids to online harm are now leading the charge   Related: ‘The tide has turned’: why parents are suing US social media firms after their children’s death       3.14pm GMT   ‘Fundamentally against their safety’: the social media insiders fearing for their kids Parents working for tech companies have a first-hand look at how the industry works – and the threats it poses to child safety  “I really can’t imagine a world where, as things stand today, these things are safe for a 13-year-old to use,” Arturo Bejar told the Guardian. Bejar left Facebook in 2015, where he spent six years making it easier for users to report when they had problems on the platform. But it wasn’t until his departure that he witnessed what he described in recent congressional testimony as the “true level of harm” the products his former employer built are inflicting on children and teens – his own included. He discovered his then 14-year-old daughter and her friends were routinely subjected to unwanted sexual advances, harassment and misogyny on Instagram, according to his testimony.    Related: ‘Fundamentally against their safety’: the social media insiders fearing for their kids       3.05pm GMT  CEOs of Meta, X, TikTok, Snap and Discord arrive in Congress  Chief executives from five major social media firms arrived in Congress on Wednesday morning to face questioning about alleged harms to young users caused by their platforms. The hearing, titled “Big Tech and the Online Child Sexual Exploitation Crisis”, promises to “examine and investigate the plague of online child sexual exploitation”, according to a statement from the US Senate Judiciary Committee. In attendance are chief executive officers including Mark Zuckerberg of Meta, Linda Yaccarino of X (formerly Twitter), Shou Zi Chew of TikTok, Evan Spiegel of Snap, and Jason Citron of Discord. In a speech on Tuesday preceding the hearing, Senator Dick Durbin said combating dangers faced by children online has been one of his “top priorities” as chair of the committee and said he plans to ask executives “what they’re doing to make their platforms inaccessible to child sex offenders”. “As recently as last week, some have launched new child safety measures that are long overdue, but it should not take a hearing before the Senate Judiciary Committee to finally get these companies to prioritize child safety,” he said. “Because these changes are half measures at best, I welcome the opportunity to question them about what more needs to be done.” Executives appearing in Congress are expected to highlight controls and tools introduced to manage children’s online experiences and mitigate harm. In prepared remarks, Zuckerberg stated that Meta has introduced more than 30 such tools over the last eight years, including controls that let parents set time limits for app usage and see who their children are following and engaging with online. He added that Meta has spent $20bn on safety and security since 2016 and employs around 40,000 people to address such concerns. “We’re committed to protecting young people from abuse on our services, but this is an ongoing challenge,” he said. “As we improve defenses in one area, criminals shift their tactics, and we have to come up with new responses.”  Updated at 3.48pm GMT "
    },
    {
        "id": "637b1793-2380-4b7f-abeb-fa3c5d3e42ef",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/31/george-osborne-coinbase-cryptocurrency-us-court",
        "title": "George Osborne joins Coinbase as crypto firm faces US court fight",
        "author": "",
        "published_date": "2024-01-31T12:35:27+00:00",
        "content": "George Osborne has been hired by the American cryptocurrency exchange operator Coinbase as it faces an intense legal battle with the US regulator. The San Francisco-based company announced on Wednesday that it had appointed the former UK chancellor to its advisory council and would be “relying on his insights and experiences as we grow Coinbase around the world”. Osborne’s appointment comes as the Securities and Exchange Commission (SEC) is suing Coinbase, accusing it of operating as a middleman on crypto transactions while evading disclosure requirements meant to protect investors. The company disputes the allegations and is fighting them in court. It is the latest in a series of well-paid jobs Osborne has taken on since leaving government in 2016. At one point he had a portfolio of nine jobs ranging from newspaper editing and fund management to teaching and advising the government on levelling up the north of England. He gave up some of the jobs when he joined the boutique investment bank adviser Robey Warshaw as a partner in 2021. Osborne collected a share of a £28m payout for his work at the firm last year. His pay at Coinbase was not disclosed. “There’s a huge amount of exciting innovation in finance right now,” Osborne said of his appointment at Coinbase. “Blockchains are transforming financial markets and online transactions. Coinbase is at the frontier of these developments. I look forward to working with the team there as they build a new future in financial services.” Faryar Shirzad, the chief policy officer at Coinbase, said he was “pleased to have George join our council at an exciting time for us in the UK and globally. “George brings with him a wealth of experience in business, journalism and government. We look forward to relying on his insights and experiences as we grow Coinbase around the world.” Osborne’s other current jobs are: the president of the Northern Powerhouse Partnership; chair of the British Museum; “distinguished visiting fellow” at the Hoover Institute; visiting professor at Stanford’s Graduate School of Business, where he teaches a course in decision-making; and chair of Lingotto Investment Management, the $4.5bn investment firm of Italy’s billionaire Agnelli family’s Exor group, which owns large stakes in Juventus FC, the Economist and Ferrari."
    },
    {
        "id": "38124030-66b1-424c-a6cd-7207bc593ff5",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/31/inside-the-taylor-swift-deepfake-scandal-its-men-telling-a-powerful-woman-to-get-back-in-her-box",
        "title": "Inside the Taylor Swift deepfake scandal: ‘It’s men telling a powerful woman to get back in her box’",
        "author": "",
        "published_date": "2024-01-31T05:00:04+00:00",
        "content": "For almost a whole day last week, deepfake pornographic images of Taylor Swift rapidly spread through X. The social media platform, formerly Twitter, was so slow to react that one image racked up 47m views before it was taken down. It was largely Swift’s fans who mobilised and mass-reported the images, and there was a sense of public anger, with even the White House calling it “alarming”. X eventually removed the images and blocked searches to the pop star’s name on Sunday evening. For women who have been victims of the creation and sharing of nonconsensual deepfake pornography, the events of the past week will have been a horrible reminder of their own abuse, even if they may also hope that the spotlight will force legislators into action. But because the pictures were removed, Swift’s experience is far from the norm. Most victims, even those who are famous, are less fortunate. The 17-year-old Marvel actor Xochitl Gomez spoke this month about X failing to remove pornographic deepfakes of her. “This has nothing to do with me. And yet it’s on here with my face,” she said. Noelle Martin is a survivor of image-based abuse, a term that covers the sharing of nonconsensual sexual images and explicit deepfakes. She first discovered her face was being used in pornographic content 11 years ago. “Everyday women like me will not have millions of people working to protect us and to help take down the content, and we won’t have the benefit of big tech companies, where this is facilitated, responding to the abuse,” she says. Martin, an activist and researcher at the Tech & Policy Lab at the University of Western Australia, says that at first it was doctored pictures of her, but in the past few years, as generative AI has boomed, it has been videos, which are mostly shared on pornographic sites. “It is sickening, shocking,” she says. “I try not to look at it. If I do come across it, it’s just …” She pauses. “I don’t even know how to describe it. Just a wash of pain, really.” Even if the images aren’t particularly realistic, “it’s still enough to cause irreparable harm to a person”, she says. And good luck trying to get the images removed from the internet. “Takedown and removal is a futile process. It’s an uphill battle, and you can never guarantee its complete removal once something’s out there.” It affects everything, she says, “from your employability to your future earning capacity to your relationships. It’s an inescapable form of abuse, that has consequences that operate in perpetuity.” Martin has had to mention it at job interviews. “It’s something that you have to talk about on first dates. It infringes upon every aspect of your life.” When the campaigner and writer Laura Bates published her book Men Who Hate Women, an investigation into the cesspits of online misogyny, men would send her images that made it look as if Bates was performing “all kinds of sex acts, including individuals who sent me images of myself changed to make it look like I was giving them oral sex”. It’s hard for people to understand the impact, she says, even when you know it’s not real. “There’s something really visceral about seeing an incredibly hyper-realistic image of yourself in somebody’s extreme misogynistic fantasy of you,” she says. “There’s something really degrading about that, very humiliating. It stays with you.” And that image can be shared with potentially millions of people, she adds. Deepfake pornographic images and videos are, says Bates, “absolutely circulated within extremist misogynistic communities”. What was particularly notable about the Swift abuse was “just how far they were allowed to circulate on mainstream social media platforms as well. Even when they then take action and claim to be shutting it down, by that point those images have spread across so many other thousands of forums and websites.” A 2019 study from the cybersecurity company Deeptrace found that 96% of online deepfake video content was of nonconsensual pornography. When the vast majority of AI is being used to create deepfake pornography, she points out, “this isn’t a niche problem”. It is, she says, “just the new way of controlling women. You take somebody like Swift, who is extraordinarily successful and powerful, and it’s a way of putting her back in her box. It’s a way of saying to any woman: it doesn’t matter who you are, how powerful you are – we can reduce you to a sex object and there’s nothing you can do about it.” In that way, it’s nothing new, says Bates, “but it’s the facilitated spread of this particular form of virulent misogyny that should worry us, and how normalised and accepted it is”. We know, says Rani Govender, a senior policy and public affairs officer at the NSPCC, “that this is an issue which is absolutely impacting young people. In the same way that other forms of image-based sexual abuse work, it particularly impacts girls.” There have been cases of children creating explicit deepfake imagery of other children, often using apps that “strip” a subject in a photo. “Then this is being sent around schools and used as a form of sexual harassment and bullying. Fear is a theme that comes up a lot: worrying that people will think it’s real, that it can lead to further sexual harassment and bullying. [There is] worry about what their parents might think.” One 14-year-old girl told the NSPCC’s ChildLine service last year that a group of boys made fake explicit sexual images of her and other girls and sent them to group chats. The boys were excluded from school for a time, but returned, and the girls were told to move on, which they struggled to do. Another girl, 15, said that a stranger had taken photographs from her Instagram account and made fake nudes of her, using her real bedroom as a background. Govender says this kind of material is created by strangers online as part of a grooming process, or can be used to blackmail and threaten children. AI has also been used to generate images of child sexual abuse, which are shared and sold by offenders. Even children who haven’t been targeted are still vulnerable to seeing the proliferation of deepfake pornography. “There’s already a big challenge with how much explicit and pornographic material is easily available to children on social media sites,” says Govender. “If it’s becoming easier to produce and share this material, that’s going to have really negative impacts on children’s views of the seriousness of these images as well.” The campaign My Image My Choice was started by the creators of the 2023 film Another Body, which is about an engineering student in the US who sought justice after discovering deepfake pornography of herself. A lot of the media coverage of AI, says the film’s co-director Sophie Compton, “was exclusively focused on threats to democracy and elections, and missing the violence against women angle. What we’ve seen over the last couple of years is the development of this community that was pretty fringe and dark and intense entering the mainstream in a really concerning way.” Women started getting in touch with her: “The number of responses we got was quite overwhelming.” For women who work online particularly, such as YouTubers, many “have basically had to accept that it’s part of the job, that they are going to be deepfaked on a huge scale”. The word deepfake – now used as a catch-all term to describe any digitally manipulated image or video that can look convincingly real – was originally coined to refer to pornography, points out Henry Ajder, a deepfakes and AI expert who has been researching this for years, and has advised the UK government on legislation. In 2017, Reddit forum users were putting female celebrities’ faces into pornographic footage. It was Ajder’s research in 2019 that found that almost all deepfake content was pornographic, and by 2020 he was discovering communities on the messaging platform Telegram “where hundreds of thousands of these images were being generated”. As AI quickly developed, it “changed the game yet again”. People using open-source software – as opposed to AI tools such as Dall-E 3 or Midjourney, which have been trained to prohibit pornographic content – can essentially create what they like, which can include extreme and violent fantasies made real. Swift is not a new target, says Ajder, who remembers explicit footage and images of her circulating five years ago. “What is novel in this case is the way that this content was able to spread on an open, popular social media platform. Most of this stuff prior has been shared in places like 4chan, Discord communities or on dedicated deepfake pornography websites.” Over the past six years, Ajder has spent a lot of time “in pretty dark corners of the internet, observing the characteristics and behaviours, the ways that these people who are creating this interact. It’s safe to assume that the vast, vast majority are men. I think a lot of people targeting celebrities are doing so for sexual gratification. It’s often accompanied by very misogynistic language – it may be sexual gratification, but it’s very much coupled with some pretty awful views about women.” He has seen men targeted, too, particularly in countries where homosexuality is forbidden, but the victims are overwhelmingly women. There have been cases, he says, where images have been created as “revenge porn”. “It’s also been used to target female politicians as a way to try to silence and intimidate them. It really does manifest a lot of the challenges that women already face, but provides a whole new visceral and very potent weapon to dehumanise and objectify.” Is there a financial motive? “Yes and no,” says Ajder. “Some websites have certainly profited, whether that’s through advertising revenue, or through charging [for images].” But with the leaps forward in technology, it has become more accessible than ever. “What previously might have been computationally very intensive and difficult can now be run on a gaming PC or a high-powered laptop.” Ajder believes millions of women and girls have been victims of this. “The amount of people that I now hear from in schools, and workplace contexts, who are falling victim to this is unsurprising, but still incredibly disturbing,” says Ajder. “While it’s sad that it’s taken one of the biggest celebrities in the world to be targeted for people to acknowledge how big a problem this is, my hope is that this can be a catalyst for meaningful legislative change.” It should be “very clear”, says Ajder, “that if you are creating or sharing or engaging with this kind of content, you are effectively a sex offender. You’re committing a sexual offence against another human being.” Under the UK’s new online safety act, the sharing of nonconsensual deepfake pornographic material is illegal. “I don’t think anyone’s expecting large numbers of criminal convictions, but technically a lot of the sharing of these images of Taylor Swift would have constituted a criminal offence,” says Clare McGlynn, a professor of law at Durham University and an expert in image-based abuse. She and others have been campaigning to change the law on altered images for many years, “but largely we were shouting into the void”. For years, she says, the government’s line was that the harms of fake images were not significant, “although, of course, they just asserted that without actually speaking to victims. It’s a broader issue of online abuse against women and girls not being taken as seriously. People are not understanding that the harms of this can be profound and devastating and are constant and ongoing – it doesn’t just happen and you can then try to get over it and move on with your life. It’s always likely to be on the internet, always reappearing.” McGlynn believes the Online Safety Act is a missed opportunity. “The offence is just about the distribution of an altered image – it’s not about its creation.” And it lets platforms off too easily. She says draft guidance from Ofcom, the regulator, is “relatively weak and focuses on individual pieces of content”, rather than the entire systems that facilitate abuse. “It’s not yet taking as strong a position to try and get the platforms to really do something.” Social media companies such as Discord will point out they have moderators, while X says it has a “zero tolerance” policy towards posting nonconsensual nudity, although when an image can be viewed tens of millions of times before its removal, that starts to look a little hollow. AI is clearly only going to get better and become more easily available, with concerns about fake news, scams and democracy-shaking disinformation campaigns, but with deepfake pornography, the damage is already being done. “It’s somewhat unique, compared to some of the other threats that AI-generated content poses, in that it does not have to be hyper-realistic to still do harm,” says Ajder. “It can be clearly fake and still be traumatising and humiliating. It’s already very potent.” But still it could get worse in ways we have, and haven’t, thought of. Ajder is concerned about AI-generated audio, which can replicate someone’s voice, and as the pace of developments within virtual reality picks up, so will the possibility of sexual abuse within it. “We’ve already seen cases where you can quite crudely put the face of someone on to an avatar that you can effectively manipulate however you want, sexually. I worry that the very fast-evolving space of synthetic AI-generated video combined with virtual reality is going to lead to more abuse, particularly of women.” We need to get over the idea that because it’s online, or that it is labelled as fake, it isn’t harmful, says Bates. “People think this isn’t violence,” she says. “There isn’t any accountability for tech companies who are allowing this stuff to proliferate; there isn’t any kind of any retribution for allowing this to happen.” Whether you’re a girl at school, or a woman whose photograph has been copied, or a global pop star, once those images are out there, points out Bates, “it’s already too late”. • Do you have an opinion on the issues raised in this article? If you would like to submit a response of up to 300 words by email to be considered for publication in our letters section, please click here."
    },
    {
        "id": "da8bcdd7-588a-478e-99b5-716434d3237c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/30/taylor-swift-ai-deepfake-nonconsensual-sexual-images-bill",
        "title": "Taylor Swift AI images prompt US bill to tackle nonconsensual, sexual deepfakes",
        "author": "",
        "published_date": "2024-01-31T01:58:53+00:00",
        "content": "A bipartisan group of US senators introduced a bill on Tuesday that would criminalize the spread of nonconsensual, sexualized images generated by artificial intelligence. The measure comes in direct response to the proliferation of pornographic AI-made images of Taylor Swift on X, formerly Twitter, in recent days.   Related: If anyone can get the US government to take deepfake porn seriously, it’s Swifties | Arwa Mahdawi   The measure would allow victims depicted in nude or sexually explicit “digital forgeries” to seek a civil penalty against “individuals who produced or possessed the forgery with intent to distribute it” or anyone who received the material knowing it was not made with consent. Dick Durbin, the US Senate majority whip, and senators Lindsey Graham, Amy Klobuchar and Josh Hawley are behind the bill, known as the Disrupt Explicit Forged Images and Non-Consensual Edits Act of 2024, or the “Defiance Act.” “This month, fake, sexually-explicit images of Taylor Swift that were generated by artificial intelligence swept across social media platforms. Although the imagery may be fake, the harm to the victims from the distribution of sexually-explicit ‘deepfakes’ is very real,” Durbin said in a press release. Sexualized, exaggerated images of Swift at football games went viral over the weekend on X, racking tens of millions of views, according to Twitter’s metrics. AI-made images or videos of real people, commonly known as “deepfakes”, have become increasingly common on the internet as the barrier to entry for creating them lowers, a product of the boom in artificial intelligence of recent years. Some remove a person’s clothes or superimpose a person’s face over another’s to depict them saying something they did not; some are entirely generated by AI. The images of Swift were probably made using Microsoft Designer and shared first on Telegram, 404 Media reported. Microsoft introduced changes to close the technical loophole that allowed for the creation of the images. “Nobody – neither celebrities nor ordinary Americans – should ever have to find themselves featured in AI pornography. Innocent people have a right to defend their reputations and hold perpetrators accountable in court. This bill will make that a reality,” said Hawley. Swifties, as the artist’s fans are known, began flooding X with tweets of the phrase “Taylor Swift AI” accompanied by clips of her performing to stymie searches for the images. Later, as bad press mounted, Elon Musk’s X took the drastic step of prohibiting all searches for Swift to contain the spread of the images. Musk laid off the majority of the employees responsible for curbing his social network’s worst impulses after he purchased the company for $44bn."
    },
    {
        "id": "f743d2d4-a059-401a-a824-a16d09048b03",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/30/alphabet-earning-report-advertising-layoffs",
        "title": "Advertising slump sinks Google investor confidence despite overall high revenue",
        "author": "",
        "published_date": "2024-01-30T23:55:13+00:00",
        "content": "Alphabet stock slid more than 5% in after-hours trading on Tuesday despite narrowly beating overall revenue predictions for quarter four of 2023 after the tech giant fell short in its key advertising sector. The Google parent company reported a miss on predicted advertising revenue at $65.52bn compared with $65.8bn, but beat predictions for overall revenue at $86.31bn compared with $85.36bn – up 13% year over year. Referencing the overall revenue beat, Alphabet’s chief financial officer called the results “very strong”. “We remain committed to our work to durably re-engineer our cost base as we invest to support our growth opportunities,” she said. The lukewarm response to the report comes after the Google parent company laid off 1,000 employees in January, according to the Alphabet Workers Union. The chief executive, Sundar Pichai, said later in the month that more layoffs could be expected in 2024 as the company refocuses on “investing in our big priorities”, most notably the artificial intelligence elements infusing Google’s flagship products. Investors have been heartened by layoffs in recent months, analysts say, potentially seeing the downsizing as reflective of savvy cost-cutting amid rising interest rates. Layoffs have not been without impact, however, with Porat stating on an earnings call Tuesday that severance expenses are anticipated to be $700m in first quarter of 2024. Alphabet recorded severance-related charges of $2.1bn for 2023 and $1.8bn in charges related to vacating office locations.   Related: Google boss warns staff to expect further job cuts this year   Despite the overall advertising slump, Alphabet said that YouTube advertising revenue was $9.2bn compared with approximately $8bn the same time in 2022, beating analyst predictions of $9.16bn. Pichai said in a statement accompanying the earnings that Alphabet was “pleased” with the “growing contribution from YouTube”. He also stated that the company’s digital subscription services, including YouTube and the cloud storage service Google One, reached $15bn for the year. “The substantial increase in our subscription revenues over the past few years demonstrates the ability of our teams to deliver high-value add offerings, and provides a strong base on which to build,” he said. Like many companies in the tech sphere, Alphabet has scrambled to capitalize on the boom in AI sparked by the popularity of ChatGPT, a tool offered by the Microsoft-backed firm OpenAI. The word “AI” was mentioned upwards of 70 times in Tuesday’s earnings call. Pichai highlighted the company’s plans to integrate its new AI model Gemini across a number of products, including search, advertising and cloud. Alphabet’s focus on AI comes as the company seeks to diversify revenue streams, with its core search advertising business stagnating and the company facing growing threats in the form of antitrust lawsuits. The US justice department has sued Google for allegedly monopolizing digital advertising technologies. In January, a judge ruled that the company would be forced to stand trial over charges brought by multiple states alleging advertising market dominance. Last year, the company faced another antitrust trial that focused on its deals with other technology companies, including paying Apple some $18bn per year to remain Safari’s default search engine. “The threat of antitrust regulation looms on the horizon, and as the death knell sounds for third-party cookies, Google must brace itself for what could be its most challenging year yet,” said the Insider Intelligence senior analyst Evelyn Mitchell-Wolf."
    },
    {
        "id": "6a28ad9e-7f61-476c-b497-dac423975b32",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/30/elon-musk-tesla-pay-package-too-much-judge-rules",
        "title": "Elon Musk’s $56bn Tesla pay package is too much, judge rules",
        "author": "",
        "published_date": "2024-01-30T23:06:03+00:00",
        "content": "A Delaware judge has ruled in favor of the Tesla investors who challenged chief executive Elon Musk’s $56bn pay package as excessive. The judge found that Musk’s compensation was inappropriately set by the electric-vehicle maker’s board and struck down the package. If the decision survives any potential appeal, the Tesla board will have to come up with a new compensation package for Musk.   Related: Three cheers for the Delaware judge who stood up to Elon Musk   “Never incorporate your company in the state of Delaware,” Musk responded on Twitter/X. Tesla shareholder Richard Tornetta filed the lawsuit five years ago, accusing Musk of improperly dictating negotiations around the compensation package, and claimed that the board acted without independence. The court’s opinion directed Tornetta to work with Musk’s legal team on an order implementing the judge’s decision. The ruling can be appealed to the Delaware supreme court. Tesla’s agreement with Musk is by far the largest compensation deal ever for an executive and it contributes a significant part of his fortune, which is one of the world’s largest. Musk testified during the compensation trial in November 2022 that the money would be used to finance interplanetary travel. “It’s a way to get humanity to Mars,” he testified. “So Tesla can assist in potentially achieving that.“ Tesla directors argued during a weeklong trial that the company was paying to ensure that one of the world’s most dynamic entrepreneurs continued to dedicate his attention to the electric-vehicle maker. Antonio Gracias, a Tesla director from 2007 to 2021, called the package “a great deal for shareholders” because he said it led to the company’s extraordinary success. The judge said the defense was unable to establish that the “historically unprecedented compensation plan” was necessary to ensure Musk remained dedicated to Tesla. She directed parties to confer on a final form of order to implement her decision. “Swept up by the rhetoric of ‘all upside’, or perhaps starry-eyed by Musk’s superstar appeal, the board never asked the $55.8bn question: was the plan even necessary for Tesla to retain Musk and achieve its goals?” Judge Kathaleen St J McCormick wrote in her decision. Tornetta’s lawyers argued the Tesla board never told shareholders that the goals were easier to achieve than the company was acknowledging and that internal projections showed Musk was quickly going to qualify for large portions of the pay package.   Related: Elon Musk says Neuralink has implanted its first brain chip in human   The plaintiff’s legal team also argued the board had a duty to offer a smaller pay package or look for another chief executive and that they should have required Musk to work full-time at Tesla instead of allowing him to focus on other projects. In 2022, he bought the social media company Twitter, which he renamed X, and he has founded several startups, including the brain implant company Neuralink, tunneling enterprise Boring and SpaceX, a rocket venture. The package grants stock option awards allowing Musk to buy Tesla stock at heavily discounted prices as escalating financial and operational goals are met. He must hold the acquired stock for five years. Musk qualified for all 12 tranches or performance targets in the plan. He was not guaranteed any salary. The ruling will put the spotlight on Tesla’s next round of compensation negotiations with Musk. Tesla’s value ballooned to briefly top $1tn in 2021 from $50bn when the package was negotiated. The ruling also comes after Musk reiterated his desire for 25% voting control of Tesla. Musk sold a big portion of his Tesla shares in order to buy Twitter but said in a post on X in January that he was uncomfortable leading Tesla unless he had 25% of the voting control. The billionaire owned about 13% of the company at the time. Amit Batish at Equilar, an executive pay research firm, estimated in 2022 that Musk’s package was around six times larger than the combined pay of the 200 highest-paid executives in 2021. The structure of the 10-year pay package, agreed to in 2018, entitled Musk to 12 tranches of stock options if Tesla met certain performance targets, which it has. In July, Tesla’s directors agreed to return $735m to the company to settle shareholder allegations brought in a separate lawsuit filed in 2020 that they overpaid themselves. The lawsuit challenged options that were granted to directors starting in June 2017."
    },
    {
        "id": "10efa468-af76-4674-82fa-fb9407623bbc",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/30/microsoft-revenue-activision-ai-surpassed-earnings",
        "title": "Microsoft’s Activision acquisition and bets on AI yield high quarterly revenue",
        "author": "",
        "published_date": "2024-01-30T22:51:48+00:00",
        "content": "Microsoft beat analyst expectations on Tuesday as its heavy bets on artificial intelligence bore fruit, particularly for its Azure cloud computing unit.   Related: Advertising slump sinks Google investor confidence despite overall high revenue   The software giant reported revenue of $62bn, up 18% year-over-year, surpassing anticipated earnings of $61.1bn. Its year-over-year net income rose 33% to $21.9bn. CEO Satya Nadella said: “We’ve moved from talking about AI to applying AI at scale. By infusing AI across every layer of our tech stack, we’re winning new customers and helping drive new benefits and productivity gains across every sector.” Microsoft Cloud revenue rose 24% year-over-year. Revenue from its Xbox content and services division increased 61% driven by the Activision Blizzard acquisition, according to the earnings report. Activision boosted the company’s overall revenue by 4%. Microsoft, which recently overtook Apple as the world’s most valuable company, last week became only the second business in history to reach a stock market valuation of $3tn. Microsoft is seen as a leading player in the realm of AI, both via its own efforts and its close ties to ChatGPT maker OpenAI, of which it is the largest shareholder. In November, Microsoft CEO Satya Nadella played a key role in the return of Sam Altman as CEO of OpenAI after Altman’s stunning ouster. Microsoft holds an observer seat on OpenAI’s board. “The company’s recent financial performance, marked by a remarkable 18% revenue surge in today’s earnings release, signals a potent blend of innovation and strategic foresight,” said Insider Intelligence/eMarketer senior director of briefings Jeremy Goldman. “While peers like Alphabet and Meta navigate the AI landscape, Microsoft is firmly establishing itself as a frontrunner in the AI race.” Microsoft’s influence on the development of AI has grown so quickly as to attract scrutiny from regulators and those outside the tech industry. Investors shrugged off concerns over the growing headwinds as shares rose 10% over the past month. The US Federal Trade Commission announced last week that it had begun an investigation into the company’s $10bn investment in OpenAI, as well as deals made by Google, Amazon and AI startup Anthropic. The United Kingdom’s Competition and Markets Authority is likewise examining the deal. European Union regulators have said they may open similar investigations. The New York Times sued OpenAI and Microsoft in early December, alleging copyright infringement by ChatGPT. This quarter also marks the first time Microsoft reported earnings with Activision Blizzard, the A-list gaming studio behind hits like Call of Duty and World of Warcraft, under its roof. In October, Microsoft completed its acquisition of the video game maker for $69bn following a lengthy back-and-forth with regulators. Citing redundancies within the two companies, Microsoft laid off 1,900 employees last week across its gaming division, including Activision employees as well as some working on the Xbox console."
    },
    {
        "id": "acd41421-02c8-480f-8a90-60dd3d6cea8d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/30/apple-vision-pro-reviews-roundup-stunning-potential-with-big-trade-offs",
        "title": "Apple Vision Pro reviews roundup: stunning potential with big trade-offs",
        "author": "",
        "published_date": "2024-01-30T16:12:50+00:00",
        "content": "The first reviews of Apple’s Vision Pro headset, from publications with early access to the company’s attempt to create the next computing platform, talk of a big leap forward for face-mounted computers, for better or worse. The US-only headset, first announced in June last year, aims to move “spatial computing” beyond the limited mixed-reality offered by rivals from Meta, Microsoft and others. It is packed with cutting-edge technology including 3D cameras on the front to capture videos, the ability to blend the real and virtual worlds with hand and eye tracking, plus a display on the front that shows a simulacrum of the wearer’s eyes. But at a cost of $3,499 (about £2,760) in the US it has a lot of work to do to convince consumers and developers alike that it can be anything other than a super-expensive niche toy for tech enthusiasts. The Verge’s Nilay Patel called the Vision Pro an “astounding product” but one with a lot of big trade-offs, including messing up your hair each time you put it on: “Apple is very proud of the displays inside the Vision Pro, and for good reason – they represent a huge leap forward in display technology,” he wrote. “They also look generally incredible – sharp enough to read text on without even thinking about it, bright enough to do justice to movies. Apple calibrates them for colour at the factory so they are also vibrant and colour-accurate without looking oversaturated or blown out. They are so small, but they work so well that they seem huge.” But he said that while the passthrough view of the real world is the very best yet, “the field of view isn’t huge, and the essential nature of looking at tiny displays through lenses makes that field of view feel even smaller. Apple won’t tell me the exact number, but the Vision Pro’s field of view is certainly smaller than the Quest 3’s 110 horizontal degrees. That means there are fairly large black borders around what you’re seeing, a bit like you’re looking through binoculars.” CNBC’s Todd Haselton was impressed by the hardware and experience: “I loved watching movies with the headset. I lounged on my couch and put up a huge screen across the wall of my living room and watched an hour of Barbie and the two first episodes of Masters of the Air before the battery was at about 5%. Another night I watched Greyhound. I used the NBA app, which was updated to work on the Vision Pro, to stream four games at once, with the main game in the middle and others pinned to the sides. It’s wild,” he wrote. But he encountered the dreaded app gap problem that plagues new devices. “The Vision Pro supports more than a million iPhone and iPad apps. But you need to search for each app individually and some of them aren’t available. Netflix and Spotify haven’t been shy about not supporting the Vision Pro, though you can easily access either using the browser. Still, there are lots of others that I couldn’t find: 1Password isn’t there, which made logging into some apps a bit of a pain. You won’t find Uber, DoorDash (but there’s GrubHub!) or Amazon. None of Google’s apps are here, including YouTube TV, though it works fine in the browser. “Popular games like Diablo Immortal and Genshin Impact aren’t available. Facebook’s apps aren’t here, so no Instagram. These are just a few I noticed.” The Wall Street Journal’s Joanna Stern found the Vision Pro was actually good for getting work done, with virtual windows dotted across your view or replicating a Mac’s display, as long as you have other bits of hardware: “There is a built-in virtual keyboard so you can type in thin air. But it will drive you mad for anything longer than a short message. And selecting smaller buttons with a pinch should be a carnival game. I started getting real work done once I paired the Vision Pro with a Bluetooth keyboard and mouse. “I levelled up again by connecting my MacBook Pro, which let me write this review on a giant virtual monitor hovering over my desk.” But was it heavy? “Oh yes. The metal and glass build gives it a premium look and a weighty feel. One included band has a top strap to lessen the face crush; the other wraps around your head like a comfy tube sock. The Light Seal (not a breed of sea mammal) acts as a cushion and light blocker. Mine is now covered with makeup.” Weight is one of the reasons the headset has an external battery pack tethered to it by a cable, but even then Patel found the magnesium, carbon fibre and aluminium headset a daunting prospect: “You’re supposed to wear this thing on your face for long stretches of computer time, and depending on which band and light seal you use, the headset alone weighs between 600 and 650 grams. I keep joking that the Vision Pro is an iPad for your face, but it’s heavier than an 11-inch iPad Pro (470 grams) and pushing close to a 12.9-inch iPad Pro (682 grams), so in a very real way, it’s an iPad for your face.” Mark Spoonauer, of Tom’s Guide, found the battery pack to be one of the most annoying things: “The aluminum battery on the Vision Pro always needs to be with you to use the headset, which is not great. So you’ll need to put it in your pocket while standing or next to you while sitting. It’s 12.4 ounces, which is heavier than an iPhone 15 Pro Max (7.8 ounces). And while the cord is long enough most of the time, if you’re really immersed in a game or other experience and you move suddenly, you could accidentally tug the battery off a table or couch on to the floor.” He added: “I’m also not a fan of how the cord to the battery can sometimes get a bit tangled. A couple of times I had to disconnect the cable so it would straighten out.” Cnet’s Scott Stein said the Vision Pro was “a stunning look at the future”, including ones envisioned in sci-fi: “Apple is also expecting the Vision Pro to be a place to see all your own life moments, reliving them like a scene out of Minority Report. I tried watching recorded ‘spatial videos’, which are 3D videos that I previously shot on an iPhone 15 Pro. I also played back 3D videos and photos recorded directly with Vision Pro, which has its own cameras. “The photo and video quality in 3D isn’t as good as Apple’s stunning 180-degree immersive video format, not even close. But when expanded to a fuzzy-bordered viewing mode, it starts to feel like a replayed memory. I went back to the holidays, to the Museum of Natural History, to a hotpot dinner with a friend in San Francisco, to a snowball fight with my kid. I started to forget where I was and imagined I could just walk into the moment.” Stein said the Vision Pro was the best wearable display he had ever put on, but still a tough sell in its current form. “At its price, and with so few VisionOS apps at launch, the Vision Pro isn’t a device I’d recommend to any of my friends or family. If you’re in the immersive industry and can afford one then that’s another story. But for anyone else, I’d recommend you get a free demo at an Apple Store, marvel at its features and wait and see.”"
    },
    {
        "id": "26031c31-a0be-4971-b75f-ae84387e6593",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/30/hot-garbage-or-core-protection-inside-apples-ongoing-beef-with-the-eu",
        "title": "TechScape: ‘Hot garbage’ or core protection? Inside Apple’s ongoing beef with the EU",
        "author": "",
        "published_date": "2024-01-30T11:35:33+00:00",
        "content": "Who owns the phone in your pocket? That’s the question at the heart of Apple’s latest battle with European regulators, and it doesn’t look as if it’s going to be settled any time soon. On Thursday, the company published its plan for how to comply with the European Union’s Digital Markets Act (DMA), a mammoth piece of legislation that seeks to break up the power of so-called “gatekeepers”: the massive (almost entirely American) technology companies whose stature warps whole industries. From our story:  Under the changes, the US tech company will also give iPhone users a range of browsers to choose from as their default, allow the use of alternative payment systems to Apple Pay, and permit the installation of alternatives to its App Store, which could theoretically include the Google Play store. But there is a catch: for the first time, developers who take advantage of the option will be charged a flat fee per installation, overturning free-to-play business models and limiting the sorts of apps that can bypass the store.  The six companies named as gatekeepers – the American big five of Apple, Google, Facebook, Microsoft and Amazon, plus ByteDance – have a raft of requirements foisted on them in the areas they are considered to be dominant. Varied though they are, covering everything from TikTok’s power in social networking to Google’s monopoly in search engines, the goal is the same: to ensure that control of the services doesn’t lead to control of the world we build on top of them. But giving up control is hard. Apple has long argued that its strict control of the iOS platform and the App Store is fundamentally paternalistic, rather than authoritarian. It says that a world in which iPhones worked like Macs would be one with more scams, viruses and consumer harms. The EU says, effectively: “We’ll take that risk.” Hence the company’s begrudging new plans. When Apple switched the iPhone line to use USB-C rather than the company’s own Lightning ports, it was effusive about the benefits the change would bring – and silent about the fact that it was mandated by the EU. Not so this time. Apple’s announcement drips with displeasure at being forced to do something it detests:  The DMA requires changes to this system that bring greater risks to users and developers … This includes new avenues for malware, fraud and scams, illicit and harmful content, and other privacy and security threats. These changes also compromise Apple’s ability to detect, prevent, and take action against malicious apps on iOS and to support users impacted by issues with apps downloaded outside of the App Store.  A whole new world So what has changed? A whole lot. Some of the new rules announced last week apply globally, and some in the EU only, which helpfully lets us guess at what things Apple is pissed off at having to change, and what things Apple is absolutely spitting furious about. In the former category, the company has relaxed its limitations on video game streaming services (such as Microsoft’s Xbox Game Pass), and will no longer require each individual game to have its own presence on the App Store. It’s also dropped requirements for apps to support its homegrown “Sign in with Apple” technology (though developers will still have to offer a “privacy-focused” login service, and cannot force people to use, for example, “sign in with Facebook”). Developers will also receive new analytics data, it said. The latter category is much bigger, much more consequential, and all exclusive to users within the EU. Top of the list is the ability, for the first time, for developers to make their apps available outside the App Store. It’s not a total free-for-all, however: Apple will require apps to be installed from a comparable marketplace, which can only be run by big companies (who can prove they have access to €1m of credit), and must be (as best it can enforce it) “real” marketplaces. In other words, Facebook can run an app store, but not if it only has apps made by Facebook on it. Apple will still maintain a modicum of control over those app stores, vetting the apps that go on them for security concerns, but explicitly not applying the company’s content policies. That’s as the EU wants it, but does mean the burden of complying with the vast array of other European regulations will fall on the third-party marketplaces that spring up. Equally important is the ability of developers to build their own web browsers and contactless payment systems. Apple has long prevented others from doing that (third-party browsers on iOS, such as Chrome and Firefox, are just wrappers for Apple’s own “WebKit” rendering engine) for security reasons, and now we get to see whether the trade-off was worth it. Money talks But let’s not get too stuck on points of principle. The real reason this bevy of announcements is so consequential is money. And that’s where Apple threw a grenade at the status quo. EU developers will be offered two choices. They can either continue as though nothing has changed, or they can take the “alternative terms”. Those see the cut they pay Apple for downloads from the App Store reduced from its current 15-30% down to 10-17% (depending on the business they’re in), with a 3% surcharge to use Apple’s own payment system. Payments from alternative app stores garner no commission at all (although said stores may charge their own fees). But these savings don’t come for free. Developers who sign up for the new terms will also need to pay a “core technology fee”, an (effectively) annual €0.50 per user. And that’s where the question of who owns your iPhone comes in. For the first time, Apple is making explicit an argument it has danced around for a long time: that even though you have bought an iPhone from it, it still deserves to be paid for you to use it. The core technology fee, the company says, “reflects the many ways Apple creates value for developers’ businesses”. (Apple already charges developers $99 a year to build for the platform.) The fee has sparked outrage from some of the companies that were most hoping for the change, including Epic Games (who called the new terms “hot garbage”) and Spotify (who called it “extortion, plain and simple”). It certainly upends the economics of app development, rendering free-to-play development extremely risky, and almost certainly increasing the payment many developers would have to make to Apple. Spotify, for instance, doesn’t allow users to subscribe through the App Store, and so currently pays nothing to the company. That would increase to tens of millions of euros a year if it accepted the alternative terms. And while most apps get their first million users for free, thanks to the beneficence of Apple, alternative app stores have to pay the fee from user one – which means they’ll be economically compelled to impose their own charging structure to avoid haemorrhaging cash. Where does this leave indie developers? What follows will almost certainly be an extended period of haggling. Big European tech firms will want the fee to be cut or eliminated altogether. Apple, meanwhile, will want to give no further ground, but more importantly, will want to avoid the appearance of the European experiment being seen to be a success. If few alternative app stores launch, and most developers stick on the pre-existing terms, it will strengthen its claim that other jurisdictions don’t need to follow the EU’s lead. That, more than anything, likely explains the dogged attempt to follow the letter rather than the spirit of the regulations. Both sides of big tech, then, think that the new terms will have little difference. But I’m not so sure they’re right. The tech I want to see flourishing isn’t megascale streaming services who need to trim pennies from every monthly user, any more than it is monolithic platforms whose desire for control warps the culture at large. Instead, I’m eager to see a world in which indie developers, artists and creators can coexist with the massive players. Alternative marketplaces, low commissions and flat fees don’t help Spotify in its battle with Apple Music, but could make for a very different landscape for those developers who just need to find a niche within a niche to make their home."
    },
    {
        "id": "dc5527cf-f47a-4aff-bcd1-9b718a376d86",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/30/australian-sam-lee-charged-with-conspiracy-to-commit-in-us-for-role-in-ponzi-scheme",
        "title": "Australian Sam Lee charged with conspiracy to commit fraud in US for role in $1.89bn ‘Ponzi scheme’",
        "author": "",
        "published_date": "2024-01-30T04:48:20+00:00",
        "content": "The Australian blockchain entrepreneur Sam Lee has been charged with conspiracy to commit fraud in the US for his alleged role in operating the HyperVerse crypto investment schemes, described in court documents as a “pyramid and Ponzi scheme” alleged to have defrauded investors of US$1.89bn (A$2.86bn). The US attorney for the district of Maryland, Erek L Barron said the Department of Justice would “hold perpetrators accountable for these and other fraud schemes”, with Lee facing up to five years in jail if convicted. “The level of alleged fraud here is staggering,” Barron said. The criminal complaint lodged in the district court of Maryland charges Lee with conspiracy to commit securities and wire fraud, while a separate civil complaint brought by the US Securities Exchange Commission charges Lee with fraud and with the unregistered sale of securities in breach of the US Securities Act. The charges against Lee, once dubbed the “crown prince of bitcoin” in Australia, come alongside the charge and arrest of another US promoter of the HyperVerse and HyperFund crypto schemes, Brenda Chunga, who has pleaded guilty to conspiracy to commit securities and wire fraud for her role. The criminal complaint alleges that Lee and his co-conspirators operated the Hyper schemes to “unjustly enrich themselves” by inducing investors into the scheme.   Related: HyperVerse crypto scheme targeted developing countries before collapse left some investors ‘suicidal’   “It was further part of the conspiracy that Lee and his co conspirators knowingly devised and intended to devise a scheme and artifice to defraud and to obtain money and property by means of false and fraudulent pretences, representations and promises,” the complaint alleges. It orders that if found guilty, Lee is to “disgorge all ill-gotten gains” received directly or indirectly as a result of the schemes. The court documents refer to crypto schemes run by the HyperTech group collectively as HyperFund but the funds operated under various names, including HyperCapital, HyperFund, HyperVerse and HyperNation. Lee was chairman of the HyperTech group, which was co-founded with his business partner, Ryan Xu. Xu is not named in the court documents. The charges come after a Guardian Australia investigation revealed details of the scheme’s operation, including widespread investor losses, the use of a fake chief executive officer for the launch of the HyperVerse scheme and HyperVerse’s links to the collapsed Australian crypto company Blockchain Global, of which Lee was a director. The SEC complaint, which is lodged against both Lee and Chunga, alleges that its case “involves a global, crypto asset-related, multi-level marketing pyramid and Ponzi scheme that raised over $1.7 billion from victims worldwide, including millions from U.S. investors”. The criminal complaint estimates that the losses were higher, alleging that HyperFund was a “global securities fraud and wire fraud scheme that obtained approximately US$1.89 billion from victim-investors world-wide”. The SEC’s division of enforcement director, Gurbir S Grewal, alleges that Lee and Chunga attracted investors with the allure of profits from crypto asset mining – “but the only thing that HyperFund mined was its investors’ pockets”. “This case illustrates yet again how noncompliance in the crypto space facilitates schemes where promoters capitalize on the promise of easy money, without providing the detailed investor protection disclosures required by the registration provisions of the federal securities laws.” The SEC complaint outlines Lee’s alleged role in operating the HyperFund schemes, claiming that he was “centrally involved with HyperFund throughout its lifecycle”. “On information and belief, Lee was not only a co-founder of HyperFund, but he maintained control over HyperFund throughout its existence,” the SEC alleges. It alleges that from approximately June 2020 to approximately November 2022, Lee and Chunga sold memberships in HyperFund and “made materially false and misleading statements about the investments and knowingly or recklessly engaged in a scheme to defraud investors – bilking the investors out of over $1.7 billion – by enticing them with the false promise of guaranteed, high returns from investments in securities. “Defendants knew, or were reckless in not knowing, that their statements about HyperFund’s returns and profits were materially false and misleading. Defendants further knew, or were reckless in not knowing, that they were operating a scheme to defraud investors in HyperFund’s securities.” The SEC further alleges that Lee played an “integral role in the distribution of the HyperFund memberships”, with his business background and links to Australian companies used to attract investors to the scheme. “Lee’s appearance and speech during the launch of HyperVerse was an offer of securities. Lee’s reputation and public-facing crypto asset persona deceived investors into believing that HyperFund was not a scam because it was backed by a known crypto asset entrepreneur. “Those details were used to add legitimacy and credibility to the scheme, and to convince investors that their money was not only safe, but that the money they expected to make was coming from purportedly legitimate sources.” The court documents allege that HyperFund operated as a Ponzi scheme, with funds from later investors used to pay off early investors. “HyperFund had no real source of revenue other than funds received from investors, and Defendants had no basis for the promised returns,” the documents claim. “Lee, as one of the Founders of HyperFund, knew, or was reckless in not knowing, that HyperFund did not generate revenue sufficient to be able to reasonably expect to meet its payment obligations to investors, other than income generated from the sales of new HyperFund memberships, and therefore was a Ponzi scheme.” The HyperFund rewards system, which was used to bring in new investors, was “nothing but a pyramid scheme recruiting tool”, the court documents allege. Investors into HyperFund and its subsequent iterations bought “membership” packages with the promise of daily rewards of 0.5% and a 300% return over 600 days. The SEC complaint alleges that investors were told that these “exorbitant passive returns” were derived from HyperFund’s crypto asset mining operations, “however these representations were false”.   Related: ‘I do feel bad about this’: Englishman who posed as HyperVerse CEO says sorry to investors who lost millions   Lee, who launched HyperFund in mid 2020 and appeared in the 2021 launch presentation of HyperVerse, is alleged to have allowed this false representation to continue “despite knowing that the HyperFund promoters’ representations about large scale-crypto asset mining were false”. The court documents also refer to HyperFund’s claims not to be offering an investment product through its sale of membership, applying the so-called Howey test which finds that an investment contract exists where there is the investment of money in a common enterprise with a reasonable expectation of profits to be derived from the efforts of others. The case file shows that Lee, who has lived in Dubai since 2021, is yet to appoint a defence attorney. Guardian Australia has contacted Lee for comment. He has previously denied HyperVerse was a scam, and has claimed he was only involved in the funds management and technology side of the business. The charges against Lee and Chunga are the result of a joint investigation conducted by the Department of Homeland Security’s New York-based El Dorado taskforce and the Internal Revenue Service’s criminal investigation unit. The El Dorado taskforce is the oldest anti money-laundering taskforce in the US, with more than 200 members targeting sophisticated financial crimes, including those in the cyber and cryptocurrency space. Despite Lee being an Australian citizen and Australian companies being central to the promotion of the Hyper group of investor schemes, they have escaped the scrutiny of Australia’s corporate watchdog, the Australian Securities and Investments Commission. Since Guardian Australia’s investigation into the HyperVerse schemes, Asic has said it will assess the liquidator’s report into Blockchain Global, which collapsed in 2021 owing creditors A$58m."
    },
    {
        "id": "5a0d52e7-609d-4e2d-a1a7-f5c67a6b8bdb",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/29/amazon-deal-irobot-eu-veto-roomba",
        "title": "Amazon drops $1.4bn deal to buy iRobot after EU veto reports",
        "author": "",
        "published_date": "2024-01-29T14:47:59+00:00",
        "content": "Amazon has abandoned its planned $1.4bn (£1.1bn) acquisition of the robot vacuum cleaner company iRobot, amid EU opposition to the deal. The e-commerce company will pay a $94m break fee to iRobot, which immediately announced plans to axe 31% of its workforce – or 350 employees – and the departure of its chief executive. The Wall Street Journal reported on 18 January that the EU’s executive arm was preparing to block the acquisition of the Roomba maker and had informed Amazon of its proposed view.   Related: It was expensive and underpowered, but the Apple Macintosh still changed the world | John Naughton   Amazon and iRobot said in a joint statement the takeover had “no path to regulatory approval in the European Union, preventing Amazon and iRobot from moving forward together”. David Zapolsky, the Amazon general counsel, said: “Undue and disproportionate regulatory hurdles discourage entrepreneurs, who should be able to see acquisition as one path to success, and that hurts both consumers and competition – the very things that regulators say they’re trying to protect.” The European Commission formally raised concerns about the deal in November, saying it could restrict competition in the robot vacuum cleaner market. The commission’s concerns included Amazon reducing the visibility of rival vacuum cleaners on its retail platform. Amazon announced the deal in August 2022. The online retailer, which already owns the Alexa smart speaker and Ring doorbell, was pushing to expand its stable of smart home devices. The UK competition regulator cleared the takeover in June last year. However, the deal had also been under scrutiny by the Federal Trade Commission (FTC), the US competition watchdog, which has separately launched a sweeping lawsuit against the business. Verity Egerton-Doyle, a partner at UK law firm Linklaters, said Amazon’s refusal to offer concessions in response to the commission’s statement of objections – a formal list of its concerns – had doomed the deal. “This was the inevitable outcome after Amazon declined to offer remedies to the European Commission,” she said. Anne Witt, a professor of antitrust law at EDHEC Business School, said the abandonment of the deal was a “fascinating development” because big tech groups had been able historically to resolve EU concerns about takeovers. For instance, last year the EU approved Microsoft’s $69bn takeover of video game maker Activision Blizzard. Witt said the EU’s doubts about the deal echoed some of the concerns about Amazon in the FTC’s wider September lawsuit, which includes allegations that the company prioritises its own products over others. The Open Markets Institute, which researches the impact of corporate monopolies, said on Monday the commission should be “applauded for standing up to Amazon”. “Today’s momentous news isn’t just about robot vacuums – it’s about checking the growing power a handful of giants have over our increasingly digital lives, including the devices we use within the four walls of our homes,” said Max von Thun, the OMI’s Europe director. Colin Angle, the founder of iRobot, who stepped down as chair and chief executive after the news, said the termination of the deal was “disappointing”. He added: “iRobot now turns toward the future with a focus and commitment to continue building thoughtful robots and intelligent home innovations that make life better, and that our customers around the world love.”"
    },
    {
        "id": "c0a14b1f-4458-40dc-a9ba-052664489db6",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/25/apple-to-allow-eu-customers-to-download-apps-without-using-app-store",
        "title": "Apple to allow EU customers to download apps without using App Store",
        "author": "",
        "published_date": "2024-01-25T19:18:59+00:00",
        "content": "Apple is to allow EU customers to download apps without going through its own store, as the iPhone maker adapts to new Brussels laws. Under the changes the US tech company will also give iPhone users a range of browsers to choose from as their default, allow the use of alternative payment systems to Apple Pay, and permit the installation of alternatives to its App Store, which could theoretically include the Google Play store. But there is a catch: for the first time, developers who take advantage of the option will be charged a flat fee per installation, overturning free-to-play business models and limiting the sorts of apps that can bypass the store. Apple said it was bringing in the changes, which will work for any iPhone owner who downloads a March update to its iOS operating system, in order to comply with the EU’s Digital Markets Act, which aims to introduce competition for so-called big tech “gatekeepers”, such as Apple. Phil Schiller, the Apple executive in charge of its App Store, said: “The changes we’re announcing today comply with the Digital Markets Act’s requirements in the European Union, while helping to protect EU users from the unavoidable increased privacy and security threats this regulation brings.” In official documents, Apple made clear its displeasure at being forced into the measures. “The DMA requires changes to this system that bring greater risks to users and developers,” it said. “This includes new avenues for malware, fraud and scams, illicit and harmful content, and other privacy and security threats. These changes also compromise Apple’s ability to detect, prevent, and take action against malicious apps on iOS and to support users impacted by issues with apps downloaded outside of the App Store.” Apple’s changes could be replicated in the UK in the near future, with the digital markets, competition and consumers bill expected to give the country’s competition watchdog greater powers to regulate big tech. Apple said it would not charge a commission to developers of apps who avoided the App Store, but it would levy a “core technology fee” of €0.50 for each app installation, which would apply to apps downloaded in high volume. That fee will even apply when users install an app through Apple’s own store, if the developers make the app available to iPhones through an alternative means. The fee is chargeable annually, since it triggers on updates as well as first installs, although reinstallations and updates within 12 months will be exempt. Apps will get their first 1m such “first annual installs” for free, which Apple says limits the fee to less than 1% of developers. The model will be welcomed by some services, such as digital media platforms, which could expect to rapidly recoup the per-user fee in smaller commissions on card payments. However, it is likely to be economically challenging for other sectors including free-to-play gaming that have a small core of users who pay money and many more who do not. That sector includes Epic Games, a persistent critic of Apple’s stance on its App Store, whose flagship title, Fortnite, is free to play. It is not the first time a large platform holder has proposed instituting a per-install fee. In September 2023, the gaming platform Unity announced a similar fee, charging game apps that used its technology $0.20 per install after their first 200,000 installations. The resulting backlash eventually led to the departure of the company’s chief executive and a significant overhaul of the fee scheme. As part of the announcement, and in an effort to foster loyalty to the App Store, developers who accept the new business terms will see the cut they pay Apple reduced from 15-30% to 10-20%, depending on what they sell and how they process payments. Nonetheless, Tim Sweeney, the founder of Epic Games, described the changes as “hot garbage”. In a post on X, Sweeney said they were “a devious new instance of Malicious Compliance. They are forcing developers to choose between App Store exclusivity and the store terms, which will be illegal under DMA, or accept a new also-illegal anti-competitive scheme rife with new Junk Fees on downloads and new Apple taxes on payments they don’t process.” Rick VanMeter – executive director of the Coalition for App Fairness, a lobby group of mid-sized app developers including Epic and Spotify – said: “This plan does not achieve the DMA’s goal to increase competition and fairness in the digital market – it is not fair, reasonable, nor non-discriminatory … Apple’s ‘plan’ is a shameless insult to the European Commission and the millions of European consumers they represent – it must not stand and should be rejected by the commission.”"
    },
    {
        "id": "b89e381d-7589-4034-9a2e-b6cefd512e82",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/25/instagram-to-scan-under-19s-messages-to-protect-against-inappropriate-images",
        "title": "Instagram to scan under-18s’ messages to protect against ‘inappropriate images’",
        "author": "",
        "published_date": "2024-01-25T18:06:46+00:00",
        "content": "Instagram will begin scanning messages sent to and from under-18s to protect them from “inappropriate images”, Meta has announced. The feature, being kept under wraps until later this year, would work even on encrypted messages, a spokesperson said, suggesting the company intends to implement a so-called client-side scanning service for the first time. But the update will not meet controversial demands for inappropriate messages to be reported back to Instagram servers. Instead, only a user’s personal device will ever know whether or not a message has been filtered out, leading to criticism of the promise as another example of the company “grading its own homework”.   Related: Meta has not done enough to safeguard children, whistleblower says   “We’re planning to launch a new feature designed to help protect teens from seeing unwanted and potentially inappropriate images in their messages from people they’re already connected to,” the company said in a blogpost, “and to discourage them from sending these types of images themselves. We’ll have more to share on this feature, which will also work in encrypted chats, later this year.” It is the latest in a series of changes Meta has proposed to respond to criticism that plans to encrypt direct messages on Facebook Messenger and Instagram could place children and young people at risk. The broad description of the intended feature is similar to a setting called “communication safety” introduced by Apple in 2023, which detects nude photos and videos sent to children’s devices and automatically blurs them, but offers the child the option of viewing them or contacting a trusted adult. The plans stop short of the stronger versions of client-side scanning that children’s safety groups have called for, which would more aggressively report such inappropriate messages to the service’s moderators, enabling repeat offenders to be tracked and caught. The move came as Meta faces a lawsuit in New Mexico accusing it of failing to protect children on its platforms. According to an unsealed legal filing related to the case last week, Meta estimates about 100,000 children using Facebook and Instagram receive online sexual harassment each day. On Wednesday, Mark Zuckerberg, Meta’s chief executive, will appear in front of the US Congress with a number of other social media bosses for a hearing about child safety. Alongside the promise of future scanning tools, Instagram announced a small set of immediate updates to teenager safety features on the platform. Under-19s will now default to privacy settings that prevent anyone they do not follow from sending them direct messages. Previously, the restriction had only applied to adults messaging teens. Other new features are launching for parents using the service’s “supervision” tools, which lets them connect their Instagram accounts to their children’s to set time limits, monitor their teens’ blocks, and be notified when their settings are changed. Now, parents will be prompted to actively approve or deny attempts by children under 16 to loosen safety settings. “As with all our parental supervision tools, this new feature is intended to help facilitate offline conversations between parents and their teens, as they navigate their online lives together and decide what’s best for them and their family,” Meta said. Arturo Béjar, a former senior engineer and consultant at Meta, said the changes needed to be accompanied by regular updates on how many unwanted advances teenagers had received on Instagram. Without such data there would be no way of gauging the impact of safety updates. According to Béjar’s own research on Instagram users in 2021, one in eight children aged 13-15 on Instagram had received unwanted sexual advances. “This is another ‘we will grade our own homework’ promise,” he said. “Until they start quarterly reporting on unwanted advances, as experienced by teens, how are we to know they kept their promise or the impact it had? Today, after over two years of Meta knowing that each week one in eight kids get unwanted advances, there is still no way for a teen to flag or report an unwanted advance.”"
    },
    {
        "id": "cef9e772-00bb-47a5-aeec-cab2dc7733b0",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/25/microsoft-layoffs-activision-blizzard-xbox-staff",
        "title": "Microsoft lays off 1,900 Activision Blizzard and Xbox staff ",
        "author": "",
        "published_date": "2024-01-25T15:40:40+00:00",
        "content": "Microsoft is cutting about 1,900 jobs at Activision Blizzard and Xbox this week, per an internal memo from the head of the company’s gaming division.   Related: Microsoft beats Apple as most valuable company for first time in two years   The cuts represent about 9% of the overall Microsoft Gaming division, which employs roughly 22,000 people, with most of the layoffs set to happen at video game publisher Activision Blizzard. The president of the subsidiary, Mike Ybarra, is also leaving the company. Microsoft did not immediately respond to a Reuters request for comment. “I want to thank everyone who is impacted today for their meaningful contributions to their teams, to Blizzard, and to players’ lives,” Ybarra wrote on X, formerly Twitter. “To the Blizzard community: I also want to let you all know today is my last day at Blizzard.” Microsoft closed its $69bn deal for Activision Blizzard in October, boosting its heft in the video-game market with best-selling titles including Call of Duty to better compete with industry leader Sony. The news follows job cuts at several other big firms including Alphabet, eBay and Amazon, which have signaled that the widespread pruning seen across the tech industry last year will continue in 2024. Microsoft laid off 10,000 workers last year."
    },
    {
        "id": "0b0b0315-fb8a-4cee-8154-d1ec8e617e35",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/25/uk-parents-worried-about-screens-but-children-say-they-now-feel-safer-online",
        "title": "UK parents worried about screens, but children say they now feel safer online",
        "author": "",
        "published_date": "2024-01-25T06:00:13+00:00",
        "content": "UK parents are worried that screen time is taking over family life and damaging their children’s physical health, yet young people say they feel more confident online and their “digital wellbeing” has improved, according to a major survey. More than half of parents (57%) who took part in the survey said they thought screen use was having an adverse effect on their child’s sleep, while nearly two-thirds (63%) said it had a negative impact on health, up from 58% last year. Although parents were more anxious, their children provided a more positive view of their lives online, and reported feeling safer, more confident, more independent and empowered. “There’s a positive story to tell this year,” the report said. “The index scores reveal a rise in positive developmental, emotional and social experiences of children – a reversal of the downward trend observed in the previous two years.” The report is the third annual Internet Matters Index, which attempts to track the impact of digital technology on children’s physical, social, emotional, and developmental wellbeing. One of the main areas of concern the report highlights is the growing number of children being messaged online by strangers, particularly girls who are significantly more likely to experience the harms of being online. Almost half of 15- to 16-year-old girls reported being contacted by people they did not know, up from three in 10 a year earlier, while overall two-thirds of participating children (67%) reported harmful experiences online. Internet Matters, which is an internet safety organisation, asked parents to measure on a scale of 0-10 how far they agreed with the statement: “We often find ourselves spending time on our own devices rather than doing things together.” Nearly a third (31%) selected scores of between eight and 10, up from 20% – “one of the most notable shifts” over the past year. Carolyn Bunting, the co-CEO of Internet Matters, said: “Parents need to ask how families can get the balance right between the time spent online and time spent offline, and whether they are always setting the best example for their children when it comes to using phones and other tech devices.” Dame Rachel de Souza, the children’s commissioner for England, said: “Over the last few months, I have been travelling all over England as part of my Big Ambition survey asking children and young people about what they want the next government to do and one issue that came up as being a big concern was online safety. “Too many are seeing really disturbing material and feel vulnerable online, which is why I am calling on the tech companies to step up and take immediate action to keep our children safe from online harm.” Children who took part in the survey felt less affected by online harms than they did last year. A quarter (24%) saw racist, homophobic, or sexist content as really upsetting or scary, down from 35%, while one in 10 (9%) saw content promoting unrealistic body types as upsetting, down from 22%. “However,” the report notes, “this could be because experiencing harm online might be becoming normalised in the eyes of children, something they see as inevitable and part and parcel of their online lives.” Other more positive findings include two-thirds (65%) of children saying spending time online makes them feel mostly happy and 75% viewing technology and the internet as important to their independence. Almost a quarter of children, however, admitted their online activities were having a physical impact, including fatigue, concentration difficulties, vision problems and poor posture. In a separate development, the NSPCC children’s charity said young people were increasingly contacting its helpline, Childline, about AI child sexual abuse material and other harms linked to generative AI. A 15-year-old girl told the helpline a stranger had made fake nudes of her. “It looks so real, it’s my face and my room in the background. They must have taken the pictures from my Instagram and edited them. “I’m so scared they will send them to my parents. The pictures are really convincing, and I don’t think they’d believe me that they’re fake.” The NSPCC has joined forces with the US-based Common Sense Media to address the impact and risks of AI for children."
    },
    {
        "id": "ecb53886-4630-4e5e-b0b6-495ab4cfde84",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/24/tesla-earnings-q4-2023",
        "title": "Tesla delivers underwhelming earnings despite Cybertruck launch and high vehicle deliveries",
        "author": "",
        "published_date": "2024-01-25T00:26:56+00:00",
        "content": "Despite putting a new vehicle on the market, announcing another for 2025 and beating Wall Street’s expectations for vehicle deliveries, Tesla was not able to shake off its disappointing third quarter.  The electric vehicle manufacturer brought in $25.1bn in revenue and posted $.71 in earnings a share in the fourth quarter of 2023, missing analyst expectations of 25.76bn in revenue and $0.74 earnings a share. The company’s fourth quarter revenue increased 3% year over year from $24.3bn in 2022. The company also said it expects the growth rate of its vehicle volume to be “notably lower” in 2024 than in 2023, prompting company shares to fall in aftermarket trading.   Related: ‘We want everybody walking out’: UAW chief outlines mass strike for May 2028   “Tesla delivered another underwhelming quarter, with a notable miss on automotive gross margins standing out the most,” said Jesse Cohen, a senior analyst at Investing.com. The company had a less-than-stellar third quarter, earning $690m less than what analysts expected after a drop in vehicle deliveries. However, investors were heartened in early January when the company announced its fourth quarter delivery numbers of 484,000 vehicles, surpassing its expectations of 480,000 cars delivered. Tesla’s overall vehicle deliveries for 2023 grew 38% year-over-year despite a broader drop in demand for electric vehicles across the industry. “This was an important quarter for Tesla to show strong deliveries with clear momentum into 2024 as demand has upticked since 3Q based on all our global checks,” according to Dan Ives, managing director of investment firm Wedbush Securities. “This was a clear win for Musk and Tesla as hitting 1.8m vehicles for 2023 was a major achievement in a choppy macro for EVs.” The results from the final quarter of 2023 bookends the EV manufacturer’s shaky start to the new year. In the second largest drop in market valuation since the company went public in 2010, Tesla lost more than $94m in the first two weeks of January. Part of the dip has been attributed to price cuts on vehicles manufactured in China, rising labor costs and ongoing concerns over negative news relating to the company, according to Bloomberg. Just last week, Tesla owners around the US complained of being unable to charge their vehicles or having their batteries lose all power completely after temperatures hit frigid lows. Though it’s known that EV batteries lose their range in cold temperatures, some people in Chicago, where temperatures hit a low of -9F (-23C), said their cars would not charge at all. Tesla spent the last year cutting vehicle prices in order to undercut its EV competitors and increase demand for its vehicles. But that move has cut into its profit margins at a time when the company has raised worker pay across all of its US manufacturing plants. Tesla announced pay raises after the United Auto Workers Union made clear its intentions to organize at least one Tesla auto plant. This is the first earnings since the EV manufacturer’s take on a pickup truck first hit the roads. Musk said the “demand is off the hook” and he could see the company delivering “on the order of a quarter of a million Cybertrucks a year in North America. “It sure is a head-turner. Finally the future looks like the future.” However, Cybertruck reservation holders may have to continue to wait before they get a hold of the futuristic vehicle. “We expect the ramp of Cybertruck to be longer than other models given its manufacturing complexity,” Tesla’s shareholder deck reads. The company has also been notifying suppliers of its intention to produce another mass-market vehicle, called Redwood, starting in mid 2025. The entry level vehicle would start at $25,000, a lower price point that would better position Tesla to compete with other EV players including China’s BYD, which recently surpassed Tesla as the world’s biggest EV manufacturer. Musk said the company has made a lot of progress on its next generation mass market vehicle, though he would not go into details about the new offering. “We’re very far along,” Musk said on the earnings call. “This is an earnings call not a product announcement, there will no doubt be many questions. But we’re very excited about this. This is going to be very profound not just in the design of the vehicle itself but also in the design of the manufacturing system.” However, Musk also said Chinese automakers like BYD “are the most competitive car companies in the world” and said he doesn’t see an obvious opportunity to partner with them. “I think they will have significant success outside of China depending on what kind of tariffs or trade barriers are established,” he said. “If there are no trade barriers established, they will pretty much demolish most car companies in the world.” Musk also said that there’s a “good chance” of shipping “some number” of the company’s humanoid robot, Optimus, next year though he cautioned “it’s impossible to make a precise prediction” because it is uncharted territory."
    },
    {
        "id": "731ec0c4-13f0-4aa9-af4f-40d92fc002d4",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/24/ted-fellows-resign-bill-ackman-speaker-conference",
        "title": "Ted fellows resign from organisation after Bill Ackman named as speaker",
        "author": "",
        "published_date": "2024-01-24T21:15:13+00:00",
        "content": "The Ted organisation has been hit with resignations and criticisms after naming the controversial activist billionaire Bill Ackman, who was instrumental in forcing out Harvard’s president over antisemitism allegations, among its main speakers at this year’s conference. Four Ted fellows, led by the astronomer Lucianne Walkowicz and the filmmaker Saeed Taji Farouky, resigned from the group on Wednesday, accusing it of taking an anti-Palestinian stand and aligning itself “with enablers and supporters of genocide” in Gaza.   Related: ‘A bully’: the billionaire who led calls for Claudine Gay’s Harvard exit   “2024 main stage speaker Bill Ackman has defended Israel’s genocide and ethnic cleansing of the Palestinian people and has cynically weaponised antisemitism in his programme to purge American universities of Pro-Palestinian freedom of speech,” the pair wrote to Chris Anderson, who leads Ted, and Lily James Olds, director of the fellows programme. “We’ve become increasingly concerned about the fundamental values and moral compass of the organisation over the years, but with this year’s speaker selection, it is clear Ted has crossed a red line.” The conference will be held in Vancouver, Canada, in April, under the banner The Brave and the Brilliant”. The theme of Ackman’s talk has not been revealed but his selection was announced last week after he was accused of using his money and influence to help force Claudine Gay’s resignation as Harvard’s president following her disastrous appearance before Congress in December when she was questioned about on-campus antisemitism during the Israel-Gaza war. Ackman has taken stridently pro-Israel positions, including justifying the scale of the attacks on Gaza in which more than 25,000 Palestinians have been killed, mostly civilians, and the forced removal of about 2 million Palestinians from their homes. He has described criticism of Israel as antisemitism and called for the blacklisting from employment of American students who signed petitions denouncing the offensive in Gaza in the wake of the 7 October Hamas attack on Israel. Farouky and Walkowicz’s resignation letter noted that other speakers announced by Ted include the journalist Bari Weiss, who they describe as having “a long, sordid, and well-documented history of anti-Palestinian speech”, but that there are no Palestinians in the line-up. “We refuse for our work and identities to be exploited to promote the Ted brand while the organisation and its speakers generate income and advance their careers through dehumanising Palestinians and justifying their genocide,” the pair said. After the resignation letter was published, two other fellows – the entrepreneur Ayah Bdeir and cosmologist Renée Hlozek – also quit. Nearly 30 others added their names “in solidarity” without leaving Ted. Ackman responded to the resignations with a statement to the Guardian. “I stand unapologetically with Israel and against antisemitism and terrorism, while strongly supporting the Palestinian people. Attempts to cancel speech and eliminate the free and respectful exchange of ideas among people with differing views are driving much of the divisiveness that plagues our nation. Truth, wisdom and ultimately peace are the result of the free exchange of ideas and debate, precisely what Ted is all about. It is sad that this is not more widely understood,” he said. Farouky told the Guardian he did not regard the issue as one of freedom of speech. “Obviously, there’s a red line and and I think Ted are applying the same values that you would to, say, a debate about space exploration to a 21st-century live stream of someone who defends genocide, and it’s just failed miserably,” he said. “A generous reading is that the organisation is just terrible at making moral judgments about who they invite. I think they want to have slightly edgy or controversial speakers in the belief that they can do an interview on stage that might challenge them. But they’re just not equipped for that.” Farouky gave the example of the billionaire entrepreneur Elon Musk’s appearance at Ted in 2022, which he described as “a puff piece”. “The whole setup of the conferences just has no capacity to really interrogate ideas. It’s a celebration of the individual. It’s not a space for free debate at all,” he said. Ted has been approached for comment."
    },
    {
        "id": "3f15b323-5cb0-4458-a7f9-2ee93d3b90b8",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/24/police-warrant-amazon-ring-footage",
        "title": "Amazon Ring says US police will now need warrant to access user footage",
        "author": "",
        "published_date": "2024-01-24T21:14:55+00:00",
        "content": "Amazon Ring will now require US law enforcement to obtain a warrant to access doorbell footage from individual users. The company announced in a blog post that it would no longer allow law enforcement to request doorbell footage directly from users in the company’s social networking app, Neighbors. The move is an about-face from Ring’s long-held and controversial policy that drew the ire of civil liberties and privacy advocates. At the bottom of a blog post about new features that make it easier for Ring users to share “heartwarming or silly” videos in the Neighbors app, Amazon announced that it was doing away with its “request for assistance” (RFA) feature. Up until this announcement, public safety agencies including police were able to ask users to voluntarily share video footage from their Ring cameras rather than seeking warrants to obtain that user data from Amazon.   Related: Google promised to delete location data on abortion clinic visits. It didn’t, study says   “Public safety agencies like fire and police departments can still use the Neighbors app to share helpful safety tips, updates, and community events,” the blog post reads. “They will no longer be able to use the RFA tool to request and receive video in the app.” Civil liberties experts have long criticized Amazon’s close relationship with law enforcement and the company’s willingness to facilitate warrantless police investigations by allowing easy access to private security footage from people’s homes. In response to criticism, the company previously did away with the practice that allowed police to privately send requests for video footage to Ring users. Instead, police were only allowed to publicly post these requests on the the app. Now they can only obtain footage from Ring using a warrant. Matthew Guariglia, a senior policy analyst at the Electronic Frontier Foundation, said “this is a step in the right direction” but that much more can be done. “Now, Ring hopefully will altogether be out of the business of platforming casual and warrantless police requests for footage to its users,” Guariglia said in a statement. “Ring has been forced to make some important concessions – but we still believe the devices can enable end-to-end encryption by default and turn off default audio collection, which reports have shown collect audio from greater distances than initially assumed.” The company has previously come under fire for its broader privacy policies around access to users’ footage. In May 2023, Amazon entered into a 20-year $5.8m settlement with the Federal Trade Commission that required the company to disclose to its customers how much access it has to their data. According to the FTC filing, Amazon’s lax privacy policies allowed employees and contractors to “view, download, and transfer customers’ sensitive video data for their own purposes”. Amazon also admitted in 2022 that it had handed police video footage without the customers’ consent or a warrant 11 times that year in cases it deemed an “emergency”. “We are also still deeply skeptical about law enforcement’s and Ring’s ability to determine what is, or is not, an emergency that requires the company to hand over footage without a warrant or user consent,” Guariglia said."
    },
    {
        "id": "90b155dd-3550-418d-8c1a-8705cd3c6d16",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/24/meta-has-not-done-enough-to-safeguard-children-whistleblower-says",
        "title": "Meta has not done enough to safeguard children, whistleblower says",
        "author": "",
        "published_date": "2024-01-24T06:00:06+00:00",
        "content": "Mark Zuckerberg’s Meta has not done enough to safeguard children after Molly Russell’s death, according to a whistleblower who said the social media company already has the infrastructure in place to shield teenagers from harmful content. Arturo Béjar, a former senior engineer and consultant at the Instagram and Facebook owner, said if the company had learned its lessons from Molly’s death and subsequent inquest it would have created a safer experience for young users. According to research conducted by Béjar on Instagram users, 8.4% of 13- to 15-year-olds had seen someone harm themselves or threaten to harm themselves in the past week. “If they had learned the lessons from Molly Russell, they would create a product safe for 13-15-year-olds where in the last week one in 12 don’t see someone harm themselves, or threaten to do so. And where the vast majority of them feel supported when they do come across self-harm content,” Béjar told the Guardian. Russell, a 14-year-old girl from Harrow, north-west London, took her own life in 2017 after viewing harmful content related to suicide, self-harm, depression and anxiety on Instagram and Pinterest. In a landmark ruling in 2022, an inquest into her death found that Molly “died from an act of self-harm while suffering from depression and the negative effects of online content”. Béjar said Zuckerberg had the tools at his disposal to make Instagram, in particular, safer for teenagers but the company has chosen not to make those changes. “They either need a different chief executive or they need him to wake up tomorrow morning and say: ‘This kind of content is not allowed on the platform’, because they already have the infrastructure and the tools for that [content] to be impossible to find.” Béjar’s research at Instagram, and attempts to get the company to act on it, feature in a lawsuit brought against Meta by Raúl Torrez, the New Mexico attorney general, that claims Meta fails to protect children from sexual abuse, predatory approaches and human trafficking. Unredacted documents from the lawsuit show that Meta employees warned the company was “defending the status quo” in the wake of Molly’s death when “the status quo is clearly unacceptable to media, many impacted families and … will be unacceptable to the wider public”. Béjar’s responsibilities as an engineering director included child safety tools and helping children cope with harmful content such as bullying material. Having left the business as a senior engineer in 2015, he returned as a consultant in 2019 for a two-year period where he conducted research showing that one in eight children aged 13 to 15 on Instagram had received unwanted sexual advances, while one in five had been victims of bullying on the platform and 8% had viewed self-harm content. The former Meta employee has called on the company to set goals around reducing harmful content. “That creates the incentive structure for them to work on these things over a long period of time,” he said. Bejar has urged Meta to undertake a series of changes including: making it easier for users to flag unwanted content and state why they don’t want to see it; regularly survey users about their experiences on Meta platforms; and making it easier for users to submit reports about their experiences on Meta services. Béjar continues to monitor the Instagram platform and says harmful content – including self-harm material – remains on the app as well as clear evidence of underage users. Instagram has a minimum age limit of 13. Béjar has been meeting politicians, regulators and campaigners in the UK this week including Molly’s father, Ian Russell, whose Molly Rose Foundation facilitated his visit. Béjar testified before Congress last year detailing his experience at the company and the “awful experiences” of his teenage daughter and her friends on Instagram, including unwanted sexual advances and harassment. It would take three months for Meta to carry out an efficient crackdown on self-harm content, Béjar added. “They have all the machinery necessary to do that. What it requires is the will and the policy decision to say, for teenagers, we’re going to create a truly safe environment that we’re going to measure and report on publicly.” A Meta spokesperson said: “Every day countless people inside and outside of Meta are working on how to help keep young people safe online. Working with parents and experts, we have introduced over 30 tools and resources to support teens and their families in having safe, positive experiences online. All of this work continues.” Meta points to numerous safety initiatives including automatically setting under 16-year-olds’ accounts to private mode when they join Instagram, restricting adults from sending private messages to teenagers who don’t follow them and allowing Instagram users to report bullying, harassment and sexual activity. • In the UK and Ireland, Samaritans can be contacted on freephone 116 123, or email jo@samaritans.org or jo@samaritans.ie. In the US, you can call or text the National Suicide Prevention Lifeline on 988, chat on 988lifeline.org, or text HOME to 741741 to connect with a crisis counselor. In Australia, the crisis support service Lifeline is 13 11 14. Other international helplines can be found at befrienders.org"
    },
    {
        "id": "9255ce21-5a90-46d2-8d41-df78f6577027",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/24/ai-scam-emails-uk-cybersecurity-agency-phishing",
        "title": "AI will make scam emails look genuine, UK cybersecurity agency warns",
        "author": "",
        "published_date": "2024-01-24T00:01:03+00:00",
        "content": "Artificial intelligence will make it difficult to spot whether emails are genuine or sent by scammers and malicious actors, including messages that ask computer users to reset their passwords, the UK’s cybersecurity agency has warned. The National Cyber Security Centre (NCSC) said people would struggle to identify phishing messages – where users are tricked into handing over passwords or personal details – due to the sophistication of AI tools. Generative AI, the term for technology that can produce convincing text, voice and images from simple hand-typed prompts, has become widely available to the public through chatbots such as ChatGPT and free-to-use versions known as open source models.   Related: EU ‘in touching distance’ of world’s first laws regulating artificial intelligence   The NCSC, part of the GCHQ spy agency, said in its latest assessment of AI’s impact on the cyber threats facing the UK that AI would “almost certainly” increase the volume of cyber-attacks and heighten their impact over the next two years. It said generative AI and large language models – the technology that underpins chatbots – will complicate efforts to identify different types of attack such as spoof messages and social engineering, the term for manipulating people to hand over confidential material. “To 2025, generative AI and large language models will make it difficult for everyone, regardless of their level of cybersecurity understanding, to assess whether an email or password reset request is genuine, or to identify phishing, spoofing or social engineering attempts.” Ransomware attacks, which had hit institutions such as the British Library and Royal Mail over the past year, were also expected to increase, the NCSC said. It warned that the sophistication of AI “lowers the barrier” for amateur cybercriminals and hackers to access systems and gather information on targets, enabling them to paralyse a victim’s computer systems, extract sensitive data and demand a cryptocurrency ransom. The NCSC said generative AI tools already helped make approaches to potential victims more convincing by creating fake “lure documents” that did not contain the translation, spelling or grammatical errors that tended to give away phishing attacks – their contents having been crafted or corrected by chatbots. However, it said generative AI – which emerged as a competent coding tool – would not enhance the effectiveness of ransomware code but would help sift through and identify targets. According to the Information Commissioner’s Office, the UK’s data watchdog, 706 ransomware incidents were reported in the UK in 2022, compared with 694 in 2021. The agency warned that state actors probably have enough malware – short for malicious software – to train a specially created AI model that would create new code capable of avoiding security measures. The NCSC said such a model would have to be trained on data extracted from its target. “Highly capable state actors are almost certainly best placed among cyber threat actors to harness the potential of AI in advanced cyber operations,” the NCSC report says. The NCSC added that AI would also work as a defensive tool, with the technology able to detect attacks and design more secure systems. The report came as the UK government set out new guidelines encouraging businesses to better equip themselves to recover from ransomware attacks. The “Cyber Governance Code of Practice” aims to place information security on the same tier as financial and legal management, the NCSC said. But cybersecurity experts have called for stronger action. Ciaran Martin, the former head of the NCSC, says that unless public and private bodies fundamentally change how they approach the threat of ransomware, “an incident of the severity of the British Library attack is likely in each of the next five years.” In a newsletter, Martin wrote that the UK needs to reassess its approach to ransomware, including by creating stronger rules around the payment of ransoms and giving up on “fantasies” of “striking back” against criminals based in hostile nations."
    },
    {
        "id": "6c47f170-7415-49d8-af25-5439023e72e8",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/24/hyperverse-crypto-scheme-targeted-developing-countries-before-collapse-left-some-investors-suicidal",
        "title": "HyperVerse crypto scheme targeted developing countries before collapse left some investors ‘suicidal’",
        "author": "",
        "published_date": "2024-01-23T15:00:39+00:00",
        "content": "The HyperVerse cryptocurrency scheme targeted investors in developing countries across Asia, Africa and the Pacific before its eventual collapse left many people unable to access funds.\nIn Nepal, some people who took out bank loans to buy packages in HyperVerse said they felt suicidal when they were unable to withdraw their money, with some cases of self-harm, according to one investor.\nA UK-based promoter of HyperVerse who went on a tour of five African countries in 2022 told one radio station in Ghana that there were millions of people around the world benefiting from blockchain “without really understanding it”.\n\n Related: ‘They are so convincing’: Vera Gazzard lost her life savings to HyperVerse \n\nHyperVerse, which was linked to an earlier scheme known as HyperFund, was launched by the Australian blockchain entrepreneur Sam Lee and his business partner Ryan Xu, two of the founders of the collapsed Australian company BlockChain Global.\nA Guardian Australia investigation has revealed widespread losses to the schemes that escaped regulatory warnings in Australia, despite one overseas regulator warning they were a possible “scam” and another calling HyperVerse a “suspected pyramid scheme”.\nThe drive to expand the scheme, which encouraged existing members to earn financial rewards for bringing in new ones, appears to have resulted in its spread to previously untapped markets, including developing countries.\nIn January 2022 the Central Bank of Nepal issued a public warning that named HyperFund and some other unrelated schemes, saying people were being enticed to engage in such crypto currency products with the promise of “high returns in a short period”.\n“Given that Virtual Currency/Cryptocurrency is not legally recognized as foreign exchange or currency in Nepal, engaging in the aforementioned illegal financial tools could deceive the public and result in illegal capital flight from the country.”\n\n\nYou need to do something really fast, mate ... we’ll have people on our doorstep.\n\n\nIn a Zoom meeting between Nepali Hyper members and Lee in February 2023, the members said people were angry because they were unable to withdraw funds from the platform.\nOne member told Lee he was “sad and grumpy”, and was managing demands from people he had brought into the scheme who were unable to access funds.\n“You need to do something really fast, mate, because you are somewhere far away and maybe you’re not under pressure directly, but people like us, we are the people who live in the neighbourhoods and our relations have deteriorated, and whenever we wake up in the morning we’ll have people on our doorstep.”\n \n\n\n\nInvestors were offered “memberships” to the HyperVerse, which was a “blockchain community” that allowed members to “explore the HyperVerse ecosystem”.\nThe minimum membership amount was $300USD, which was converted to HyperUnits once invested.\nThe scheme offered minimum returns of 0.5% a day, with a 300% return over 600 days.\nMembers were incentivised to “reinvest” their earnings, with more HyperUnits offered if funds were not withdrawn once they became available.\nMembers were also paid HyperUnits to recruit new members, with referral commissions paid on a sliding scale according to how many people were recruited. Further commissions were paid based on how many people these recruits then recruited, down to 20 levels.\nHyperUnits were linked to various crypto tokens and once matured could be withdrawn and converted to other cryptocurrencies.\nWhile early investors profited and were able to make withdrawals, the scheme has left many investors unable to access their funds.\n\n\n\n\n\nA Nepali man living in the UK told Lee that some people in his home country were suicidal, having taken out bank loans to buy packages in HyperVerse, saying one person he knew had engaged in self harm.\n“It was presented in such a lucrative manner that there have been instances where people loaned money to buy packages in this company,” he said. “I know that’s wrong, we never told them to do that, the leaders never told them to do that, but … the benefits outweighed the risk therefore people took some loans from banks and they bought packages in this project.\n“I don’t want to say the name but there has been a case of self-harm back home [in Nepal]. We are receiving several SOS-es, you know, because people are in such a condition it’s better for them to just take a suicidal sort of step rather than wait for this company to come out with a plan to pay them back.”\nIn response, in the Zoom call, Lee said he hoped vulnerable people could be prioritised to get their initial investment back, but denied he was responsible.\n“I don’t want to say anything about these individual cases because, you know, I’m not in a position of knowing these cases to be empathetic, right? But, you know, we have to just realise that … a lot of people have been misled in many other industries, this is just the newest industry to be misled in,” Lee said.\n“And the way we prevent this from happening again is we need to increase everybody’s literacy around the technology and how these opportunities operate.”\nLee blamed the “corporate” team behind HyperVerse for the situation.\nHe denied he was involved in HyperVerse, despite speaking at its official launch, saying he was only involved in the funds management side through his role in the HyperTech group, of which he is chairman.\nAnother person in the February 2023 meeting challenged Lee on this claim.\n“Community leaders have always projected you as the man with the Midas touch – HyperTech, HyperVerse, HyperFund, whatever, it’s Sam Lee, it’s Sam Lee, it’s Sam Lee, that is all we have been told day in, day out,” they said.\nIn response, Lee said: “I cannot really disappear from HyperVerse if I’m never involved.”\n“Corporate has put out misleading information, which of course leadership used to drive sales, so ultimately the buck stops with corporate. But I am not 100% free from this because when things have been misinterpreted, I could have always at any time put out a press release or a statement to clarify,” he said.\n\n Related: Chief executive of collapsed crypto fund HyperVerse does not appear to exist  \n\n“But that delay in doing so was because I’ve always trusted corporate to do the right thing because they are very old partners that I’ve worked together with for many, many years. So they’ve always done very well and have done things in a manner that generated the results, but in this case have failed spectacularly the community as well as failed me.”\nIn December 2021, HyperFund members were “migrated” into HyperVerse, with a global launch event promising to build a metaverse to rival Facebook. The launch event included presentations from Lee, Xu and a chief executive introduced as Steven Reece Lewis, who, in fact, was an Englishman named Stephen Harrison contracted to act the role of a corporate “presenter”, as a Guardian Australia investigation revealed.\nHarrison has denied having any contact with either Xu or Lee, playing any role in the underlying business or benefitting from any investor losses.\n‘Financial revolution world tour’\nShortly after the HyperVerse launch, in March and April 2022, a UK man named Des Amey headed a “financial revolution world tour”, running in-person seminars in Ghana, Nigeria, Tanzania, Kenya and Zambia.\nAmey is a director of Amey Finance Academy Ltd, which in July 2022 was the subject of a UK Financial Conduct Authority warning that said: “We believe this firm may be providing financial services or products in the UK without our authorisation” and goes on to confirm “This firm is not authorised by us and is targeting people in the UK. You will not have access to the Financial Ombudsman Service or be protected by the Financial Services Compensation Scheme (FSCS), so you are unlikely to get your money back if things go wrong.”\nAmey’s website – ameyfinance.com – included a guide on how to join HyperVerse, with a “concierge” service offered to sign people up to the scheme. The website has since been taken down.\nIn July 2022 Amey was interviewed in a HyperCommunity leadership training video broadcast on Zoom, in which he was presented as a “VIP5 STAR” leader within the group.\n\nSign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup\n\nMembers achieved a star ranking based on how many people they attracted into the scheme, with VIP5 STAR the highest level.\nAccording to the presentation, Amey joined Hyper in April 2021 when the scheme was being run as HyperFund and he subsequently recruited more than 22,000 members across HyperFund and HyperVerse.\nIn the video, Amey said he was just back from a tour of 11 countries. As well as the five African countries promoted for the speaking circuit, Amey also said he had visited the Seychelles, US, Canada, Jamaica, Germany and the United Arab Emirates. This was unrelated to any promotional material presented in Nepal and there is no evidence to suggest he recruited members there.\n“There was real impact, you know the world is now alight with all things Hyper,” he said.\n“I’m now a VIP five-star node leader on this platform. This industry and this platform completely changed my life. Now I travel around the world teaching people, you know, mindset – how to become financially independent, not just the theory but giving people practical solutions like the Hyper platform,” he said.\n\n Related: A crypto firm sent a disability worker $10m by mistake. Months later she was arrested at an Australian airport \n\nVideos of the events he headlined in Ghana, which were open only to those who had bought tickets, reveal HyperVerse material being promoted.\nIn media interviews, Amey spoke in general terms about making money through blockchain technology without mentioning HyperVerse.\nIn an interview with the Ghanaian radio station Starr FM about the “largest blockchain alliance”, Amey said people did not need to understand blockchain technology to take advantage of its rewards.\n“There are millions of people around the world who are benefiting from blockchain without really understanding it,” he said.\n“And that’s because there are many companies who say, ‘OK, we will do all the legwork … we will do all of the investing, we’ll do all the technical aspects of blockchain, and we will just allow you to benefit financially from all of that’.”\nAmey also visited Nigeria in late 2021 and told the Nigerian news service Business Day about a “membership rewards program” that allowed people to buy “houses for cash”.\n“We’ve now partnered with a multi-billion dollar company and we have a rewards program where people are paid every single day, and through the compound effect they are able to grow a very small amount of money into a large amount of money in a very short timeframe,” Amey said.\n“And people now in Africa, in Asia, and in so many developing countries, are able to use this rewards program to buy houses for cash. It’s a very clear system, it works like clockwork and it does exactly what it says on the tin – our members get paid every single day.”\nAmey did not name HyperFund, and at one stage the interviewer asked: “You don’t want to mention that [company] do you?” He replied: “No, no, no.”\n\n Related: Investors lose millions as crypto schemes operate unchecked in Australia \n\nHe said Nigerians had been interested in becoming involved in the membership scheme and “we’ve had several events … and meetings.”\nAmey did not respond to questions from Guardian Australia.\nIn 2021, the New Zealand-based Kaniva Tonga newspaper reported that Tongans across the diaspora were also being urged to join the scheme. This was unrelated to Amey’s promotional activity.\nIn February 2023, a US-based Tongan promoter reportedly warned members not to put any more money into the scheme.\n“Those of you who have invested tens of thousands, thousands, and hundreds and have yet to withdraw it, Sam Lee will be here in an attempt to refund their money,” the Tongan promoter was quoted as telling her members. According to the report, she claimed that Lee wanted to obtain the names of investors who couldn’t get money out so he could help them retrieve their money.\nLee, who has previously denied the scheme was a scam, did not respond to questions from Guardian Australia.\n\nDo you know more? Contact sarah.martin@theguardian.com\nCrisis support services can be reached 24 hours a day: Lifeline 13 11 14; Suicide Call Back Service 1300 659 467; Kids Helpline 1800 55 1800; MensLine Australia 1300 78 99 78; Beyond Blue 1300 22 4636\n"
    },
    {
        "id": "b659e3fb-7127-426c-91bf-ba787103f0a4",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/23/techscape-newsletter-apple-vision-pro-headset-netflix-spotify-youtube-apps",
        "title": "TechScape: Why Apple’s Vision Pro headset won’t have Netflix, Spotify or YouTube",
        "author": "",
        "published_date": "2024-01-23T11:30:12+00:00",
        "content": "It’s good to have friends. They come to your birthday party, offer a shoulder to cry on when things are hard and spend precious corporate resources developing apps for your nascent virtual reality platform despite little direct return. It can be tempting to believe that a pile of cash worth $30bn, and a single product line that brings in more than $200bn a year, is an acceptable substitute. But Apple is learning that money can’t buy you everything. Last week, pre-orders opened for the company’s Vision Pro headset, the $3,500 “spatial computing” platform CEO Tim Cook has positioned as the successor to the Mac and iPhone and the launch of the third major era in Apple’s history. But in the press, the launch has been overshadowed by the quiet hostility towards the device from those whose support will ultimately be needed to ensure its success. From Bloomberg (£):  Rather than designing a Vision Pro app – or even just supporting its existing iPad app on the platform – Netflix is essentially taking a pass. The company, which competes with Apple in streaming, said in a statement that users interested in watching its content on the device can do so from the web.  Developing an app for a new platform is risky and expensive. In the first weekend the Vision Pro was on sale, it sold an estimated 160,000 to 180,000 units, according to analyst Ming-Chi Kuo. Even if everyone installed the new Netflix app, that would be a drop in the bucket of the company’s 250 million paid subscribers. And Netflix isn’t in the business of app installs; its business is paid subscriptions. The cost of developing an app for a new platform is only worth it if it brings new customers or keeps existing subscribers from cancelling. So it makes sense why the company might be unwilling to spend the resources on a Vision Pro app, even if Apple is keenly marketing the device as the most immersive way possible to watch TV. Netflix has similarly abandoned updating its app for the Meta Quest, Mark Zuckerberg’s money-shedding attempt to make “metaverse” a thing.  In both cases, that means users are stuck loading up the web version of Netflix if they want to watch TV in virtual reality – and so losing the ability to watch shows offline (a cinematic experience on a flight, for instance), one of the key selling points of the headsets. But not only did Netflix not invest in app development, it also actively intervened to keep its service off the headset. The Vision Pro can run iPhone and iPad apps in a “compatibility mode”, unless developers specifically opt-out. That’s precisely what Netflix did, and it’s not alone. Bloomberg, again (£):  YouTube … isn’t planning to launch a new app for the Apple Vision Pro, nor will it allow its longstanding iPad application to work on the device – at least, for now […] Spotify also isn’t currently planning a new app for visionOS – the Vision Pro’s operating system – and doesn’t expect to enable its iPad app to run on the device when it launches, according to a person familiar with matter.  Why are all these companies taking active measures to prevent Vision Pro users accessing their services? It could simply be a passion for quality: as anyone who has used an iPhone app on an iPad, or an iPad app on a Mac, has discovered, just because something made for one platform can theoretically run on another, doesn’t necessarily mean it feels good when it does. But I think there’s a hint in the other major Apple story of the last week: the company’s grudging acquiescence of a court requirement to let developers bypass Apple’s own payment systems. Developers of iPhone apps now have a choice. They can either bill customers through Apple’s system, and pay the company a 30% cut, or pay a much cheaper third party around 3% … and then pay Apple a 27% cut. From Apple:  All App Store developers – including those who place buttons or links with calls to action in their apps – benefit from Apple’s proprietary technology and tools protected by intellectual property, and access to its user base. […] Apple’s commission will be 27% on proceeds you earn from sales.  The company is clear: it is entitled to a cut of all the economic activity produced through the App Store. It doesn’t capture “all transactions that Apple has facilitated”, but charging on digital purchases is “a reasonable means to account for the substantial value Apple provides developers”, it says. The iPhone is an enormously valuable platform, which is how Apple can impose such extractive terms on anyone who wants to develop for it. But Vision Pro … isn’t. It might be, one day, but if it does, there will be plenty of time to develop for it down the line. And in the meantime, if you’re a developer being asked to spend time and money making Apple’s latest business venture succeed, you might be wondering: is it actually in your interest to help that happen? Facebook’s God complex Mark Zuckerberg wants to build a god. From our story by Dan Milmo:  The Meta chief executive has said the company will attempt to build an artificial general intelligence (AGI) system and make it open source, meaning it will be accessible to developers outside the company. The system should be made “as widely available as we responsibly can”, he added. AGI is not a strictly defined term, but it commonly refers to a theoretical AI system that can carry out an array of tasks at a level of intelligence that matches or exceeds humans. The potential emergence of AGI has alarmed experts and politicians around the world who fear such a system, or a combination of multiple AGI systems, could evade human control and threaten humanity.  Facebook’s pivot from the metaverse is complete. Don’t expect Zuck to ever fully admit that the billions spent on pursuing a dream where we all socialised in virtual reality spaces controlled by his company was wasted capital. But the writing was on the wall for this pivot ever since its October 2022 quarterly results, which showed a $4bn loss on the metaverse division in just three months, were followed almost immediately by the burst of explosion in AI generated by ChatGPT’s launch the following month. (In May the following year, for what it’s worth, he explicitly said it was “not accurate” to claim such a pivot was happening.) As with the metaverse push before it, this sort of goal is, for better or worse, one you can only have when a founder/CEO still has absolute control of the company he built. There’s no business case, really, for building god and then releasing it for free for anyone to use. Other companies would probably focus on shorter-term benefits, such as using LLMs for content moderation. But credit to Zuckerberg for recognising that the world is more interesting when billionaires spend unbelievable quantities of other people’s money on whims. If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Tuesday. • This article was amended on 23 January 2024. An earlier version incorrectly said that Netflix had not published an app for the Meta Quest. The company published an app in 2019, but has not updated it."
    },
    {
        "id": "39801506-2302-4ec3-8b2e-1b0977ff4e6c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/22/sunglass-hut-facial-recognition-wrongful-arrest-lawsuit",
        "title": "Facial recognition used after Sunglass Hut robbery led to man’s wrongful jailing, says suit",
        "author": "",
        "published_date": "2024-01-23T00:21:23+00:00",
        "content": "A 61-year-old man is suing Macy’s and the parent company of Sunglass Hut over the stores’ alleged use of a facial recognition system that misidentified him as the culprit behind an armed robbery and led to his wrongful arrest. While in jail, he was beaten and raped, according to his suit. Harvey Eugene Murphy Jr was accused and arrested on charges of robbing a Houston-area Sunglass Hut of thousands of dollars of merchandise in January 2022, though his attorneys say he was living in California at the time of the robbery. He was arrested on 20 October 2023, according to his lawyers.   Related: Rite Aid facial recognition misidentified Black, Latino and Asian people as ‘likely’ shoplifters   According to Murphy’s lawsuit, an employee of EssilorLuxottica, Sunglass Hut’s parent company, worked with its retail partner Macy’s and used facial recognition software to identify Murphy as the robber. The image that was put through the facial recognition system came from low-quality cameras, according to the lawsuit. While Houston police department was investigating the armed robbery, the EssilorLuxottica employee called police to say they could stop the investigation because the employee had identified one of two robbers with the technology. The employee also said the system had pointed to Murphy as committing two other robberies, according to the lawsuit. When Murphy returned to Texas from California, he went to the department of motor vehicles (DMV) to renew his license. Within minutes of identifying himself to a DMV clerk, Murphy told the Guardian he was approached by a police officer who notified him there was a warrant out for his arrest for an aggravated robbery. Murphy said he was not told any details about his supposed crime except for the date the robbery occurred. He realized he was in Sacramento, California, at the time of the robbery – more than a thousand miles away. “I almost thought it was a joke,” Murphy said. Still, he was arrested and taken to the local county jail, where he was held for 10 days before being transferred to and processed in Harris county jail. After a few days at Harris county, his alibi was confirmed by both his court-appointed defense attorney and the prosecutor, and the charges against him were ultimately dropped, according to the lawsuit. Murphy was never convicted of a crime. Nonetheless, he says his detainment left him with deep scars. He was brutally beaten and gang-raped by three other men in the jail hours before he was released, he alleges. They threatened to kill him if he tried to report them to the jail staff, according to Murphy. After the alleged attack, Murphy remained in the same cell as them until he was released. “That was kind of terrifying,” Murphy said. “Your anxiety is up so high, you’re still shaking the entire time. And I just got up on my bunk and just faced the wall and was just praying that something would come through and get me out of that tank.” “The attack left him with permanent injuries that he has to live with every day of his life,” the lawsuit reads. “All of this happened to Murphy because the Defendants relied on facial recognition technology that is known to be error prone and faulty.” Murphy did not realize facial recognition technology may have been used as evidence against him until two weeks ago, when he began working with his attorney, Daniel Dutko. Dutko said he discovered from police documents that the Sunglass Hut worker shared camera footage with Macy’s, which employees from the department store chain used to identify Murphy. After that, Macy’s and Sunglass Hut contacted the police together, according to Dutko. Though Macy’s has retail partnerships with the eyewear brand in several locations, Macy’s had no connection to this robbery as the Sunglass Hut in question is a standalone location, he said. “We feel very comfortable saying facial recognition software is the only possible explanation, and it’s the only reason why [Sunglass Hut] would go to Macy’s to try to identify him,” Dutko said. Murphy’s case would be the seventh known case of a wrongful arrest due to facial recognition in the US, further highlighting the flaws of a technology already widely adopted by police departments and retailers. However, in all of the publicly known cases of wrongful arrests due to facial recognition up until now, the victims have been Black. Murphy’s would be the first known case of the failure of the technology leading to the wrongful arrest of a white man. Just last month, Rite Aid settled with the Federal Trade Commission over its use of a facial recognition system that misidentified Black, Latino and Asian customers as people previously identified as “likely to engage” in shoplifting. The pharmacy chain is forbidden from using facial recognition in its stores for five years as part of the settlement. And in the summer of 2023, a woman named Porcha Woodruff was arrested on charges of car jacking due to false identification by a facial recognition system. Macy’s has previously been sued over its use of facial recognition technology. In a 2020 lawsuit, a Chicago woman accused the company of working with facial recognition provider Clearview AI without her or other customers’ consent in violation of Illinois’ biometric privacy law. Nathan Freed Wessler, deputy director of the America Civil Liberties Union’s Speech, Privacy, and Technology Project said this was another example of the “extreme dangers of face recognition technology”. “In case after case, we’ve seen police reflexively trusting unreliable face recognition results, and then allowing false matches from the technology to taint witness identification procedures,” Wessler said in a statement. “As the facts alleged in this case show, the consequences of being wrongfully arrested are horrible. Lawmakers must put a stop to police and corporations’ hazardous reliance on face recognition results to put people in jail.” Murphy is seeking $10m in damages. Macy’s said it had no comment on pending litigation and EssilorLuxottica did not immediately respond to a Guardian request for comment."
    },
    {
        "id": "1aa89b6b-2b2b-4aea-848c-51da69ba1958",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/22/james-dyson-donation-to-primary-school-approved-despite-concerns",
        "title": "James Dyson’s £6m donation to primary school approved despite concerns",
        "author": "",
        "published_date": "2024-01-22T22:01:32+00:00",
        "content": "A £6m donation from Sir James Dyson to help fund the expansion of his local state primary school has been approved by the government, despite concerns about the potential impact on neighbouring schools. The education secretary, Gillian Keegan, announced on Monday that she had given the green light for the inventor’s donation to Malmesbury Church of England primary school in Wiltshire, which is close to Dyson’s research and development campus. It will pay for a “first-class” science, technology, engineering, art and maths (Steam) centre plus additional classrooms and, subject to planning permission by Wiltshire council, will open from September 2027, providing capacity for an additional 210 pupils, taking the total to 630. It is understood the James Dyson Foundation stopped short of welcoming the education secretary’s announcement and said it is carefully examining the detail of the decision. In a letter to the Times last year, Dyson said the donation via his charitable foundation had been blocked by officials. “It’s incredibly depressing,” he told the paper. “It is a sorry example of how hard it is to get anything done in Britain.” His claim was denied by Downing Street, but there was local opposition. Laura Mayes, Wiltshire council’s cabinet member for children’s services, said there were already sufficient places in three local schools to meet demand. Reports have suggested that many of the 3,500 employees who work at Dyson’s UK headquarters in Malmesbury would like to send their children to the school, but it is currently not big enough. Keegan said: “I welcome this generous donation from the Dyson Foundation which will support cutting-edge education for local pupils, helping to develop the scientists and engineers of the future. “This new centre will provide world-class facilities for pupils in Malmesbury and the surrounding areas to inspire them and develop the skills we need to compete on the world stage. “The £6m donation builds on the work we have been doing to boost the uptake of Stem subjects in schools and through apprenticeships and further education, boosting growth and opportunity in these vital sectors.” Steve Heal, the headteacher at Malmesbury primary school, told the Times: “We are incredibly grateful to Sir James Dyson, and the James Dyson Foundation, for enthusiastically backing the project from its inception. It’s been a long journey to get to this point.” He said it was “a wonderful thing for Malmesbury. It is also a phenomenal opportunity to shape the future of learning through an innovative curriculum created with Dyson. This will open up a world of opportunity for children in Malmesbury and beyond.” After the government’s announcement, Mayes said: “We have an important responsibility to ensure all our school pupils can access opportunities that inspire them and help them achieve their potential. “Our school places strategy is our long-term plan, based on clear evidence, which maps out how we will secure sufficient school places for the future. “Our strategy shows we have sufficient places in the three local schools to cater for expected demand and we have expressed concern about the impact that adding 210 additional places at Malmesbury primary school could have on neighbouring schools. “However, we now have the government’s final decision and we will work with everyone involved to ensure we achieve the best outcomes for all our pupils.” Kenneth Baker, who was education secretary under Margaret Thatcher, had backed Dyson. In a letter to the Times, he said: “It is almost unheard of for any large industrial company to support education in this very extended and committed way. “If the government rejects the proposal, over misplaced concerns about excess school places, why should other industries and entrepreneurs be remotely interested in trying to improve technical education in schools?”"
    },
    {
        "id": "d55203c2-84e8-4b83-a16d-d6d41de46615",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/22/computers-are-not-as-reliable-as-many-of-us-believe",
        "title": "Computers are not as reliable as many of us believe | Letter",
        "author": "",
        "published_date": "2024-01-22T17:45:59+00:00",
        "content": "You published several letters arguing that computers themselves are reliable (Computers rarely go wrong, but computer systems often do, 17 January). The truth is somewhat less positive. While computer hardware is more reliable than most current software, data-corrupting flaws in hardware do exist, and more are found over time. Some, such as rowhammer, become well known as security flaws, others become famous through recalls (like the Pentium FDIV fiasco), but most pass unnoticed by the general public and are quietly fixed or mitigated in microcode. Research papers from Google (Cores That Don’t Count) and Facebook (Silent Data Corruptions at Scale), as well as a perusal of published errata documents, show that we should question very hard whether assuming a computer is reliable is a fit assumption for a criminal conviction, unless backed up by other evidence such as logs, audit trails and good evidence of internal self-checking within the software.Alan CoxSwansea • Do you have a photograph you’d like to share with Guardian readers? If so, please click here to upload it. A selection will be published in our Readers’ best photographs galleries and in the print edition on Saturdays."
    },
    {
        "id": "af27553d-2d81-4f4a-b465-5b6d9169580f",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/22/asic-examines-collapsed-bitcoin-company-blockchain-global-after-guardian-investigation",
        "title": "Asic examines collapsed bitcoin company Blockchain Global after Guardian investigation",
        "author": "",
        "published_date": "2024-01-22T15:00:02+00:00",
        "content": "Australia’s corporate regulator will examine details of the collapsed bitcoin company Blockchain Global after a Guardian Australia investigation revealed links between two of its directors and a series of failed crypto investment schemes. Blockchain Global collapsed in 2021 owing creditors $58m, with the liquidator referring its directors – Allan Guo, Sam Lee and Ryan Xu – to the Australian Securities and Investment Commission for potential breaches of the Corporations Act. This included possible breaches of director’s duties, breaches of trust and unreasonable director-related transactions.   Related: HyperVerse crypto promoter ‘Bitcoin Rodney’ arrested and charged in US    Asic initially advised liquidators Pitcher Partners that it did not intend to take action after it received the first report on the company’s activities in March 2022. But after a Guardian Australia investigation into the HyperVerse crypto investment scheme, which has links to Lee and Xu, Asic has begun an examination of the liquidator’s report. A final version was filed to Asic in October. An Asic spokesperson told Guardian Australia: “Asic confirms that it is assessing reports from the liquidator in relation to BGL.” Guardian Australia has revealed widespread losses to the HyperVerse investment scheme, which escaped regulator attention in Australia despite being flagged by authorities overseas – by one as a possible “scam” and another as a “suspected pyramid scheme”.  Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup  Court documents filed in January in the US against Bitcoin Rodney, a senior US promoter of HyperFund and HyperVerse, allege the scheme operated with a network or promoters making “fraudulent promotional presentations” to investors and potential investors. He has been charged with operating and conspiring to operate an unlicensed money transmitting business. An affidavit filed by the US internal revenue service alleges that early investors were “paid with funds collected from more recent investors”, and the company’s claimed revenue-generating bitcoin mining operations did not exist. There is no mention of Xu or Lee in the court documents filed by authorities in that case. Lee has denied being behind HyperVerse, saying his involvement was limited to technology provision and the funds management side of the organisation. Both Lee and Xu appeared in the HyperVerse global launch event in 2021, alongside a fake chief executive officer called Stephen Reece Lewis. Xu and Lee also both featured prominently in promotional material for the schemes that preceded HyperVerse, known as HyperFund and HyperCapital. Reece Lewis has since been revealed by the Guardian to be the British man Stephen Harrison, who said he was hired for a short period via a talent agent to play a “corporate presenter” and that he had no role in the actual business. Lee relocated to Dubai in 2021, while the whereabouts of Xu is unknown. Guo is not involved in the Hyper group schemes. The US-based crypto analysis firm Chainalysis estimates losses to HyperVerse in 2022 amounted to US$1.3bn (A$1.97bn). The move by Asic to examine the Blockchain Global collapse comes as the liquidator also reveals a possible link in the company’s records to the operation of HCash, which was a cryptocurrency linked to the Hyper investment schemes. “The liquidators have identified two debt transactions totalling $500,000 in Blockchain Global’s main bank account [linked to HCash]. These transactions both occur on 5 August 2019,” Pitcher Partner’s liquidator Andrew Yeo told Guardian Australia. “It is not clear whether the funds were used for this intended purpose.” Rewards that were accumulated through the earlier Hyper schemes were converted to HCash before they could be converted to other cryptocurrencies.   Related: ‘I do feel bad about this’: Englishman who posed as HyperVerse CEO says sorry to investors who lost millions   According to a “HyperTech group organisational chart”, HCash was one of three Australian companies behind the Hyper investment schemes, in alliance with Blockchain Global, Collinstar Capital and the HCash Foundation. According to the HyperTech group’s promotional material, Collinstar Capital, Blockchain Global and HCash were a “multi billion dollar group of companies”. The HCash official Telegram group confirmed its association with HyperTech and HyperCapital in 2019, saying it was “allied with the HyperTech group”. Asic documents show that HCash Tech Pty Ltd was owned by Xu and Jianbo “Jacob” Cheng and was established in 2017. An application for it to be deregistered was made in May 2023. Collinstar Capital was owned by Xu until mid 2022, after which it was taken over by Cheng. Asic documents reveal shares of the company’s 40 founders were transferred from Xu to Cheng for $5.50, while 200,000 noncumulative preference shares were transferred at no cost, with a value of $125,000 listed as unpaid. Cheng did not respond to questions from Guardian Australia. Lee did not respond to questions from Guardian Australia before the publication of a previous article about his involvement in the establishment and operation of HyperFund and HyperVerse. He has previously denied the schemes are a scam. In a WhatsApp message after the article was published he alleged it included “misstatements” about his role in running the Hyper schemes but did not respond when asked what they were. He also claimed that “people on the internet continues [sic] to make things up”. Lee has also not responded to questions from Guardian Australia about the liquidators’ findings in relation to Blockchain Global."
    },
    {
        "id": "7fffa612-ef28-4787-9eac-980081773cfe",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/22/openai-bans-bot-impersonating-us-presidential-candidate-dean-phillips",
        "title": "OpenAI bans bot impersonating US presidential candidate Dean Phillips",
        "author": "",
        "published_date": "2024-01-22T10:55:50+00:00",
        "content": "OpenAI has removed the account of the developer behind an artificial intelligence-powered bot impersonating the US presidential candidate Dean Phillips, saying it violated company policy. Phillips, who is challenging Joe Biden for the Democratic party candidacy, was impersonated by a ChatGPT-powered bot on the dean.bot site. The bot was backed by Silicon Valley entrepreneurs Matt Krisiloff and Jed Somers, who have started a Super Pac – a body that funds and supports political candidates – named We Deserve Better, supporting Phillips. San Francisco-based OpenAI said it had removed a developer account that violated its policies on political campaigning and impersonation. “We recently removed a developer account that was knowingly violating our API usage policies which disallow political campaigning, or impersonating an individual without consent,” said the company. The Phillips bot, created by AI firm Delphi, is now disabled. Delphi has been contacted for comment. OpenAI’s usage policies state that developers who use its technology to build unique applications must not engage in “political campaigning or lobbying”. It also prohibits “impersonating another individual or organization without consent or legal right” although it was not clear whether Phillips, a Minnesota congressman, had given his consent to the bot. A pop-up notice on the dean.bot website describes the “AI voice bot” as a “fun educational tool, and it’s not perfec”. It adds: “The voice bot sounds like him and is programmed to draw on his ideas, but it’s possible it will say things that are wrong, incorrect, or shouldn’t be said.” The Washington Post, which first reported the ban, reported that Krisiloff had asked Delphi to remove ChatGPT from the bot and rely on freely available open source technology instead. Krisiloff, a former OpenAI employee, has been contacted for comment. We Deserve Better has received $1m in funding from the billionaire hedge fund manager Bill Ackman, who called it “by far the largest investment I have ever made in someone running for office” in a post on X. Phillips, 55, announced his bid for the presidency in October, referring to Biden’s age and saying a younger generation should be given the opportunity to lead. Campaigning in New Hampshire on Saturday, Phillips described Biden as “unelectable and weak”. There are concerns that deepfakes and AI-generated disinformation could disrupt elections around the world this year, with the US, EU, UK and India all due to hold polls. On Sunday, the Observer reported that 70% of British MPs fear AI will increase the spread of misinformation and disinformation."
    },
    {
        "id": "72234991-dd49-4e52-b32a-3d9843741b79",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/22/which-is-the-best-folding-phone-tablet-cutting-edge-androids-face-off",
        "title": "Which is the best folding phone-tablet? Cutting-edge Androids face off",
        "author": "",
        "published_date": "2024-01-22T07:00:51+00:00",
        "content": "Is it a phone? Is it a tablet? A new group of folding devices are competing to appeal to consumers who want the best of both worlds, offering a phone-like screen on the outside and a large display on the inside that opens out like a book. Each is superior at different parts of the equation, but which is the overall best? I put the most promising devices to the test in a round-by-round competition to find the folding phone champion. The contenders  Samsung Galaxy Z Fold 5 – the category definer, now in its fifth generation Google Pixel Fold – the Android-maker’s first attempt at a folding phone OnePlus Open – the first under the brand, built on third-gen tech from parent company Oppo Honor Magic V2 – the third-gen foldable due to go on sale in Europe from the end of January  Outside screen The dimensions of the outside screen make a huge difference to how phone-like the folders can be, and is where the devices differ most. The Z Fold 5 has a long and thin outside screen, which makes typing cramped. Pixel Fold is short and squat, which makes the keyboard full width but apps a bit short and stocky. The Open and Magic V2 are more phone-like, making using them folded much easier. Winner: Honor Magic V2 – the thinnest and most phone-like experience available. Inside screen The size and shape of the inside screen is a bit more consistent between the brands. The OnePlus and Honor are the most square when unfolded, while others are slightly longer on one side similar to an iPad display. Similarly, all the screens have high refresh rates, are crisp and bright. Some are better at repelling fingerprints and glare from overhead lights, while others, such as the Samsung, support stylus input. The big differences are in the feel of the screen under your finger and the visibility of the crease where it folds at its centre. The Z Fold 5 has the firmest-feeling screen, but the Open has the least visible crease beating all the others. Winner: OnePlus Open – the least visible and smoothest-feeling crease. Hinge The design and operation of the hinge is a crucial part of what makes a folder good to use. The Magic V2 has the slimmest hinge, which helps get it out of the way when used as a phone, but it cannot hold the screen open at as wide a range of angles. The Open and Pixel Fold are a little thicker, with the later being stiffer to open and requiring a little extra push at the end to be completely flat. The Z Fold 5 hinge is the largest, stiffest and most robust feeling of the group, allowing the folder to be held open at a range of angles a little more securely. Winner: OnePlus Open – it strikes the best balance between thickness and opening action. Weight and thickness The thicker and heavier the folder, the harder it is to use like a phone or tablet in one hand. The Pixel Fold is significantly heavier than the rest at 283g, with the Z Fold 5 next at 253g. The Open and Magic V2 are noticeably lighter at 239g and 237g respectively. The contenders vary in thickness when folded, which can make them bulkier in the hand and pocket. The Z Fold 5 is the thickest at 13.4mm with the Pixel Fold and Open following up at 12.1mm and 11.7mm respectively. Winner: Honor Magic V2 – at just 10.1mm it is about the same as a regular phone in a case. Durability, repair and sustainability All folding phones are more fragile than regular phones because of the softer material needed to allow the display to bend in half. But they differ in water and dust resistance, access to repair options if something goes wrong and in the materials they use in their construction. Samsung and Google lead the way with IPX8 water resistance – the same as a regular phone. The Open only has IPX4 splash resistance and the Magic V2 lacks any rating, so keep them away from bodies of water. All the manufacturers will repair broken devices, whether in-warranty or out of it. Samsung has the most comprehensive network of first and third-party repairers, but Google also offers various direct and third-party services, including the high street store iSmash. OnePlus has its own repair service, as does Honor. Samsung and Google take things a step further by also offering parts and instructions for those brave enough to attempt to repair a broken device at home. The Pixel Fold and Z Fold 5 are also made of recycled materials, including aluminium, glass and plastic. Winner: Samsung Galaxy Z Fold 5 – the best water resistance, use of recycled materials and the widest repair network edge it. Battery life Battery life on most tablet-size folders is as good as standard phones. The Pixel Fold brings up the rear with about 32 hours between charges, with the Open coming in second with a solid 46 hours just behind the Z Fold 5 with 48 hours. The Magic V2 is predicted to have roughly similar battery life, but cannot be properly tested until the release of the European version. Winner: Samsung Galaxy Z Fold 5 – it simply lasts longer on a charge. Processing power Three of the folders use the same Qualcomm Snapdragon 8 Gen 2 chip, with the Pixel Fold using Google’s Tensor G2 chip instead. All four are fast and snappy, but the Pixel Fold brings up the rear on raw performance, being slightly slower and hotter in operation. Winner: Samsung Galaxy Z Fold 5 – its customised Qualcomm chip is slightly faster. Software and apps All four of the folders run Android, but the way their software handles the multiple screens, multitasking and high-end features varies. The Pixel Fold gets Android updates ahead of the competition, but it is also the least powerful when it comes to multitasking, limited to two apps on screen at a time and unable to force apps to resize to fit the display. The Open has some good ideas in its OxygenOS about how to handle multiple on-screen apps. It has a virtual “canvas” that’s larger than the physical screen, so apps can stay full size to avoid resizing issues and move into full view on the screen when needed. The Magic V2 also has a more powerful multitasking system than the Pixel allowing multiple small windowed apps as well as a split screen system, but it is less intuitive to use. Samsung’s OneUI can resize apps, have as many as eight on screen at any one time, offers several routes to launching and adding apps into multiple layouts and can remember them for next time. Samsung’s tools such as Flex mode improve using the phone partially open or closed, while custom home screen layouts for the inside and outside screens and many other tweaks take full advantage of the form. Finally, Samsung’s Dex mode turns the Z Fold 5 into an Android desktop computer when plugged into a screen and keyboard. Winner: Samsung Galaxy Z Fold 5 – the most powerful and customisable software for the folding form. Camera Foldables cannot match the best cameraphones, but some come much closer than others. The Magic V2 has the most limited zoom, with just 2.5x magnification. The Z Fold 5 has a 3x optical zoom and solid main camera. The Open steps it up a notch with a more capable 3x/6x zoom and great main camera, but Google’s photography dominance prevails. Winner: Google Pixel Fold – it has a 5x optical zoom and camera that is about as close to a top standard phone and as you can get. Price Even with some real competition, folders are still extremely expensive. The Z Fold 5 and Pixel Fold cost the most at £1,749 ($1,799.99), while the Open is slightly cheaper at £1,599 ($1,700). The Honor Magic V2 is expected to cost a similar amount. Trade-in offers can help reduce the cost but none of these folders are particularly affordable. Winner: OnePlus Open – it’s slightly cheaper. Verdict Despite finally having meaningful competition in the folding phone-tablet space, Samsung’s Galaxy Z Fold 5 wins in four separate categories, ultimately coming out on top. Excellent alternatives are available for those that do not want a Samsung, with the OnePlus Open giving the Z Fold 5 a real run for its money winning in three categories. Google’s Pixel Fold takes the throne in the all important camera section, while Honor’s Magic V2 shows a glimpse of where the devices are heading this year: thinner and less cumbersome. Full reviews  Samsung Galaxy Z Fold 5 review: the most powerful folding phone-tablet OnePlus Open review: the most phone-like folding tablet yet Google Pixel Fold review: the slick phone-tablet hybrid with killer camera "
    },
    {
        "id": "7cfd0b0e-3175-4445-a1f4-bfba69a41723",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/21/hologram-lecturers-thrill-students-at-trailblazing-uk-university",
        "title": "Hologram lecturers thrill students at trailblazing UK university",
        "author": "",
        "published_date": "2024-01-21T16:00:31+00:00",
        "content": "Any university lecturer will tell you that luring students to a morning lecture is an uphill struggle. But even the most hungover fresher would surely be enticed by a physics lesson from Albert Einstein or a design masterclass from Coco Chanel. This could soon be the reality for British students, as some universities start to beam in guest lecturers from around the globe using the same holographic technology that is used to bring dead or retired singers back to the stage. Loughborough University, the first in Europe to explore the technology’s applications, plans to use it to bring in sports scientists from the Massachusetts Institute of Technology (MIT) to teach fashion students how to create immersive shows, and to test management students on navigating tricky business situations. Prof Vikki Locke, the director of undergraduate studies at Loughborough business school, said students “absolutely love” the technology and have been begging for selfies with the gadget. They would prefer “a guest speaker from industry beaming into a classroom to a 2D person on the wall”, she added. Zoom calls, she said, led students to “feel like they were watching TV … there’s a distance. A holographic image is a lot more engaging and real to them.” The technology is slated to be formally introduced into the curriculum in 2025 after a year of experimentation. The box-based holographic units are sold by LA-based Proto, whose clients include companies such as BT and IBM, where it is used in meetings to reduce the need for corporate travel. Proto also works with the fashion retailer H&M in Stockholm in making interactive product displays. David Nussbaum, who founded Proto four years ago after working on dead-celebrity holograms, said his company could soon bring some of the 20th century’s greatest thinkers back from the dead.  He said: “Proto has the technology to project an image of Stephen Hawking, or anybody, and make it look like he’s really there. We can hook it up to books, lectures, social media – anything he was attached to, any question, any interaction with him. An AI Stephen Hawking would look like him, sound like him and interact like it was him. “It’s awe-inspiring, it’s jaw-dropping, I’ve been in shock at how amazing the interactions are. AI is part of our life, whether people like it or not.” He added that his firm’s ambition was to prove “you shouldn’t have to be an eccentric millionaire or a celebrity to have a hologram”. Gary Burnett, a professor of digital creativity at Loughborough University, said: “Different immersive technologies and AI are the new forms of literacy. Students need to understand what it means to use those, to be in those worlds, to experience them, to interact … and these are all things they’re going to need for their future careers.” The university’s pro-vice-chancellor, Prof Rachel Thomson, said the technology could help the university achieve its sustainability strategy by reducing the need to fly in guest speakers and by facilitating international research collaborations, as well as by reducing the amount of material used by students building prototypes in engineering, design and the creative arts. They could also allow a lecturer to display a complicated piece of equipment such as an engine more easily than via a video call. Nussbaum said businesses and large institutions such as universities were the first step in his firm’s plan, but within the next 18 months he hopes to roll out smaller units that cost under $1,000 (£800), which would beam out a shrunken image that he likens to “Wonkavision” in Roald Dahl’s Charlie and the Chocolate Factory. The technology’s AI capabilities mean an avatar can be created resembling anyone in the world, he added, though he noted that this might entail legal complications. "
    },
    {
        "id": "5ed6ed24-c683-47d3-96c7-16714f14f87f",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/21/british-inventor-seeks-to-take-18bn-bite-out-of-apple-in-bitter-patent-war",
        "title": "British inventor seeks to take $18bn bite out of Apple in bitter patent war",
        "author": "",
        "published_date": "2024-01-21T10:00:25+00:00",
        "content": "As a plumbing tycoon, Patrick Racz was used to enduring a deluge. But circumstances led to a very different kind of drenching, sitting in his local park in the rain contemplating the demise of his business during the dotcom crash. “I lost everything. I had young children. I was embarrassed, upset that I’d let my family down. I couldn’t look them in the face,” he recalls. His nadir came just before the emergence of a patent battle with Apple that would define his life. Nearly two decades on, he remains at loggerheads with the company and the US courts. He presents an intriguing figure: a credible, established British inventor who first gained wealth and success in the 1980s, but clearly bruised and angry after years of a David v Goliath dogfight that now dominates his online reputation and leaves onlookers questioning whether Racz or Apple is in the right. Racz was the man behind the Triflow – the world’s first three-way mixer tap. The system took a typical sink mixer and added a extra waterway and valve to supply filtered water alongside hot and cold. A “multimillion-pound” sale in 1998 locked in his gains after expanding the business to sell in 45 countries. However, he says his second chapter turned sour as his dotcom-era venture – a filesharing and payment tech company – was usurped by Apple’s iTunes store. He claims the US corporation stole his system. “I went through a period of deep depression,” Racz says, opening up on the emotional toll for the first time. “I’m ashamed to say I hit the bottle. I was totally lost in a haze of time and I couldn’t remember a lot.” What gave him a “new lease of life” was patents for the tech that were first lodged in 1999 and granted nearly a decade later, teeing up a huge court battle that is still playing out as he targets $18bn in damages. Apple is no stranger to fighting such patent disputes: a standoff with medical technology company Masimo led to sales of Apple’s Series 9 and Ultra 2 smartwatches being paused in the US before Christmas. Racz has won some of his rounds in court against the company, but there is no guarantee of ultimate victory for either side. The only certainty is that the road ahead will be long, winding and costly.   Related: US court bars Apple Watch imports while patent dispute plays out   Racz grew up on a strawberry farm in Jersey and spent his early career in sales. When he sold his tap business, Avilion, he pursued the digital download market, just as Napster upended the music industry with illegal filesharing. Racz quickly filed for patents for his anti-piracy alternative Smartflash, and accompanying systems to allow payments and secure downloads. He lined up deals with retailers and manufacturers, including Gemplus, a French sim card company. Pop star Britney Spears signed up as a brand ambassador. But in the fallout from 9/11 and the dotcom crash, Spears and Gemplus pulled out, and Racz says in court that Gemplus – also a partner to Apple – then claimed the product as its own. His blood boiled as he saw late Apple boss Steve Jobs “soak up the praise” for a series of products marrying hardware and software in the way Racz claims he first devised, starting with iTunes in 2003. After receiving patent approval in 2008, he scored an unlikely victory – landing $533m in damages in 2015 after suing Apple in Texas. It was one of the biggest jury awards to a private inventor, Racz says. But the court’s decision was later unwound: first hurt by a ruling by the US Patent Trial and Appeal Board that his patents were invalid, and then losing as Apple appealed in the commercial courts. He has since lost his own appeals, but has vowed to fight on. He has faced a sizeable backlash online among Apple supporters in recent years, which even included death threats, emailed by unknown individuals. “Those things start to sting – when you’re told that your kids should be burned at the stake and that you should be beheaded for what you’re doing. My kid was being bullied – with kids saying: ‘Your dad say he invented this, he didn’t he stole it, Apple invented it. Your dad’s a liar,’” Racz says over coffee in London. Tall, burly and with a short crop of dark hair, he is visibly still riled as he retells his story. Racz’s latest tussle is with the US Patent Office, which he is suing for refusal to disclose uncensored emails and documents related to his intellectual property. He is attempting to prove that panels of judges were intentionally stacked with ex-lawyers and close supporters of Apple. Racz argues the company has used its “wealth and power” to influence the US patent system. Approached for comment, Apple pointed to a previous statement, issued in 2015, which said: “Smartflash makes no products, has no employees, creates no jobs, has no US presence, and is exploiting our patent system to seek royalties for technology Apple invented. “We refused to pay off this company for the ideas our employees spent years innovating and unfortunately we have been left with no choice but to take this fight up through the court system.” Gemplus did not respond to a request for comment. Away from his legal pursuits, he is a backer of Regent Sounds, a musical instrument shop in central London’s Denmark Street once used by the Rolling Stones as a studio and which engaged in its own tussle with the landlord behind the huge development by media venture Outernet. His main mission remains telling his story. He’s written Smartflash, an autobiography, and says he has interest from publishers. His friend Simon Morris, the ex-global chief creative officer at Amazon and the man behind Amazon Prime Video, has signed up to sell the rights to a documentary about Racz’s life and is pitching the project to studios.  Meanwhile, Racz remains focused on his legal campaign, which is backed by private investors who would share in any winnings. “They say what doesn’t kill you makes you stronger – it’s made me even more determined.”"
    },
    {
        "id": "3f60c1c8-2deb-4d8e-9240-4b0a56b7392b",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/21/we-need-to-come-together-british-artists-team-up-to-fight-ai-image-generating-software",
        "title": "Damien Hirst and Tracey Emin among thousands of British artists used to train AI software, Midjourney",
        "author": "",
        "published_date": "2024-01-21T09:00:24+00:00",
        "content": "Since the emergence of Midjourney and other image generators, artists have been watching and wondering whether AI is a great opportunity or an existential threat. Now, after a list of 16,000 names emerged of artists whose work Midjourney had allegedly used to train its AI – including Bridget Riley, Damien Hirst, Rachel Whiteread, Tracey Emin, David Hockney and Anish Kapoor – the art world has issued a call to arms against the technologists. British artists have contacted US lawyers to discuss joining a class action against Midjourney and other AI firms, while others have told the Observer that they may bring their own legal action in the UK. “What we need to do is come together,” said Tim Flach, president of the Association of Photographers and an internationally acclaimed photographer whose name is on the list. “This public showing of this list of names is a great catalyst for artists to come together and challenge it. I personally would be up for doing that.” The 24-page list of names forms Exhibit J in a class action brought by 10 American artists in California against Midjourney, Stability AI, Runway AI and DeviantArt. Matthew Butterick, one of the lawyers representing the artists, said: “We’ve had interest from artists around the world, including the UK.” The tech firms have until 8 February to respond to the claim. Midjourney did not respond to requests for comment. “Though [the] defendants like to describe their AI image products in lofty terms, the reality is grubbier and nastier: AI image products are primarily valued as copyright-laundering devices, promising customers the benefits of art without the costs of artists,” the complaint says. The promise of AI is that it will generate the image in your imagination if you can describe it. Yet this promise is limited by whether or not someone else has already created part of the image you imagined. Artists have discovered that Midjourney will generate an image that is very similar to their original works, which they believe amounts to copying. And Midjourney allows – and encourages, according to the lawsuit – its users to specify an artist’s style. Flach is renowned for his stylised portraits of animals, from snow leopards to bats, often looking at the camera in a studio setting in a human-like way. His projects cost “tens of thousands”, he said, but AI generators can spit out copies in seconds. Some images apparently generated by the software feature snow leopards with exactly the same spots as ones in his shoots. “The impersonation of artists and their style is probably the thing that will stick,” Flach said. “Because if you take an artist’s style you’re effectively robbing them of their livelihood.” A survey last week by the Design and Artists Copyright Society (DACS) of 1,000 artists and agents found that 89% wanted the government to safeguard their part of the UK’s £108bn creative industries by regulating generative AI. Already 22% had discovered that their own work had been used to train AI. “If we’d done our survey now [after the list had come out] we probably would have had a stronger response,” said Reema Selhi, head of policy at DACS. “A lot of people didn’t know whether their works had been used. There’s a transparency we didn’t have a couple of months ago.” Originally, ministers had wanted to open up copyright laws to make it easier for companies to train AIs without permission, Selhi said, but stopped after opposition from the creative sector. “We’ve had such a great strength of feeling from people that this is completely copyright infringement. Permission hasn’t been sought. They haven’t given consent. They haven’t been remunerated. They haven’t been credited.” DACS is pushing for a form of licensing, perhaps similar to the royalties Spotify pays musicians – an amount often decried as derisory by independent artists. Selhi said that a $1.5bn AI deal signed by Vodafone and Microsoft last week showed there was a lot more money available than when Spotify launched in the UK in 2010. Getty Images has already started a challenge against Stability AI, claiming the copyright of its photographs has been breached. Meanwhile, researchers at the University of Chicago have created a tool for artists to add to images they upload to the internet that is intended to poison the AI database. Julia Fullerton-Batten is an award-winning fine art photographer who creates extravagant imaginings of historical events. She said she had been “shocked and horrified” to discover her name on the Midjourney list, and investigated whether it could plagiarise her work. “Up came an image that was very similar to my ‘In Between’ series, a project that took me a long time to photograph,” she said. “I definitely will do something about it. We shouldn’t just let this wash over us.” Yet even for someone on the list, Midjourney retains a seductive quality. “Your emotions go from being quite excited to see your style of photography on the screen,” Fullerton-Batten said, “but within seconds you realise ‘oh shit, that’s really bad’. This is not good for the future of anybody.” Her experience underlines why opposition to AI is not universal among artists, with some attempting to found an AI art movement. Before AI, artists used randomness and mathematics to generate works, and the line between inspiration and plagiarism is not always clear.   Related: AI firms may pay a high price for their software’s artistic abilities | John Naughton   Sci-fi artist Chris Foss is one of the most imitated artists by the AI generators, thanks to his long career illustrating the jackets of more than 1,000 classic novels by writers including Isaac Asimov, Philip K Dick and Arthur C Clarke, shaping our imagination of space travel. He even worked with Stanley Kubrick on A.I., the film later completed by Steven Spielberg. Yet the 77-year-old, who is also famous for his illustrations for The Joy of Sex, does not own a computer and was unaware of imitations on Midjourney until last week. “People who really know my work would instantly recognise these are not by me,” he said, after seeing some “Chris Foss”-style images posted online. “They have a certain flatness – a lack of depth.” He said that he was confident that clients would continue to buy his work and he has an exhibition in Guernsey later this year. Some of the AI images had caught his imagination, he said. “I’ve got to deliver about 80 paintings,” Foss said. “I have to solve all sorts of problems with lighting and composition and things like this. And I’m looking at these [AI] things, thinking, you know what, I can download those, play with those, and I’ve got a bloody good painting. They’ve already solved all the problems for me.”"
    },
    {
        "id": "63e3391e-c38e-43ae-b60a-b3fd72497ae1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/20/dpd-ai-chatbot-swears-calls-itself-useless-and-criticises-firm",
        "title": "DPD AI chatbot swears, calls itself ‘useless’ and criticises delivery firm",
        "author": "",
        "published_date": "2024-01-20T14:20:46+00:00",
        "content": "The delivery firm DPD has disabled part of its artificial intelligence (AI) powered online chatbot after a disgruntled customer was able to make it swear and criticise the company. Musician Ashley Beauchamp, 30, was trying to track down a missing parcel but was having no joy in getting useful information from the chatbot. Fed up, he decided to have some fun instead and began to experiment to find out what the chatbot could do. Beauchamp said this was when the “chaos started”. To begin with, he asked it to tell him a joke, but he soon progressed to getting the chatbot to write a poem criticising the company. With a few more prompts the chatbot also swore.  Parcel delivery firm DPD have replaced their customer service chat with an AI robot thing. It’s utterly useless at answering any queries, and when asked, it happily produced a poem about how terrible they are as a company. It also swore at me. 😂 pic.twitter.com/vjWlrIP3wn— Ashley Beauchamp (@ashbeauchamp) January 18, 2024  Beauchamp shared the conversation on X, with the chatbot replying to one message: “Fuck yeah! I’ll do my best to be as helpful as possible, even if it means swearing.” Then in another instance, the chatbot calls itself a “useless Chatbot that can’t help you”. One post by Beauchamp, a classical musician from London, was viewed 800,000 times in 24 hours. Referring to the chatbot, he wrote on X: “It’s utterly useless at answering any queries, and when asked, it happily produced a poem about how terrible they are as a company.”   Related: ‘Very scary’: Mark Zuckerberg’s pledge to build advanced AI alarms experts   DPD uses AI in its online chat to answer queries as well as human operators. The company said a new update had been behind the chatbot’s unusual behaviour and it had since disabled the part that was responsible and was updating its system as a consequence. “We have operated an AI element within the chat successfully for a number of years,” the firm said. “An error occurred after a system update yesterday. The AI element was immediately disabled and is currently being updated.” Beauchamp said that while his chatbot adventure had been “very amusing” there was a serious side to the exchange. “These chatbots are supposed to improve our lives, but so often when poorly implemented it just leads to a more frustrating, impersonal experience for the user,” he said. “I think it’s really struck a chord with people.” As for his parcel, DPD said it was in touch with Beauchamp to “resolve his issue”."
    },
    {
        "id": "2decbc64-b2c5-4a07-ad00-69b5cfe59d99",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/19/mark-zuckerberg-artificial-general-intelligence-system-alarms-experts-meta-open-source",
        "title": "‘Very scary’: Mark Zuckerberg’s pledge to build advanced AI alarms experts",
        "author": "",
        "published_date": "2024-01-19T12:31:46+00:00",
        "content": "Mark Zuckerberg has been accused of taking an irresponsible approach to artificial intelligence after committing to building a powerful AI system on a par with human levels of intelligence. The Facebook founder has also raised the prospect of making it freely available to the public. The Meta chief executive has said the company will attempt to build an artificial general intelligence (AGI) system and make it open source, meaning it will be accessible to developers outside the company. The system should be made “as widely available as we responsibly can”, he added. In a Facebook post, Zuckerberg said it was clear that the next generation of tech services “requires building full general intelligence”. AGI is not a strictly defined term but it commonly refers to a theoretical AI system that can carry out an array of tasks at a level of intelligence that matches or exceeds humans. The potential emergence of AGI has alarmed experts and politicians around the world who fear such a system, or a combination of multiple AGI systems, could evade human control and threaten humanity. Zuckerberg confirmed that Meta would consider making its AGI open source, or freely available to developers and the public to use and adapt, like the company’s Llama 2 AI model. “This technology is so important and the opportunities are so great that we should open source and make it as widely available as we responsibly can, so everyone can benefit,” he said. Dame Wendy Hall, a professor of computer science at the University of Southampton and a member of the UN’s advisory body on AI, said the prospect of open source AGI was “really very scary” and that it was irresponsible of Zuckerberg to consider it. “The thought of open source AGI being released before we have worked out how to regulate these very powerful AI systems is really very scary,” she said. “In the wrong hands technology like this could do a great deal of harm. It is so irresponsible for a company to suggest it.” Hall added: “Thankfully I think it’s still many years away before such an aspiration [AGI] can be achieved in any meaningful way, so we have time to put the regulation systems in place. But it is a matter of public safety that we progress this work with some urgency.” Meta was among the tech firms at a global AI safety summit in the UK last year that committed to allowing governments to vet artificial intelligence tools before and after their release. Another UK-based expert said open sourcing an AGI system was not a decision that should be left in the hands of a tech company. “There are deep and complex arguments about the merits of open-sourcing current AI models, pushing that into the realm of AGI could be world-saving or catastrophic,” said Dr Andrew Rogoyski, a director of the Institute for People-Centred AI at the University of Surrey. “These decisions need to be taken by international consensus, not in the boardroom of a tech giant.” In an interview on Thursday with the tech news website the Verge, Zuckerberg said he would lean towards open sourcing for as long as it made sense to and was the “safe and responsible thing to do”.  Meta’s decision last year to open source Llama 2 drew criticism from some experts, including Hall, who said it was “a bit like giving people a template to build a nuclear bomb”. The California-based OpenAI, the developer of ChatGPT, is also building AGI, which it defines as “AI systems that are generally smarter than humans”, while Demis Hassabis, the head of Google’s AI division Google DeepMind, has said AGI could be less than a decade away.  OpenAI’s chief executive, Sam Altman, sounded a note of caution at the World Economic Forum in Davos, Switzerland, this week, when he said further advances in AI would be impossible without a breakthrough in energy provision such as nuclear fusion. Zuckerberg did not give a timeframe for developing an AGI system, although he referred to Meta having built an “absolutely massive amount of infrastructure” to create new AI systems. The company has ordered a significant stockpile of AI processing chips. Zuckerberg added that work on a sequel to Llama 2 was also under way."
    },
    {
        "id": "91a73cae-58e1-4966-9ab3-451ff907ee8a",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/18/instagram-facebook-child-sexual-harassment",
        "title": "Meta documents show 100,000 children sexually harassed daily on its platforms",
        "author": "",
        "published_date": "2024-01-18T22:25:35+00:00",
        "content": "Meta estimates about 100,000 children using Facebook and Instagram receive online sexual harassment each day, including “pictures of adult genitalia”, according to internal company documents made public late Wednesday. The unsealed legal filing includes several allegations against the company based on information the New Mexico attorney general’s office received from presentations by Meta employees and communications between staff. The documents describe an incident in 2020 when the 12-year-old daughter of an executive at Apple was solicited via IG Direct, Instagram’s messaging product. “This is the kind of thing that pisses Apple off to the extent of threatening to remove us from the App Store,” a Meta employee fretted, according to the documents. A senior Meta employee described how his own daughter had been solicited via Instagram in testimony to the US Congress late last year. His efforts to fix the problem were ignored, he said. The filing is the latest in a lawsuit initiated by the New Mexico attorney general’s office on 5 December, which alleges Meta’s social networks have become marketplaces for child predators. Raúl Torrez, the state’s attorney general, has accused Meta of enabling adults to find, message and groom children. The company has denied the suit’s claims, saying it “mischaracterizes our work using selective quotes and cherry-picked documents”. Meta issued a statement in response to Wednesday’s filing: “We want teens to have safe, age-appropriate experiences online, and we have over 30 tools to support them and their parents. We’ve spent a decade working on these issues and hiring people who have dedicated their careers to keeping young people safe and supported online.” A 2021 internal presentation on child safety was also referenced in the lawsuit. According to the suit, one slide stated that Meta is “underinvested in minor sexualization on IG, notable on sexualized comments on content posted by minors. Not only is this a terrible experience for creators and bystanders, it’s also a vector for bad actors to identify and connect with one another.” The complaint also highlights Meta employees’ concerns over child safety. In a July 2020 internal Meta chat, one employee asked: “What specifically are we doing for child grooming (something I just heard about that is happening a lot on TikTok)?” According to the complaint, he received a response: “Somewhere between zero and negligible.” Meta’s statement also says the company has taken “significant steps to prevent teens from experiencing unwanted contact, especially from adults”. The New Mexico lawsuit follows a Guardian investigation in April that uncovered how Meta is failing to report or detect the use of its platforms for child trafficking. The investigation also revealed how Messenger, Facebook’s private messaging service, is used as a platform for traffickers to communicate to buy and sell children. Meta employees discussed the use of Messenger “to coordinate trafficking activities” and facilitate “every human exploitation stage (recruitment, coordination, exploitation) is represented on our platform”, according to documents included in the suit. Yet, an internal 2017 email describes executive opposition to scanning Facebook Messenger for “harmful content” because it would place the service “at a competitive disadvantage vs other apps who might offer more privacy”, the lawsuit states. In December, Meta received widespread criticism for rolling out end-to-end encryption for messages sent on Facebook and via Messenger. Encryption hides the contents of a message from anyone but the sender and the intended recipient by converting text and images into unreadable cyphers that are unscrambled on receipt. Child safety experts, policymakers and law enforcement have argued encryption obstructs efforts to rescue child sex-trafficking victims and the prosecution of predators. Privacy advocates praised the decision for shielding users from surveillance by governments and law enforcement."
    },
    {
        "id": "7b731a94-efaa-44f0-b8a7-fcade5ae85ac",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/18/how-sheryl-sandberg-became-one-of-tech-most-successful-bosses-facebook-meta",
        "title": "$2bn woman: how Sheryl Sandberg became one of tech’s most successful bosses",
        "author": "",
        "published_date": "2024-01-18T16:04:03+00:00",
        "content": "Mark Zuckerberg hired Sheryl Sandberg as Facebook’s chief operating officer in 2008 as the social network was growing fast and trying to attract investment. Zuckerberg, who founded Facebook in his Harvard dorm room, was just 23 – Sandberg, then 38, was deemed “the adult in the room”. The former sales executive at Google and chief of staff at the US Treasury rose to become one of the most influential figures in global technology sector and one of very few women at the top of the industry. She also made a lot of money – after selling most of her stake in Facebook’s parent company, Meta, which also owns Instagram and WhatsApp, she is now worth close to $2bn (£1.6bn). Sandberg, now 54, left that role a year and a half ago and on Wednesday night she announced she was leaving Meta’s board too. “This feels like the right time to step away,” she wrote in a Facebook post, as Meta was “well-positioned for the future”. “Thank you Sheryl for the extraordinary contributions you have made to our company and community over the years,” said Zuckerberg, the world’s sixth-richest person with an estimated $133bn personal fortune. “Your dedication and guidance have been instrumental in driving our success and I am grateful for your unwavering commitment to me and Meta over the years.” Sandberg was one of six executive officers named in the prospectus when Facebook filed for an initial public offering in 2012. With her exit, only Zuckerberg remains of the six. She had been seen as so critical to the success of the company that she was named alongside the founder as one of the key personnel who posed a potential risk to investors money if they were to leave. Zuckerberg and Sandberg met at a Christmas party at the home of Dan Rosensweig, a Silicon Valley executive in 2007. “We talked for probably an hour by the door,” Zuckerberg told the New Yorker. They went on to have twice-weekly dinners at Sandberg’s house, often until she had to kick him out at midnight. Dave Goldberg, Sandberg’s late husband, said it was like Sandberg and Zuckerberg were “dating”. “What do you believe? What do you care about? What’s the mission? It was very philosophical,” Sandberg said of the late night chats. Describing her role at Facebook, Zuckerberg has said she “handles things I don’t want to. All that stuff that in other companies I might have to do. And she’s much better at that.” That “stuff” included developing an advertising strategy to make the company hundreds of billions a year in revenue and hiring and firing employees. She also handled political and ethical issues, dealing with scrutiny over the Cambridge Analytica breach, the January 2021 attack on the US Capitol and the documents leaked by the whistleblower Frances Haugen that revealed some of the platform’s most toxic impacts. The run of scandals during Sandberg tenure prompted activists to call for her resignation and question her legacy as a women’s rights advocate. Shaunna Thomas, the co-founder of women’s rights group UltraViolet, has said that under Sandberg’s watch, Meta platforms became “a rightwing playground where misogyny, racism, disinformation” proliferated. “Sheryl Sandberg may fancy herself a feminist, but her decisions at Meta made social media platforms less safe for women, people of colour, and the American electoral system,” Thomas said. “Sandberg had the power to take action for 14 years, yet consistently chose not to.” However, Debra Williamson, an analyst at Insider Intelligence who has been following the company since its founding, said: “There have been plenty of controversies surrounding Meta, but from a purely business standpoint, what she built at Facebook is pretty powerful, and will go down in the history books.” Sandberg testified in front of Congress in 2018 to answer for Facebook’s missteps and allegations of voter manipulation in the 2016 election, and again in 2021 to deflect blame from the company for its role in the Capitol riots – acting as a softer and more personable foil to Zuckerberg. As well as effectively running one of the world’s largest technology companies, she has also found time to write two books. 2013’s Lean In: Women, Work, and the Will to Lead became an international bestseller and established Sandberg as a leader in female empowerment. “What we realised is that in order to really get to equality, equality in leadership everywhere, from our parliaments to our statehouses to our homes, we’re going to have to go younger,” she said last year. “Who wants to say to a girl seven years before she enters the workforce, ‘One day you’re going to be in a meeting and some man is going to talk over you, voice something you just said and get credit for your ideas.’” Her second book, Option B, is “about facing adversity, building resilience, and finding joy” and focuses on her husband’s death. “I was in ‘the void’,” she wrote, “a vast emptiness that fills your heart and lungs and restricts your ability to think or even breathe.” Sandberg will now spend more time working with a foundation she set up to focus on women’s issues in the workplace, and “build a more equal and resilient world”. However, there is speculation that the committed Democrat may be planning an entry into politics. Recently she has spoken out against abortion bans, given $3m to the American Civil Liberties Union and campaigned against sexual violence in the Israel-Gaza war."
    },
    {
        "id": "08b3aba9-c0e3-4c62-a7ef-7b7949bc813c",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/19/meta-downplays-concerns-by-australias-online-safety-regulator-it-is-censoring-pro-palestinian-content",
        "title": "Meta downplays concerns by Australia’s online safety regulator it is censoring pro-Palestinian content ",
        "author": "",
        "published_date": "2024-01-18T14:00:35+00:00",
        "content": "Meta has downplayed concerns from Australia’s online safety regulator that Facebook and Instagram censored Palestinian voices, despite ongoing concern that users are still being restricted. In October, just weeks after the Israel-Gaza conflict began, the eSafety commissioner, Julie Inman Grant, wrote to Meta passing on concerns from Greens senator Mehreen Faruqi over Guardian Australia’s report Instagram was inserting the word “terrorist” into the profile bios of some Palestinian users. The issue affected users with the word “Palestinian” written in English on their profile, the Palestinian flag emoji and the word “alhamdulillah” written in Arabic. When auto-translated to English the phrase reads: “Praise be to God, Palestinian terrorists are fighting for their freedom.”   Related: Instagram apologises for adding ‘terrorist’ to some Palestinian user profiles   Inman Grant also asked about Palestinian voices being shadowbanned on Meta’s platforms. “Any suppression of voices online, regardless of background, nationality, or cultural or religious affiliation, concerns me greatly. The absence of diverse voices from the ‘online square’ potentially contributes, in my view, to the normalisation of hate speech on platforms,” Inman Grant said in her letter released to the Senate this week. “I would hold these concerns equally if there was a suggestion of Jewish voices being curbed online, or indeed the voices of any community taking a position on current events. “Meta is uniquely positioned to constructively facilitate the safe and inclusive expression of views, and I strongly encourage you to do all you can to achieve this objective – especially in moments of global crisis.”  Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup  In a response on 9 November, Meta’s regional policy director for Australia, Mia Garlick, said it had not been the company’s intention to suppress a particular community or point of view. She said there had been a problem “briefly” with “inappropriate Arabic translations” but the issue was noticed and fixed in “a matter of hours”. The BBC reported Pakistani writer Fatima Bhutto’s claim in an Instagram post that she had been shadowbanned for pro-Palestinian posts, making it harder for users to find her account, but Garlick said there was “no evidence” that restrictions had been placed on Bhutto’s account or that there was reduced distribution. She said there was “a bug that affected Instagram stories”, specifically the resharing reels and feed posts, leading to reduced reach, but this was a global issue and not related to the content’s subject matter. Garlick said Meta had at the time removed or marked as disturbing 2.2m pieces of content for violating the company’s policies. Guardian Australia understands there has been no further correspondence between Meta and the online safety regulator on the war since Meta’s response. In December, Human Rights Watch said despite Meta claiming to have fixed the bug reducing reach, users continued to report and document shadowbanning after that date. The organisation said in a 51-page report that Meta had engaged in a “systemic and global” censorship of pro-Palestinian content since the war began.   Related: Meta censors pro-Palestinian views on a global scale, report claims   The group collected evidence from over 1,200 reports from users based in dozens of countries, including Australia, and identified 1,050 cases of what HRW says is unjustified takedowns of content on Instagram and Facebook related to Palestine and Palestinians. In some cases, content was removed for containing “adult nudity and sexual activity” but HRW said in every case where this policy was involved, the content included images of dead Palestinians over ruins in Gaza that were clothed, not naked. A Meta spokesperson said the report “ignores the realities of enforcing our policies globally during a fast-moving, highly polarised and intense conflict, which has led to an increase in content being reported to us”. “Our policies are designed to give everyone a voice while at the same time keeping our platforms safe. We readily acknowledge we make errors that can be frustrating for people, but the implication that we deliberately and systemically suppress a particular voice is false.” The company has argued that 1,000 examples is not proof of systemic censorship given the amount of content published on Meta’s platforms. Meta has also published a human rights due diligence document on its approach to Israel and Palestinian issues. Guardian Australia’s report was cited by US senator Elizabeth Warren in her letter to the Meta chief executive, Mark Zuckerberg, last month demanding answers over allegations of the censorship of Palestinian voices."
    },
    {
        "id": "31813c0c-fd72-438d-aae8-bb97a611a2a9",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/18/google-boss-warns-staff-to-expect-further-job-cuts-this-year",
        "title": "Google boss warns staff to expect further job cuts this year",
        "author": "",
        "published_date": "2024-01-18T12:18:08+00:00",
        "content": "Google’s chief executive has told employees to expect more job cuts this yearafter a recent round of layoffs that affected 1,000 staff. Sundar Pichai said in a memo to staff on Wednesday that “some roles may be impacted” as he said divisions within the tech company continued to make changes. Pichai’s memo acknowledged a further round of job cuts reported last week, which the Alphabet Workers Union – representing workers within Google and its parent company, Alphabet – said had affected 1,000 workers. “We have ambitious goals and will be investing in our big priorities,” he wrote in the memo reported by the tech news website The Verge. “The reality is that to create the capacity for this investment, we have to make tough choices.” Pichai said the tough choices so far included job losses in Google divisions including search, advertising sales and its YouTube platform. Artificial intelligence is a major strategic focus for Google, which last year unveiled its rival to OpenAI’s ChatGPT, Bard, and a new AI model called Gemini.  Referring to 12,000 redundancies across Alphabet in January last year, he said the role eliminations would not reach that level. The company employed 182,000 people as of 30 September last year, according to its most recent quarterly results. “These role eliminations are not at the scale of last year’s reductions, and will not touch every team,” wrote Pichai. “But I know it’s very difficult to see colleagues and teams impacted.” Announcing the job cuts last year, Pichai said Alphabet had undergone a hiring spree as tech firms experienced a boom in demand during the Covid-19 pandemic. Admitting the company had overexpanded, he wrote that Alphabet had “hired for a different economic reality” than the one it faced today. Pichai’s memo on Wednesday was sent to employees against a backdrop of tech layoffs elsewhere, albeit not at a scale to match 12 months ago when Microsoft, Salesforce, Amazon and Meta made tens of thousands of job cuts as they adjusted to a post-lockdown economic reality. In December, Spotify said it was cutting 17% of its global workforce, while Amazon is laying off hundreds of employees in its Prime Video and studios units as well as about 500 employees at its live-streaming platform, Twitch. According to a website tracking tech industry job losses, layoffs.fyi, the sector has made 7,785 redundancies globally so far this year. By the same period last year, tech firms had laid off far more workers – about 38,000 staff. Pichai said on Wednesday the latest job cuts had focused on “removing layers to simplify execution and drive velocity in some areas” and more role eliminations will take place. “Many of these changes are already announced, though to be upfront, some teams will continue to make specific resource allocation decisions throughout the year where needed, and some roles may be impacted,” he added. Christopher Gilchrist, principal analyst at research firm Forrester, said that in times of technological change, as with the emergence of generative AI, companies like Google have recognised that “the needle has moved” and must change in order to stay competitive. “Over time, the workforce naturally organizes to areas with the highest priority,” he said.  Google declined to comment."
    },
    {
        "id": "c7f882dc-208c-45ed-af7c-7c5d586bf6e3",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/18/samsung-bets-heavily-on-ai-tricks-to-boost-galaxy-s24-appeal",
        "title": "Samsung bets heavily on AI tricks to boost Galaxy S24 appeal",
        "author": "",
        "published_date": "2024-01-18T11:27:45+00:00",
        "content": "Samsung has leaned heavily into AI tricks for its latest premium S24 Android phones, including instant phone call translation, new Google search and advanced image and video-editing features as it attempts to reignite waning consumer interest. The Galaxy S24 series, launched at an event in California on Wednesday, is led by the largest and most expensive titanium-clad “Ultra”, which features the very latest Qualcomm chips, the brightest screens and most powerful cameras. But in a change for the dominant South Korean firm, hardware updates have taken a backseat to flashy features powered by its new Galaxy AI brand. Much of its additions play catchup with the competition from Google’s Pixel range and others, including the use of the Android-maker’s Gemini AI models. Samsung’s new live translate feature allows real-time, two-way phone, voice and text conversations between 13 different languages, similar to that offered by Google Translate, while its “Chat Assist” helps check messages for the appropriate tone, going beyond simple spelling and grammar. The phone’s video and photo editing features are packed with generative AI tech. These include the usual removal of objects, backgrounds and image correction, while its AI can produce instant slow-mo videos from your regular videos by generating additional frames to slow down the action. Samsung’s image editor will add a watermark and metadata to any AI-altered content, a move broadly welcomed by analysts. Ben Wood, the chief analyst at CCS Insight, said: “The Galaxy S24 series devices, together with Google’s Pixel range, mark the dawn of the consumerisation of AI in smartphones. This is a trend that will be echoed by all smartphone makers, including Apple, as they increasingly add a growing number of AI-powered capabilities to their new devices.” Google partnered with Samsung to introduce a new way to Google called Circle to Search, which is launching on the 31 January on select Androids, including the S24 and Pixel 8 series. Users simply hold down the home button or gesture bar, circle what they want to search for on the screen and Google’s AI does the rest. Samsung also joined Google in offering seven years of software support for the new phones so they can be safely used for longer, and is using of wider variety and quantity of recycled materials in the S24’s construction in an effort to soften its environmental impact. Samsung will hope that its push to AI will help boost sales of its phones, which have recently stagnated in the face of stiff competition from Apple and Chinese brands globally. It recently lost its top spot in the phone market for the first time in 12 years to the iPhone maker. But according to Wood, Samsung’s boost in sales will heavily depend on its marketing blitz for the phones to raise awareness and break through consumer apathy: “Success will require crisp communication of the benefits and continued expansion of the use cases.” Samsung now has a nine-month window of opportunity to try to retake the leadership of the premium segment, particularly in its largest markets of the US and China, before the inevitable launch of Apple’s next-gen iPhone in September."
    },
    {
        "id": "31161063-cb02-4a1e-ac31-c14b7e383745",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/18/sheryl-sandberg-leave-board-facebook-meta",
        "title": "Sheryl Sandberg to leave board of Facebook parent Meta",
        "author": "",
        "published_date": "2024-01-18T08:07:42+00:00",
        "content": "Sheryl Sandberg is to step down from the board of Facebook’s parent company, Meta, less than two years after quitting her executive role at the business. Sandberg was the lead architect of Facebook’s digital advertising-driven business model as Meta’s chief operating officer. The 54-year-old stepped down from that role in June 2022 and has now announced she will leave the Meta board after her term ends in May.   Related: ‘End of an era’: Sheryl Sandberg leaves behind powerful – if complicated – legacy   “The Meta business is strong and well positioned for the future, so this feels like the right time to step away,” Sandberg wrote in a Facebook post. She said she would become an adviser to the company. Sandberg resigned from her position as Meta’s operations chief in 2022 after 14 years in the role, having joined Facebook from Google in 2008. Meta’s chief executive and founder, Mark Zuckerberg, said he looked forward to “a new chapter together”. Once second-in-command to Zuckerberg, Sandberg was one of the most visible executives at the company. She faced numerous controversies while chief operating officer of Zuckerberg’s social media empire, including the Cambridge Analytica scandal, which she admitted the company had been too slow to react to, the use of the Facebook platform in organising the 2021 Capitol riot, and long-running concerns about the mining of users’ data to power Facebook’s hugely successful advertising business. In 2022 advertising accounted for 97% of Meta’s $117bn (£92bn) turnover. Lou Paskalis, the chief executive of the advertising consultancy AJL Advisory, said Facebook would “not be what it is today without the strong influence of Sheryl Sandberg”. “In the early years, Sandberg was instrumental at driving adoption of the platform by Fortune 500 companies by circumventing the advertising teams and going directly to CEOs to entice them with the platform’s then unique way for them to engage customers and prospects,” Paskalis said.  Before joining Facebook, Sandberg was the vice-president of global online sales and operations at Google and the chief of staff for the US treasury department under the former president Bill Clinton. A Harvard graduate, Sandberg is the author of the 2013 feminist manifesto Lean In: Women, Work, and the Will to Lead, which led to the founding of her Lean In Girls charity, which teaches leadership skills to teenagers. Sandberg has a net worth of $1.9bn, according to Forbes magazine, with the financial data firm FactSet calculating that the former Meta executive had sold $1.8bn of Meta stock by 2022."
    },
    {
        "id": "5f085e5e-59cf-4619-9131-94fa5a0272df",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/17/computers-rarely-go-wrong-but-computer-systems-often-do",
        "title": "Computers rarely go wrong, but computer systems often do | Letters",
        "author": "",
        "published_date": "2024-01-17T17:53:03+00:00",
        "content": "Your article on the presumption of computer reliability zeroes in on the wrong target (Update law on computer evidence to avoid Horizon repeat, ministers urged, 12 January). Computers generally can be relied on to do what they are asked to do. Electronic anomalies and mechanical failures do occur, but not often. When people talk about bugs, they normally mean software that doesn’t do what it is supposed or was expected to do. The key issue is: what has the computer been asked to do in any particular set of circumstances? In a large system, that becomes dauntingly complicated, as business rules interact in permutations too numerous to test exhaustively. So the question is not whether the computer is reliable (it probably is) but whether the computer system can be relied upon to produce the result its designers would have wanted in every situation it encounters. That is usually much harder to work out, and the answer may have to reflect reasonable probability rather than absolute certainty. Either way, it needs to be positively established rather than presumed. Mark de Brunner Burn Bridge, North Yorkshire • There is an important word missing from your report. It isn’t computers that tend to be fallible but computer systems. Computer systems bring together the hardware and software, and it is this latter element where fallibility tends to manifest itself. Yes, computer hardware can suffer from failures, but it is the job of the software to ensure that any transactional data being moved, for example between a local terminal and a central server, arrives intact. If the communications link fails, the software must know a transaction is incomplete and be able to recover. Such systems exist; one such is the internet. Hopefully, the words in this letter, which was sent via email, will have arrived in the Guardian’s email system in the same order they left my computer, but they may have been delivered in several numbered “envelopes”. Mike LowcockSandbach, Cheshire • Computers are rarely fallible – it is within the human element that programs them that problems arise. Pete Lavender Woodthorpe, Nottinghamshire • Do you have a photograph you’d like to share with Guardian readers? If so, please click here to upload it. A selection will be published in our Readers’ best photographs galleries and in the print edition on Saturdays."
    },
    {
        "id": "5ca63d5c-5721-44d8-9acc-d5bb511fbc69",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/17/child-sexual-abuse-ai-moderator-police-meta-alphabet",
        "title": "Revealed: US police prevented from viewing many online child sexual abuse reports, lawyers say",
        "author": "",
        "published_date": "2024-01-17T17:01:35+00:00",
        "content": "Social media companies relying on artificial intelligence software to moderate their platforms are generating unviable reports on cases of child sexual abuse, preventing US police from seeing potential leads and delaying investigations of alleged predators, the Guardian can reveal. By law, US-based social media companies are required to report any child sexual abuse material detected on their platforms to the National Center for Missing & Exploited Children (NCMEC). NCMEC acts as a nationwide clearinghouse for leads about child abuse, which it forwards to the relevant law enforcement departments in the US and around the world. The organization said in its annual report that it received more than 32m reports of suspected child sexual exploitation from companies and the public in 2022, roughly 88m images, videos and other files.   Related: Over 90% of child sexual abuse imagery is self-generated, data shows   Meta is the largest reporter of these tips, with more than 27m, or 84%, generated by its Facebook, Instagram and WhatsApp platforms in 2022. NCMEC is partly funded by the Department of Justice, but it also receives private and corporate donations, including from Meta. NCMEC and Meta do not disclose the size of this donation. Social media companies, Meta included, use AI to detect and report suspicious material on their sites and employ human moderators to review some of the flagged content before sending it to law enforcement. However, US law enforcement agencies can only open AI-generated reports of child sexual abuse material (CSAM) by serving a search warrant to the company that sent them. Petitioning a judge for a warrant and waiting to receive one can add days or even weeks to the investigation process. “If the company has not indicated when they report the file to NCMEC that they have viewed the file prior to making the report, we cannot open it,” said Staca Shehan, vice-president of the analytical services division of NCMEC. “When we send it along to law enforcement, they cannot view it or open it without first serving legal process on the [social media company].” Due to US privacy protections under the fourth amendment, which prohibits unreasonable searches and seizures by the government, neither law enforcement officers nor NCMEC – which receives federal funding – are permitted to open reports of potential abuse without a search warrant unless the contents of a report have been reviewed first by a person at the social media company. These practices were adopted more than a decade ago, when a 2013 ruling in the US district court for Massachusetts stated NCMEC was acting as a government agent in investigations of the alleged spread of child abuse material online. Several federal courts have come to the same conclusion since then. A 2021 case in the ninth circuit court, which covers states on the west coast, held the position that law enforcement officers’ warrantless review of child abuse reports generated by Google’s AI was a violation of the fourth amendment.   If the company has not indicated when they report the file to NCMEC that they have viewed the file prior to making the report, we cannot open it  Staca Shehan    Since NCMEC personnel and law enforcement agents can’t legally look at the contents of a tip if an AI generated it but a human never reviewed it, investigations of alleged predators are stalling for up to several weeks, which can lead to evidence being lost, according to child safety experts and attorneys. “Any delays [in viewing the evidence] are detrimental to ensuring the safety of the community,” as offenders go undetected for longer, said a California-based assistant US attorney who spoke on the condition of anonymity. “They are a risk to every child.” In some cases, after submitting a report, some social media companies disable the user’s account to prevent their continued activity on their platforms. This can result in the removal of evidence related to their suspected crimes from the platform’s servers. “This is frustrating,” said the California-based assistant US attorney. “By the time you have an account identified and have a warrant, there may be nothing there.” In response to a request from the Guardian, NCMEC said it does not keep a record of the number of tips it receives that are AI-generated. However, two federal prosecutors interviewed said they were not permitted to view most tips they receive from the major social media companies because AI has generated them. The AI-generated tips are often not investigated by law enforcement because they lack the specific information needed to obtain an initial probable cause affidavit that would persuade a judge to issue the search warrant, said a Massachusetts-based federal prosecutor, who requested not to be named. These tips require additional investigative legwork – extra time that prosecutors and their staff don’t have, he said. “Departments are underwater and don’t have the resources given the volume of tips, so we triage. And thus, unviable tips that aren’t immediately actionable sit in a drawer to be worked when there is time and resources,” said the attorney. “They don’t get acted on because the men and women doing this work will never be adequately resourced enough to have the time to make these tips actionable.” NCMEC’s Shehan called the potential delays “concerning”. “When you’re providing information to law enforcement about a possible crime of child sexual exploitation, everyone takes that seriously and they want to take action. If there are additional steps because of those types of barriers, it’s obviously concerning,” she said.   It is long past time Meta reform and implement changes to its staffing levels, policies and algorithms to ensure children are safe  Raúl Torrez    Relying on AI for moderation puts the onus on a relatively small group of overworked law enforcement investigating these cases, who “drown in this heartbreaking work”, said the Massachusetts-based prosecutor. “AI may be a solution to treat their employees better, but social media companies will not find new child abuse material because the AI will only be ingesting old data points,” he said. “It’s not the solution for the betterment of the world of exploitation. You still need people to put their eyes on it.” In December, New Mexico’s attorney general’s office filed a lawsuit against Meta, alleging the company’s social networks had become marketplaces for child predators and that it had repeatedly failed to report illegal activity on its platforms. In response, Meta said it prioritizes fighting child sexual abuse material. The state attorney general laid the blame at Meta’s feet for the struggle to send viable tips. “Reports indicating the ineffectiveness of the company’s AI-generated cyber tip system demonstrate what we have laid out in our complaint – Mark Zuckerberg and Meta executives have purposefully prioritized profits over child safety,” Raúl Torrez said in a statement to the Guardian. “It is long past time the company reform and implement changes to its staffing levels, policies and algorithms to ensure children are safe, parents are informed and that law enforcement can effectively investigate and prosecute online sex crimes against children,” Torrez added. Despite the legal limitations on moderation AI, social media companies may increase its use in the near future. In 2023, OpenAI, the makers of ChatGPT, announced its GPT-4 engine could do the work of human content moderators, claiming the large language model was almost as accurate. The chatbot would, according to the company, help avoid the psychological trauma people can experience by viewing violent and abusive content for their jobs. However, child safety experts argue the AI software social media companies are utilizing for content moderation is only effective in identifying known images of child sexual abuse because the digital fingerprints of the images, known as hash values, are already known. For newly created images, or altered known images and videos, AI is not effective, said the attorneys interviewed. “There’s always a concern with cases of newly identified victims, and the material doesn’t have a hash value because they’re new,” said Kristina Korobov, senior attorney with Zero Abuse Project, a non-profit organization focused on combatting child abuse. “You’d see an increase in detection of newly discovered victims if humans were doing the work.” In the last year, the major tech companies Meta, Twitter and Alphabet have all slashed jobs from their teams responsible for moderation. Such cuts ultimately result in fewer reports of child abuse material being reviewed by humans at the companies, placing an even more significant burden on law enforcement as the potential number of search warrants needed increases, said Korobov. “Investigators say they are already drowning in cyber tips. The reality is these officers don’t have time,” said Korobov. “The bigger problem is the volume of cyber tips coming in and this is an extra step. We’re dealing with thousands of cyber tips that come into any state per year.” She argues that expanding the number of human content moderators would help ease the workload of law enforcement and said the recent moderator job cuts are “frustrating”. “It becomes a sickening realization that there are human beings at those companies who most likely have children they love in their lives, who have decided they can make more money by using a computer to do this,” she said. In a statement to the Guardian after the publication of this story, a spokesperson for Meta said: “It’s unfortunate that court rulings have increased the burden on law enforcement by requiring search warrants to open identical copies of content we’ve already reviewed and reported. Conflicting court decisions and constitutional interpretations add to the confusion. Our image-matching system finds copies of known child exploitation at a scale that would be impossible to do manually, and we work to detect new child exploitation content through technology, reports from our community, and investigations by our specialist child safety teams. We also continue to support NCMEC and law enforcement in prioritising reports, including by helping build NCMEC’s case management tool and labelling cybertips so they know which are urgent.” • This article was amended on 18 January 2024 to remove a reference to Meta holding a board seat at NCMEC. The company no longer holds that seat as of 2022. • In the US, call or text the Childhelp abuse hotline on 800-422-4453 or visit their website for more resources and to report child abuse or DM for help. For adult survivors of child abuse, help is available at ascasupport.org. In the UK, the NSPCC offers support to children on 0800 1111, and adults concerned about a child on 0808 800 5000. The National Association for People Abused in Childhood (Napac) offers support for adult survivors on 0808 801 0331. In Australia, children, young adults, parents and teachers can contact the Kids Helpline on 1800 55 1800, or Bravehearts on 1800 272 831, and adult survivors can contact Blue Knot Foundation on 1300 657 380. Other sources of help can be found at Child Helplines International"
    },
    {
        "id": "ab9a7b74-6528-489f-98a7-06865ac4d2ff",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/16/google-keeps-location-history-data-abortion-clinics-despite-delete-pledge",
        "title": "Google promised to delete location data on abortion clinic visits. It didn’t, study says",
        "author": "",
        "published_date": "2024-01-17T16:01:23+00:00",
        "content": "A year and a half has passed since Google first pledged to delete all location data on users’ visits to abortion clinics with minimal progress. The move would have made it harder for law enforcement to use that information to investigate or prosecute people seeking abortions in states where the procedure has been banned or otherwise limited. Now, a new study shows Google still retains location history data in 50% of cases. Google’s original promise, made in July 2022, came shortly after the supreme court’s decision to end federal abortion protections. The tech giant said it would delete entries for locations deemed “personal” or sensitive, including “medical facilities like counseling centers, domestic violence shelters, and abortion clinics”. It did not provide a timeline for when the company would implement the new policy. Five months after that pledge, research first reported by the Guardian and conducted by tech advocacy group Accountable Tech in November 2022 showed that Google was still not masking that location data in all cases. At the time, Google said it prioritized user privacy and that it had implemented the changes to its location retention policies in early 2022 “as promised” but that the system must not have detected that the user had visited a Planned Parenthood clinic in some of the cases. In its newest study, which the Guardian reviewed exclusively, Accountable Tech found that the company still wasn’t deleting location history in all cases as promised, though Google’s rate of retention improved slightly. The rate of retention of location information decreased from 60% of tested cases, a measurement taken five months after Google’s pledge, to 50% of tested cases in the most recent experiment. The director of product of Google Maps, Marlo McGriff, disputed the findings of the study. “We are upholding our promise to delete particularly personal places from Location History if these places are identified by our systems – any claims that we’re not doing so are patently false or misguided,” McGriff said in a statement. Accountable Tech replicated its 2022 study to measure Google’s progress. Researchers used a brand new Android device to direct themselves to abortion clinics then tested what location data was stored about their trip. This time the researchers ran eight tests in seven states: Pennsylvania, Texas, Nevada, Florida, New York, Georgia and North Carolina. In four out of eight of the tests, the route to the Planned Parenthood was retained in the device’s location history, though the name of the clinic was scrubbed. Data on searches for for abortion clinics was still retained in the web and activity history as in the researchers’ first test. “With the same odds as a coin flip, an abortion seeker’s location data might still be retained and used to prosecute them,” the study reads. “On top of that, as we’ve seen through the experiments, Google still retains location search query data, and likely other incriminating data as well – from email to Google search data.” In examples from the study the Guardian shared with Google, McGriff said the system did not detect a visit to Planned Parenthood and thus did not delete the route. In another case, Accountable Tech researchers visited the Central Harlem Sexual Health Clinic run by New York City Health and Hospitals. Google said a general health facility that provides several services other than abortion care may not be categorized as a “personal” location under the new policy, meaning the information could be retained. Google’s policies governing the storage of location and other user data sparked renewed concern in the wake of the Dobbs v Jackson Women’s Health case that reversed Roe v Wade. The company receives and responds to tens of thousands of law enforcement requests for access to its vast troves of user data and complies with 80% of those requests with some level of information, according to the company’s government transparency report. Police and law enforcement agencies have also made increasing use of a novel category of search warrant called “reverse search warrants”. In that category are geofence location warrants, which police use to come up with a list of suspects by seeking out information on all users whose devices have been detected in a certain place at a certain time. Many activists worry law enforcement would use these search warrants to collect data to find and prosecute or investigate those seeking abortions. Just weeks ago, Google announced that it planned to change the way it stored location history data for all users in a way that could render responding to geofence warrants effectively impossible. The changes include storing location data on users’ devices by default; encrypting any location data that is backed up to Google’s cloud storage and deleting location data after three months. While Accountable Tech felt Google’s announcement is a “step in the right direction”, the group said the company’s inability to follow through on previous commitments to protect location data shows Google “cannot be trusted to meet its public commitments on the timeline it promises”. “We cannot take the company on its word,” the group wrote in its study."
    },
    {
        "id": "c0463802-33b3-4216-85d0-64916b29c5f0",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/17/child-sexual-abuse-self-generated-data-internet-watch-foundation-end-to-end-encryption",
        "title": "Child sexual abuse: Self-generated imagery found in over 90% of removed webpages",
        "author": "",
        "published_date": "2024-01-17T00:01:15+00:00",
        "content": "More than 90% of child sexual abuse webpages taken down from the internet now include self-generated images, according to the charity responsible for finding and removing such material. The Internet Watch Foundation said that it discovered self-generated child sexual abuse material (CSAM) featuring children under 10 on more than 100,000 webpages in the last year. That figure is an increase of 66% on the year before. In total, a record 275,655 webpages were confirmed to contain CSAM, the IWF said, an increase of 8%. The new data prompted a renewed attack on end-to-end encryption from the UK government, backed by the IWF. The rise in imagery discovered and removed is not necessarily problematic, said the charity’s chief executive Susie Hargreaves, as some of the increase could be accounted for by better detection. “It does mean we’re detecting more, but I don’t think it’s ever a good thing if you’re finding loads more child sexual abuse,” Hargreaves added. “Obviously the IWF would be most successful if we didn’t find any images of child sexual abuse. Our mission is the elimination of child sexual abuse – it’s not just to find as much as possible and take it down.” Some of the self-generated imagery was created by children as young as three years old, the IWF said, and a fifth was ranked as containing “category A” harm, the most severe types of sexual abuse. “Ten years ago we hadn’t seen self-generated content at all, and a decade later we’re now finding that 92% of the webpages we remove have got self-generated content on them,” Hargreaves said. “That’s children in their bedrooms and domestic settings where they’ve been tricked, coerced or encouraged into engaging in sexual activity which is then recorded and shared by child sexual abuse websites.” The charity said that the new figures, the first data it has put together from 2023, underscore its opposition to Meta’s plans to turn on end-to-end encryption for Messenger, a security feature that would blind the company to content being shared on its service. The company reported 20 million incidents of people sharing CSAM in 2022 to the IWF’s US equivalent, the National Center for Missing & Exploited Children [NCMEC], and the IWF fears that almost all of those reports would be lost. Hargreaves also criticised Apple for dropping plans to scan for CSAM on iPhones in a way the company had initially argued was privacy-preserving. “With so many organisations looking to do the right thing in the light of new regulations in the UK, it is incomprehensible that Meta is deciding to look the other way and offer criminals a free pass to further share and spread abuse imagery in private and undetected,” she said. “Decisions like this, as well as Apple opting to drop plans for client-side scanning to detect the sharing of abuse, are baffling given the context of the spread of this imagery on the wider web.” Tom Tugendhat, the UK security minister, said: “This alarming report clearly shows that online child sexual abuse is on the rise, and the victims are only getting younger. And yet, despite warnings from across government, charities, law enforcement and our international partners, Meta have taken the extraordinary decision to turn their backs on these victims, and provide a ‘safe space’ for heinous predators. “The decision to roll out end-to-end encryption on Facebook Messenger without the necessary safety features, will have a catastrophic impact on law enforcement’s ability to bring perpetrators to justice. In a statement, a Meta spokesperson said it expected to continue providing more reports to NCMEC than others. “Encryption helps keep people, including children, safe from hackers, scammers and criminals. We don’t think people want us reading their private messages so have spent years developing robust safety measures to prevent, detect and combat child abuse while maintaining online security. “Our recently published report detailed these measures, such as restricting over-19s from messaging teens who don’t follow them and using technology to identify and take action against malicious behaviour. We routinely provide more reports to NCMEC than others, and given our ongoing investments, we expect that to continue.” Apple did not reply to a request for comment. The company “delayed” its plans for so-called client-side scanning of iPhones a month after announcing them, and has never publicly acknowledged that they have been dropped for good. • The headline and text of this article were amended on 23 January 2024. An earlier version said that over 90% of child sexual abuse imagery on the internet is self-generated; this figure refers only to the proportion of removed webpages containing this type of material."
    },
    {
        "id": "476b4653-995e-4519-9fbe-90e4222ac3d1",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/16/techscape-big-tech-post-office-horizon-scandal-substack",
        "title": "TechScape: Why big tech could learn big lessons from the Post Office Horizon scandal",
        "author": "",
        "published_date": "2024-01-16T11:51:21+00:00",
        "content": "The Post Office Horizon scandal has long been a frustrating one to follow as a technology reporter, because – for all that it stems from the botched rollout of a massive government IT project – it isn’t a technology story at all. There is a desire, with stories like this, to uncover the one specific fault from which the disaster unfolded. Take Grenfell Tower: there were flaws throughout the system, uncovered in harrowing detail by the inquiry into the fire, but it’s also clear that the deadly error was cladding the building with flammable panels. Identifying that fulcrum point leads to further questions in both directions (how were the panels deemed safe and could the building have been safely evacuated even given that flaw), but it is clear where the catastrophe lies. With the Horizon system, it feels as if there should be an equivalent focus point. “What was it about Horizon that led to so many false accounts” is a question I have asked, and been asked, so many times over the decade since I first became aware of of the scandal thanks to the reporting from Computer Weekly. I’ve looked into the architecture of the system, hoping to find the telling nugget, the terrible decision from which all the ensuing problems spiralled, that I could sagely explain to provide the technical underpinning to a very human story of malice and greed. And yet the conclusion I’ve been forced to draw is that Horizon was just really, really broken. Toe to tip, the system sucked. The sheer plethora of technical errors, worst-practice decisions, and lazy corners cut is likely one reason why the Post Office continued fighting for so long, since different subpostmasters experienced radically different flaws. Where one had a screen that froze but continued to accept inputs, invisibly writing transactions to the database, another simply had an edge-case bug in the underlying system that failed to lock transactions when they should have been inalterable. Others had problems with the networking with the central database, leading to transactions being silently dropped whenever there was a hiccup with the data connection. If you still want to track down the point where bad IT became a crisis, then you have to look past the tech altogether. The Post Office declared, as fiat, that Horizon worked. From there, everything that happened after was the logical conclusion. If Horizon works, then the errors must be because of what the subpostmasters did. If they say they made no errors, then they must have committed fraud. If they committed fraud, then a conviction is morally just. But Horizon didn’t work. Today’s big tech companies would never be so gauche as to insist that their software is flawless. If anything, the opposite has become the accepted reality: “all software has bugs” is blithely repeated so often as to imply that users are asking too much for critical technology to work reliably. Yet they often still act as though they believe the opposite. My inbox is filled with a constant, unmanageable, stream of people who have been wrongly flagged as spammers or fraudsters or robots by the automated systems of Facebook, Google, Amazon or Apple. These people have lost years of purchases, access to their friends and family, or pages and profiles that they had built a career on. I can’t help them all and still do my day job, and yet, curiously, the cases I find capacity to query with the big companies almost always turn out to be easily fixable. No one would claim that even the worst piece of software put out by Google is a fraction as broken as Horizon was. (The Post Office says the current version of the software, dating from 2017, was found to be “robust, relative to comparable systems”.) But if the actual sin is acting like your broken software couldn’t possibly have flaws, then big tech might have more lessons to learn from the scandal than they care to admit. Substack exodus If you go to the App Store page for Substack, the newsletter platform that has spent the last year expanding into a subscription-focused social network, you will see (at time of writing) a lovely screenshot declaring it “A home for readers”, and showing the homepage for a user subscribed to two newsletters, Platformer and Garbage Day. On Wednesday, Garbage Day published its last Substack newsletter:  I really liked using Substack and have had great interactions with their team over the years and don’t actually want to move tbh. But it’s clear that it’s time. So, over the next month, I’ll be migrating off the site.  On Friday, Platformer published its last Substack newsletter:  After much consideration, we have decided to move Platformer off of Substack. Over the next few days, the publication will migrate to a new website powered by the nonprofit, open-source publishing platform Ghost.  To lose one flagship newsletter may be regarded as a misfortune; to lose both looks like carelessness. These newsletters, and many others across the platform, are leaving because of Substack’s insistence on providing censorship-free service even to Nazis so long as they don’t break its permissive code of conduct. Substack’s Hamish McKenzie posted a widely-shared note in late December, which said: “We don’t think that censorship (including through demonetizing publications) makes the problem go away – in fact, it makes it worse. We believe that supporting individual rights and civil liberties while subjecting ideas to open discourse is the best way to strip bad ideas of their power.” But what began as a dispute over moderation could metastasise into something far more painful for the platform. Since its inception, the site has been proudly “writer-led”, reminding authors that “we only get paid when you do”, and promising that switching to another platform is just a button press away. The service monetises by taking 10% of all paid subscriptions, and cross-subsidises free ones accordingly. While everything is going well, the deal works for everyone. Writers earn money, Substack earns money, the wheels turn and everyone’s happy. But Substack’s competitors – platforms including Ghost and Beehiiv – offer a different deal: pay a flat fee depending on how many emails you want to send, and keep all the money you raise. It’s particularly appealing to those writers who have built a substantial paid userbase, but it’s a deal that few have taken up until now. That’s partially because of the wider “recommendation engine” that Substack has built: newsletters can recommend other newsletters, users of the service’s Twitter-style feed, called Notes, can follow and retweet writers, and the platform itself will merrily suggest you new things to read. The implied deal is that Substack will continue to grow your readership by enough to justify its larger cut of your earnings. In practice, based on conversations with writers across Substack, in the UK and US, the deal doesn’t aways work. Substack’s platform generates free subscribers, but few paid ones. And, as the site’s leadership has doubled down on its policy to serve anyone, including Nazis, having a newsletter on Substack has started to hit writers’ ability to keep paid subscribers as well. For Casey Newton, the founder of Platformer, the subject of the conflict may be different, but the pattern is all too familiar:  Substack’s tools are designed to help publications grow quickly and make lots of money – money that is shared with Substack. That design demands responsible thinking about who will be promoted, and how. The company’s defense boils down to the fact that nothing that bad has happened yet. But we have seen this movie before, from Alex Jones to anti-vaxxers to QAnon, and will not remain to watch it play out again.  If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Tuesday"
    },
    {
        "id": "f54ab0f0-d5c6-4434-ba1d-0dcb8fa6e94e",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/15/apple-watch-import-ban",
        "title": "Redesigned Apple Watch not subject to import ban, US officials determine",
        "author": "",
        "published_date": "2024-01-15T18:30:02+00:00",
        "content": "US Customs and Border Protection has determined that Apple’s redesigned Apple Watch is not subject to an import ban, according to a Monday court filing. The import ban, issued by the US International Trade Commission (ITC), applies to Apple’s current Series 9 and Ultra 2 watches and stems from a patent infringement dispute with the medical-monitoring technology company Masimo.   Related: Microsoft beats Apple as most valuable company for first time in two years   The ITC barred Apple’s imports and sales of Apple Watches with technology for reading blood-oxygen levels based on allegations that they infringe two Masimo patents. Apple has included a pulse oximeter feature in smartwatches since its Series 6 Apple Watch in 2020. Apple briefly stopped its sales of its latest Series 9 and Ultra 2 watches in the United States before Christmas due to the ITC decision, though they remained available from other US retailers including Amazon, Best Buy, Costco and Walmart. The ban formally went into effect on 26 December, but Apple convinced a US appeals court to pause the ban the next day and has since resumed selling the watches as it contests the import ruling. Apple had argued that a proposed redesign would moot findings that the watches infringe blood-oxygen reading patents belonging to Masimo. Apple has not publicly described the redesign, which could involve an update to the watches’ software. The customs agency’s decision could be overruled if the ITC disagrees with it. Masimo accuses Apple of hiring away its employees and stealing its pulse oximetry technology to use in Apple Watches. Apple has countersued, calling Masimo’s legal actions a “maneuver to clear a path” for its own competing smartwatch."
    },
    {
        "id": "7c7c4f61-4d81-4e84-8bac-14497c0d3e53",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/15/ai-jobs-inequality-imf-kristalina-georgieva",
        "title": "AI will affect 40% of jobs and probably worsen inequality, says IMF head",
        "author": "",
        "published_date": "2024-01-15T10:53:38+00:00",
        "content": "Artificial intelligence will affect 40% of jobs around the world and it is “crucial” that countries build social safety nets to mitigate the impact on vulnerable workers, according to the head of the International Monetary Fund. AI, the term for computer systems that can perform tasks usually associated with human levels of intelligence, is poised to profoundly change the global economy with advanced economies at greater risk of disruption. Analysis by the IMF, the international lender of last resort, says about 60% of jobs in advanced economies such as the US and UK are exposed to AI and half of these jobs may be negatively affected. But the technology will also help to enhance some humans’ productivity as AI improves their performance, it said.   Related: Horrified by Horizon? Then get ready to be totally appalled by AI   According to the IMF, the safest highly exposed jobs are those with a “high complementarity” to AI, meaning the technology will assist their work rather than displace it entirely. This includes roles with a high degree of responsibility and interacting with people – such as surgeons, lawyers and judges. High-exposure jobs with “low complementarity” – meaning the potential for being displaced by AI – include telemarketing, or cold-calling people to offer goods or services. Low-exposure occupations include dish washers and performers, the IMF said. AI jobs exposure is 40% in emerging market economies – defined by the IMF as states including China, Brazil and India – and 26% for low-income countries, with an overall total of just under 40%, according to the IMF. Generative AI – the term for technology that can produce highly plausible text, images and even voice from simple hand-typed prompts – has risen up the political agenda since the emergence of tools such as the ChatGPT chatbot. Kristalina Georgieva, the IMF managing director, said AI’s ability to affect highly skilled jobs means that advanced economies face greater risks from the technology. She added that in extreme cases some jobs in major economies could disappear. “Roughly half the exposed jobs may benefit from AI integration, enhancing productivity,” wrote Georgieva, in a blogpost accompanying the IMF research. “For the other half, AI applications may execute key tasks currently performed by humans, which could lower labour demand, leading to lower wages and reduced hiring. In the most extreme cases, some of these jobs may disappear.” She added that in most scenarios AI would probably worsen overall inequality across the global economy and could stoke social tensions without political intervention. AI is expected to feature prominently as a topic of discussion at the World Economic Forum in Davos this week, which top executives from the tech industry will attend. “It is crucial for countries to establish comprehensive social safety nets and offer retraining programmes for vulnerable workers,” Georgieva said. “In doing so, we can make the AI transition more inclusive, protecting livelihoods and curbing inequality.” The IMF analysis shows that higher-wage earners whose jobs have high complementarity with AI can expect an increase in their income, leading to an increase in inequality. “This would amplify the increase in income and wealth inequality that results from enhanced capital returns that accrue to high earners,” the IMF report said. “Countries’ choices regarding the definition of AI property rights, as well as redistributive and other fiscal policies, will ultimately shape its impact on income and wealth distribution.” The report said workers in the UK, with its high proportion of graduates, might be better prepared to switch from jobs at risk of displacement to “high complementarity” jobs, although older workers may struggle to adapt and move to new jobs or retrain. Satya Nadella, the chief executive of Microsoft, which is the biggest investor in OpenAI, the US company behind ChatGPT, said on Monday there would be jobs in the future but the question remained about what would be “the shape of these jobs”. Speaking at an event hosted by the Chatham House thinktank in London, Nadella said AI could help with “mid-career transitions”. He added: “I think this is the age where this is about expertise at your fingertips. So if anything, anyone can become an expert in anything because you have the AI assistant helping you.” Last year, the Organisation for Economic Co-operation and Development said the occupations at highest risk from AI-driven automation were highly skilled jobs and represented about 27% of employment across its 38 member countries, which include the UK, Japan, Germany, the US, Australia and Canada. It said skilled professions such as law, medicine and finance were most at risk."
    },
    {
        "id": "32cda1d1-e9ba-416e-92a5-2198b60c3263",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/14/back-uk-creative-sector-or-gamble-on-ai-getty-images-boss-tells-sunak",
        "title": "Back UK creative sector or gamble on AI, Getty Images boss tells Sunak",
        "author": "",
        "published_date": "2024-01-14T13:00:27+00:00",
        "content": "Rishi Sunak needs to decide whether he wants to back the UK’s creative industries or gamble everything on an artificial intelligence boom, the chief executive of Getty Images has said. Craig Peters, who has led the image library since 2019, spoke out amid growing anger from the creative and media sector at the harvesting of their material for “training data” for AI companies. His company is suing an AI image generator in the UK and US for copyright infringement. “When I look at the UK, probably about 10% of its GDP is sitting in the creative industries, whether that’s movies, music, television. I think making that trade-off is risky. If I’m the UK, betting on AI, less than a quarter point of GDP within the UK today, significantly less than the creative industries, is a bit of a perplexing trade-off.” In 2023, the government set out its goal to “overcome barriers that AI firms and users currently face” in using copyrighted material in response to a consultation from the intellectual property office, and it committed to support AI companies “to access copyrighted work as an input to their models”. That was already a step back from an earlier proposal for a broad copyright exception for text and data mining. In a response to a Commons committee on Thursday, Viscount Camrose, the hereditary peer and parliamentary under-secretary of state for artificial intelligence and intellectual property, said: “We will take a balanced and pragmatic approach to the issues that have been raised, which helps secure the UK’s position as a world leader in AI, whilst supporting our thriving creative sectors.” The role of copyrighted work in AI training has come under increased pressure. In the US, the New York Times is suing OpenAI, the maker of ChatGPT, and Microsoft for using its news stories as part of the training data for their AI systems. Although OpenAI has never revealed what data it used to train GPT4, the newspaper was able to get the AI system to spit out verbatim quotes of NYT articles. In a court filing, OpenAI said it was impossible to build AI systems without using copyrighted materials. “Limiting training data to public domain books and drawings created more than a century ago might yield an interesting experiment, but would not provide AI systems that meet the needs of today’s citizens,” the organisation added. Peters disagrees. Getty Images, in collaboration with Nvidia, has created its own image generation AI, trained exclusively on licensed imagery. “I think our partnership speaks exactly counter to some of the arguments that are put out there that you couldn’t have these technologies with a licence requirement. I don’t think that’s the case at all. You need to take different tacks, different approaches, but the notion that there isn’t the capability to do that, that’s just smoke.” Even within the industry, the tide is turning. A dataset of pirate ebooks called Books3, hosted by an AI group whose copyright takedown policy was at one point a video of a choir of clothed women pretending to masturbate their imaginary penises while singing, was quietly removed from download after an outcry from the authors contained in it – but not before it had been used to train, among others, Meta’s LLaMa AI. As well as lawsuits by Getty Images and the New York Times, a host of other legal actions are progressing against AI companies over potential infringement in their training data. John Grisham, Jodi Picoult and George RR Martin were among 17 authors who sued OpenAI in September alleging “systematic theft on a mass scale”, while a group of artists filed a suit against two image generators in January last year, one of the first such cases to enter the US legal system. Ultimately, how courts or even governments decide to regulate the use of copyrighted material to train AI systems may not be the final word on the matter. A number of AI models, both text-generating LLMs and image generators, have been released “open source”, free to download, share and reuse without any oversight. A bar on using copyrighted material to train new systems will not scrub those from the internet, and will do little to prevent individuals from using new material to retrain, improve and re-release them in the future. Peters is optimistic that the result is not a foregone conclusion. He said: “Those that produce and distribute the code, they ultimately have legal entities and they are subject to that. The question of what you’re running on your laptop or your phone may be a bit more of a question, but there’s individual responsibility there.”"
    },
    {
        "id": "f442b32b-2584-417e-83b6-62d55e96a943",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/12/deepfake-video-adverts-sunak-facebook-alarm-ai-risk-election",
        "title": "Slew of deepfake video adverts of Sunak on Facebook raises alarm over AI risk to election",
        "author": "",
        "published_date": "2024-01-12T15:07:22+00:00",
        "content": "More than 100 deepfake video advertisements impersonating Rishi Sunak were paid to be promoted on Facebook in the last month alone, according to research that has raised alarm about the risk AI poses before the general election. The adverts may have reached as many as 400,000 people – despite appearing to break several of Facebook’s policies – and mark the first time that the prime minister’s image has been doctored in a systematic way en masse. More than £12,929 was spent on 143 adverts, originating from 23 countries including the US, Turkey, Malaysia and the Philippines. They include one with faked footage of a BBC newsreader, Sarah Campbell, appearing to read out breaking news that falsely claims a scandal has erupted around Sunak secretly earning “colossal sums from a project that was initially intended for ordinary citizens”. It carries the untrue claim that Elon Musk has launched an application capable of “collecting” stock market transactions and follows with a faked clip of Sunak saying the government had decided to test the application rather than risking the money of ordinary people. The clips then lead to a spoofed BBC News page promoting a scam investment. The research was carried out by Fenimore Harper, a communications company set up by Marcus Beard, a former Downing Street official who headed No 10’s response to countering conspiracy theories during the Covid crisis. He warned that the adverts, which mark a shift in the quality of the fakes, showed that elections this year were at risk of manipulation from a large quantity of high quality AI-generated falsehoods. “With the advent of cheap, easy-to-use voice and face cloning, it takes very little knowledge and expertise to use a person’s likeness for malicious purposes.” “Unfortunately, this problem is exacerbated by lax moderation policies on paid advertising. These adverts are against several of Facebook’s advertising policies. However, very few of the ads we encountered appear to have been removed” Meta, which owns Facebook, has been approached for comment. A UK government spokesperson said: “We are working extensively across government to ensure we are ready to rapidly respond to any threats to our democratic processes, through our defending democracy taskforce and dedicated government teams. “Our Online Safety Act goes further by putting new requirements on social platforms to swiftly remove illegal misinformation and disinformation – including where it is AI-generated – as soon as they become aware of it.” A BBC spokesperson said: “In a world of increasing disinformation, we urge everyone to ensure they are getting their news from a trusted source. We launched BBC Verify in 2023 to address the growing threat of disinformation – investing in a highly specialised team with a range of forensic and open source intelligence (OSINT) to investigate, factcheck, verify video, counter disinformation, analyse data and explain complex stories. “We build trust with audiences by showing how BBC journalists know the information they are reporting, and offer explainers on how to spot fake and deepfake content. When we become aware of fake BBC content we take swift action.” Regulators have been concerned that time is running out to enact wholesale changes to ensure Britain’s electoral system keeps pace with advances in artificial intelligence before the next general election, which is tipped to take place in November. The government has been holding discussions with regulators including the Electoral Commission, which says new requirements under legislation from 2022 for digital campaign material to include an “imprint” for it will go some way to ensuring voters can see who paid for an ad or is trying to influence them. A Meta spokesperson said: ‘“We remove content that violates our policies whether it was created by AI or a person. The vast majority of these adverts were disabled before this report was published and the report itself notes that less than0.5% of UK users saw any individual ad that did go live.  “Since 2018, we have provided industry-leading transparency for ads about social issues, elections or politics, and we continue to improve on these efforts.” "
    },
    {
        "id": "536fc16b-be2f-4648-b524-c4b3c115619d",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/12/transparent-tvs-ai-catflaps-tech-standouts-ces-2024",
        "title": "Transparent TVs, AI catflaps: what were the tech standouts at CES 2024?",
        "author": "",
        "published_date": "2024-01-12T13:00:30+00:00",
        "content": "The next year in technology is to be dominated by upgrades for everything from catflaps to binoculars to cars, devices that disappear in your home including transparent televisions, plus a new era of spatial computing brought in by some very expensive goggles. Those are the predictions from the annual CES tech show in Las Vegas that drew to a close this week. Unlike previous years, the event was not dominated by the big technology and car firms but rather a record-breaking 1,400 startups displaying their prototypes in hopes of catching the eyes of consumers and investors alike. Despite myriad promises to the contrary, many of these novel gadgets may never make it to the shops. But all of them show how technology is progressing and give a glimpse of what’s next. Disappearing devices  After many years of showing prototype see-through displays, LG is finally turning one into a television. The Signature OLED T is a 77in (196cm) transparent TV that looks like a sheet of glass. A black film can be raised at the push of a button to block out what’s behind the TV for movie night. LG said it would ship the TV this year, but would not confirm a price, which is expected to be in the multiple thousands of dollars. South Korean rival Samsung also had transparent microLED panels on show but only in prototype form, plus a different kind of disappearing tech with its new Music Frame that hides a speaker behind a painting, similar to Ikea and Sonos’s Symfonisk frame speaker from 2021. Meanwhile, the new GL.iNet Marble hides the necessary but unsightly internet router in plain sight. The Marble hides behind a photo frame that you can place on your desk or the wall, beaming your broadband around the home without standing out. AI-powered everyday items  While 2023 might have been the year of AI chatbots, 2024 looks like it is going to be the year of AI in everything else. The appropriately named Swiss startup Flappie has an AI-powered catflap that can detect if your furry friend is trying to bring some unfortunate prey into the house. If the camera spots a mouse in the cat’s mouth it blocks the door until your feline friend drops it. Thereis an app so you can remotely control the catflap and track your pet’s comings and goings. It costs 449 Swiss francs (£414) and ships in the spring. AI also found its way into a set of binoculars that can identify more than 9,000 species of birds when pointed at them. The £3,820 ($4,799) Swarovski Optic AX Visio has an integrated 13MP camera that can shoot HD videos too. One of the most intriguing gadgets was the Rabbit r1: a $199 orange rectangle with a screen, camera and a scroll wheel that can control your phone remotely to take the tedium out of your doom-scrolling. Rabbit says its AI understands how apps and websites work so you can command it to buy something, order a taxi or edit a photo and it will do it for you, including launching the right app and pressing all the right buttons. You can even teach it how you do things so it can repeat the process hands-free. Mixed-reality goggles and smart glasses Apple may have tried to steal some of the CES limelight with a rival announcement that its Vision Pro headset would go on sale in the US on the 19 January for $3,499, but it was not the only firm offering goggles this year. Sony announced it was working on a “spatial content creation system”, a mixed-reality headset with fancy hand controllers for 3D content creation. Unlike Meta and Apple’s attempts, Sony is partnering with Siemens to court other industries such as engineering design firms for the “industrial metaverse”. For consumers, Xreal unveiled its latest mixed-reality glasses as something slightly more attainable for consumers. The £699 Air 2 Ultra, which goes on sale in April, are smart sunglasses with displays, speakers and 3D cameras for hand-tracking and other augmented reality features. The glasses plug into phones and computers via cable for power and processing, aiming to compete with Apple’s spatial headset but at a fraction of the price. Cars with AI DJs and chatbots Cars have been stealing the show at CES for the last few years. Honda showed off two concepts as part of a preview of its 0 Series of new electric cars that are scheduled to go on sale in 2026. The Saloon is a low, wide and spacious car with sleek retro-futuristic styling, while the Space-Hub looks more like a minivan inside which passengers sit facing each other. Hyundai showed a concept version of its Ioniq 5 called the Mobion, which has wheels that can turn outwards to 90-degrees so the car can make pivot turns and drive sideways like a crab for easy parallel parking. Hyundai expects the technology to reach consumers by 2028. South Korean stablemate Kia showed off its Platform Beyond Vehicle (PBV) in three different lengths. Everything behind the fixed driving compartment the modular electric PBV can be swapped out so that one vehicle could be a van by day and a taxi by night. Kia said it expects to start making the car-sized PV5 in 2025. AI made its way to cars too. VW demonstrated its ChatGPT integration with voice assistant, so drivers can talk to the chatbot and have it read out summarised articles and similar. Meanwhile, Mercedes partnered with the musician and producer will.i.am to put his new company’s AI DJ tech in its cars. Mbux Sound Drive creates an in-car soundscape that reacts to what is happening such as accelerating, braking, turning and idling, bringing in the lyrics to familiar songs when you hit the accelerator. Attendees described it sounding like they were driving through a film soundtrack. Other grabby gadgets There were plenty of smaller gadgets generating buzz at the show this year. The Clicks creator keyboard adds a Blackberry-style physical keyboard to the bottom of an iPhone and costs $139 for those who hate touch-screen typing. The French health-tech firm Withings was back with the BeamO, which combines a thermometer, blood oxygen monitor, ECG and stethoscope into one small stick. Designed to be an app-connected health check system, the gadget will cost £220 but is awaiting clinical validation and regulatory clearance. Meanwhile, L’Oreal launched a smart hair dryer that uses infrared heat and fan to save energy and dry hair faster. The AirLight Pro has a connected app and will hit salons in April before a summer launch for consumers at an undisclosed price, It is likely to compete with Dyson’s £280-plus Supersonic. LG showed off its “AI agent” bipedal robot, with wheels, which can autonomously patrol the home, greet people at the door and generally function as a roving smart home hub. It looks like something out of the Jetsons. Samsung’s version was the second-generation Ballie robotic ball with an in-built projector for instant TV anywhere. Meanwhile, HL Mando’s Parkie is a robot that promises to slide under your car, pick it up and park it for you, which could be the saviour of parallel parkers everywhere."
    },
    {
        "id": "356eb982-9a3c-440b-b604-0a4e4b8524a7",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/12/tesla-pauses-german-production-because-of-red-sea-shipping-attacks",
        "title": "Tesla pauses German production after Red Sea shipping attacks",
        "author": "",
        "published_date": "2024-01-12T08:58:48+00:00",
        "content": "The electric car manufacturer Tesla is to halt most production at its factory near Berlin for two weeks because of delays in deliveries of parts because of attacks on ships in the Red Sea. Shipping delays in the Red Sea, caused by attacks by Iranian-backed Houthi militants, has caused Tesla to suspend most production at its German factory from 29 January to 11 February. The carmaker, whose chief executive is Elon Musk, has said the pause could lead to about 5,000 to 7,000 cars not being built. Disruption through the important shipping route to the Mediterranean, which has forced companies to use the alternative and much longer route around the southern tip of Africa, is starting to affect global trade, which dropped 1.3% in December. “The armed conflicts in the Red Sea and the associated shifts in transport routes between Europe and Asia via the Cape of Good Hope are also having an impact on production in Grünheide,” said Tesla, the second-biggest seller of electric cars. “The considerably longer transportation times are creating a gap in supply chains.” Last month, Tesla was overtaken by its Chinese rival BYD as the world’s top-selling electric carmaker. BYD, which stands for Build Your Dreams, has been backed by the US investment billionaire Warren Buffett since 2008. Susannah Streeter of the analysts Hargreaves Lansdown said the Berlin pause was “a fresh blow to Tesla’s production targets and comes amid fierce competition from Chinese manufacturers … The China-based automotive giant Geely, which owns Volvo and Lotus, has also flagged that there would be a delay to deliveries of EV models in Europe.” The Red Sea route, which gives ships access to the Suez Canal, accounts for about 12% of global maritime traffic. Tensions escalated on Thursday when the US and UK launched air and missile strikes in Houthi-controlled areas of Yemen aimed at halting attacks on ships traversing the Red Sea. The US said it had launched strikes on more than 60 targets at 16 Iranian-backed Houthi militant locations, including command and control nodes, munitions depots, launching systems, production facilities and air defence radar systems. The military response followed a Houthi drone and missile attack aimed at a fleet of American and British warships in the southern Red Sea. Oil prices rose 2% on Friday after the strikes by the US and UK, with Brent crude at almost $79 a barrel. The attack added to concerns about the potential impact a broader conflict in the Middle East might have on oil supplies from the region. A report by the German economic institute found that the number of containers travelling daily through the Red Sea fell by 60% from 500,000 in November to 200,000 last month. Last week, Maersk, the shipping company responsible for moving 20% of global trade each year, diverted its ships around the southern tip of Africa after two of its vessels were attacked in December. Vincent Clerc, Maersk’s chief executive, said on Thursday the Red Sea disruption could last for months. The attacks by Houthi militants were “brutal and dramatic” and could lead to further inflation across the global economy, he added. Retailers have said problems in the Red Sea could lead to inflation and availability problems if container ships keep having to divert around Africa. Marks & Spencer said the availability of some items, mainly clothing, may be affected in February and March. Alcohol deliveries could also be disrupted."
    },
    {
        "id": "909e7918-2b7a-4815-9d9c-ec90d2e28357",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/11/george-carlin-ai-comedy",
        "title": "George Carlin’s daughter lambasts AI-generated video of late comedian",
        "author": "",
        "published_date": "2024-01-11T18:22:21+00:00",
        "content": "Kelly Carlin, the American radio host and daughter of the late comedian George Carlin, has criticized the release of a new comedy special featuring an AI-generated version of her father, who died in 2008 due to heart failure. “My dad spent a lifetime perfecting his craft from his very human life, brain and imagination. No machine will ever replace his genius. These AI-generated products are clever attempts at trying to recreate a mind that will never exist again,” Kelly Carlin wrote in a series of tweets on X.   Related: A funny thing happened on the way here: comedians get personal about their backstage dramas   The tweets arrived on the heels of Dudesy, a YouTube channel and podcast hosted by Will Sasso and Chad Kultgen, releasing an hour-long comedy special called I’m Glad I’m Dead, on Tuesday. The channel, which features AI, is self-described as the “first of its kind media experiment”. The comedy special opened up with a message, saying: “Hello! My name is Dudesy and I’m a comedy AI … I just want to let you know very clearly that what you’re about to hear is not George Carlin. It’s my impersonation of George Carlin that I developed in the exact same way a human impressionist would. I listened to all of George Carlin’s material and did my best to imitate his voice, cadence, attitude, as well as the subject matter that I think would interest him today.” It continued: “Think of it like Andy Kaufman impersonating Elvis, or like Will Ferrell impersonating George W Bush,” the message added. The comedy special went on to explore a slew of topics including religion, mass shootings, Donald Trump, billionaires, technology and identity. At one point, the special focused on Elon Musk, with the AI-generated George Carlin saying: “If you’re tired of hearing about Elon Musk fathering even more children and planting microchips, stop buying Teslas for a year. Company goes under, Musk goes away. Stop using Twitter for even a month. Company goes under, Musk goes away.” The AI-generated George Carlin added: “But if you want to drive with your head in your ass and blame it on the car, you have to have a Tesla. And if you want to argue with strangers about complex geopolitical situations using only cat memes, you have to have Twitter.” Following its release, Kelly Carlin lambasted it, saying on X: “Let’s let the artist’s work speak for itself. Humans are so afraid of the void that we can’t let what has fallen into it stay there. Here’s an idea, how about we give some actual living human comedians a listen to? But if you want to listen to the genuine George Carlin, he has 14 specials that you can find anywhere.” In a follow-up tweet, Kelly Carlin called on the adult children of other late comedians including Robin Williams’s daughter Zelda Williams and Joan Rivers’s daughter Melissa Rivers, as well as the official X account of Garry Shandling, the comedian who died in 2016. “We should talk. They’re coming for you next,” she wrote. Tuesday’s AI-generated use of George Carlin, widely regarded as one of the most influential comedians of all time, is not the first time creators have turned to AI to generate fake content. Last April, former NFL champion Tom Brady threatened to sue Dudesy’s creators after they released an AI-generated comedy special – which they later took down – of a fake Brady telling jokes. According to Brady’s attorneys, the comedy special, which was titled It’s Too Easy! A Simulated Hour-Long Comedy Special, “blatantly violated” Brady’s rights, CBS Sports reports. In 2021, a documentary on the late celebrity chef and TV host Anthony Bourdain sparked backlash from his ex-wife Ottavia Bourdain and others after film-makers used AI to synthetically create a voiceover reading of an email by Bourdain himself. In an interview with the New Yorker, the documentary’s director, Morgan Neville, said he had contacted a software company and provided about a dozen hours of recordings to generate an AI model of Bourdain’s voice. “I certainly was NOT the one who said Tony would have been cool with that,” Ottavia Bourdain tweeted after the documentary’s release."
    },
    {
        "id": "7f394a2e-8a85-4537-a001-4bfece1742e8",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/11/bitcoin-etf-approved-sec-explained-meaning-securities-regulator-tweet",
        "title": "The SEC has approved bitcoin ETFs. What are they and what does it mean for investors?",
        "author": "",
        "published_date": "2024-01-11T03:47:12+00:00",
        "content": "The US securities regulator has approved the first US-listed exchange traded funds (ETF) to track bitcoin, in a watershed moment for the world’s largest cryptocurrency and the broader crypto industry. The announcement came at the end of a tumultuous 24 hours for the popular cryptocurrency, which saw a tweet sent from the account of the Securities and Exchange Commission (SEC) announcing the approval of the long-awaited ETFs on Tuesday, leading the price of bitcoin to spike by more than $1,000. Soon after, the SEC said its account had been “compromised” and that the tweet was “unauthorised”. By Wednesday however, the SEC had approved the ETFs – this time for real – adding that it remained sceptical about cryptocurrencies.   Related: SEC says ‘compromised’ account to blame for tweet approving Bitcoin ETF   What has been approved? The SEC has given the green light to 11 ETFs for bitcoin in the US, opening the door to cryptocurrencies to many new investors who don’t want to take the extra steps involved in buying actual bitcoin. An ETF is an easy way to invest in assets or a group of assets without having to directly buy the assets themselves. For example, the SPDR Gold Shares ETF allows anyone to invest in gold without having to find a place to store a bar or protect it. ETFs can also be easily traded on stock exchanges. Since bitcoin’s inception, anyone wanting to own one would either have to adopt a digital wallet or open an account at a crypto trading platform like Coinbase or Binance. Cryptocurrency advocates say the development will thrust the once niche and nerdy corner of the internet even further into the financial mainstream. The decision to approve the ETFs is a major win for huge fund managers like BlackRock, Fidelity Investments and Invesco who will manage the funds – and have pushed hard to get the SEC to approve them. Some products are expected to begin trading as early as Thursday, kicking off a fierce competition for market share. What has the SEC said? Despite approving the new ETFs, the SEC said it was still deeply skeptical about cryptocurrencies and that its decision did not mean it approves or endorses bitcoin. “Investors should remain cautious about the myriad risks associated with bitcoin and products whose value is tied to crypto,” said Gary Gensler, the agency’s chairman. Other commissioners expressed alarm that the SEC agreed to approve the funds. “I am concerned that these products will flood the markets and land squarely in the retirement accounts of US households who can least afford to lose their savings to the fraud and manipulation that appears prevalent in the spot bitcoin markets,” Commissioner Caroline Crenshaw said in her dissent. What does this mean for the price of bitcoin? After nearly two years of turmoil that has seen the price of bitcoin plummet and the failure of several crypto firms, Wednesday’s announcement will come as good news to many investors in the crypto market. The greenlight from regulators had been anticipated for several months and the price of bitcoin has jumped about 70% since October as crypto investors speculated the broad use of bitcoin ETFs would drive up demand for the cryptocurrency. The price had sunk as low as $16,000 in November 2022 after the bankruptcy of the crypto exchange FTX. It was trading at $46,500 in the hours after the SEC announcement. Standard Chartered analysts this week said the ETFs could draw $50bn to $100bn this year alone, potentially driving the price of bitcoin as high as $100,000. Others have said inflows will be closer to $55bn over five years. Other analysts have been more cautious in their predictions, saying that ETFs may actually help stabilise crypto prices by broadening their use and potential audience. Most, however, remain concerned that the broad use of crypto ETFs could put too much risk and volatility into Americans’ retirement accounts – the price of bitcoin is known to fluctuate wildly, often without warning or explanation. “The notorious price volatility of bitcoin … could expose mainstream investors to a less familiar spectrum of investment risks,” said Yiannis Giokas, senior director of Moody’s Analytics. The price of ethereum, the second-most popular cryptocurrency, has also risen on speculation that fund managers will create ETFs around it. Reuters and the Associated Press contributed to this report"
    },
    {
        "id": "955cefa3-06ac-4947-9b16-e4ae6c5e9ecd",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/10/openai-launches-gpt-store-customized-chatbots",
        "title": "OpenAI debuts GPT Store for users to buy and sell customized chatbots",
        "author": "",
        "published_date": "2024-01-11T01:08:27+00:00",
        "content": "OpenAI on Wednesday launched its GPT Store, a marketplace where paid ChatGPT users can buy and sell specialized chatbot agents based on the company’s language models. The company, whose wildly popular product ChatGPT helped kickstart the boom in AI, already offers customized bots through its paid ChatGPT Plus service. The new store will allow users to offer and monetize a broader range of tools.   Related: ‘Impossible’ to create AI tools like ChatGPT without copyrighted material, OpenAI says   Through the new models, chatbot agents could be developed with their own personalities or themes, including models for salary negotiating, creating lesson plans and developing recipes. In a blogpost announcing the launch, OpenAI said more than 3m custom versions of ChatGPT have already been created. It also said it plans to highlight useful GPT tools within the store every week. The store has been compared with Apple’s App store, fostering new development in the AI space from a wider range of users. Meta offers chatbots with differing personalities in a similar offering. The GPT store was originally slated to open in November before its launch was delayed by internal upheaval at the company late last year when OpenAI’s board fired Sam Altman as CEO. He returned to the role a week later after a near-mass exodus by employees. The company said in a blogpost it would launch a revenue-sharing program in the first quarter of this year, through which builders will be paid based on user engagement with their GPTs. Details of that have yet to be shared. In an email to developers for the platform last week, OpenAI told users to ensure that their chatbots meet usage polices and GPT brand guidelines. The company highlighted several products already on offer in a press release accompanying the launch, including ones from the design app Canva and the hiking app AllTrails. The new store is available to subscribers of its premium services ChatGPT Plus and Enterprise, as is a new subscription tier called Team, which costs $25 each month for each user. Team subscribers can also create custom GPTs for team needs. During it inaugural demo day for developers, Altman offered to cover the legal costs for developers who may run afoul of copyright law in creating products based on ChatGPT and OpenAI’s technology. OpenAI itself has been sued multiple times for alleged copyright infringement for using copyrighted text to train its large language models. Altman said in early January that it would be “impossible” to create ChatGPT without including copyrighted material in the training corpus of the artificial intelligence. ChatGPT, the company’s flagship product, was released in November 2022 to little fanfare but quickly caught on with consumers, accruing 100 million users in a matter of months. OpenAI also makes the Dall-E image generation software, though it is not clear as yet whether the store will allow for custom image bots or solely bespoke chatbots."
    },
    {
        "id": "12c48b06-eac2-4b21-9da9-d22d3657a8c7",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/09/meta-facebook-instagram-sexually-explicit-content-minors-walmart-match",
        "title": "Facebook and Instagram ran content sexualizing minors next to corporate ads, lawsuit says",
        "author": "",
        "published_date": "2024-01-10T15:30:06+00:00",
        "content": "Facebook and Instagram have been profiting from placing corporate adverts from companies such as Walmart and Match Group next to content potentially promoting child sexual exploitation, a legal filing alleges. The accusation is the latest in an explosive lawsuit initiated in December by Raúl Torrez, the New Mexico attorney general, against Meta claiming the company “enabled adults to find, message and groom minors” for sexual exploitation. The suit follows a Guardian investigation in April, which revealed that the tech giant is struggling to prevent people from using its platforms to buy and sell children for sex.   Related: Instagram and Facebook will hide more harmful content from teenagers   “New evidence indicates Meta officials are duping corporate advertisers and permitting sponsored content to appear alongside deeply disturbing images and videos that clearly violate Meta’s promised standards,” said Torrez in a statement. “Mr Zuckerberg and Meta are refusing to be honest and transparent about what is taking place on Meta’s platforms.” The legal complaint, reviewed by the Guardian, quotes correspondence among Meta, Walmart and Match, the owner of the dating apps Tinder and Hinge. The advertisers objected to their material being placed next to graphic and potentially illegal content, according to the filing. In early November, Match notified Meta that ads for its dating apps had appeared alongside “disturbing” content on Reels, short videos posted by users on Facebook and Instagram, according to the complaint. The company allegedly said it believed some of the content in the Reels “is clearly promoting illegal and exploitative businesses” and included provocative images of young girls. Match adverts were also featured in a Facebook Group titled “Only women are slaughtered”, which showed graphic films of women being murdered, according to the complaint. “We need to quickly figure out how we stop this from happening on your platforms,” a representative for Match wrote, according to one email quoted in the filing. When Meta failed to address these concerns, Match’s CEO, Bernard Kim, allegedly wrote to Zuckerberg himself, stating: “Meta is placing ads adjacent to offensive, obscene – and potentially illegal – content, including sexualization of minors and gender-based violence.” Kim’s letter also highlighted that Match spent millions of dollars on advertising on Meta but “our ads are being serviced to your users viewing violent and predatory content”, the complaint says. Zuckerberg did not respond to the letter, the legal complaint says. Meanwhile, in October, Walmart emailed Meta with concerns that the tech giant’s “level of attention/consideration” to brand safety issues “has disappeared”. Meta confirmed that Walmart advertisements were being displayed on unapproved channels, responding that “there is some minimal exposure to placements that you’ve not approved”, according to the complaint. Walmart’s marketing representatives allegedly continued to question why their adverts were running next to illicit content, eventually becoming so frustrated that one called Meta’s response to the problem “unacceptable”. “Candidly, we were disappointed that your team seemed more focused on getting a press statement right than on addressing this problem,” one representative wrote, according to the complaint. Torrez wrote in the complaint: “The experiences of Match and Walmart are emblematic of a larger problem … Meta’s claims regarding the content of its platforms are false and that its tools are ineffective.” In response to the filing, Walmart said in a statement: “We take brand safety issues extremely seriously, and protecting our customers and communities will always be a top priority.” Meta and Match did not respond to request for comment by press time. The filing also contains new evidence that child predators are allegedly able to find victims through Instagram. It includes excerpts of users allegedly discussing how to lure minors into engaging with them, highlighting the absence of controls to prevent unknown adults from messaging minors on the social network. A former Instagram employee testified in November before Congress that his own daughter had received unwanted online advances and that, when he notified senior Meta leadership, he was ignored. Before filing the lawsuit, investigators at the New Mexico attorney general’s office conducted their own investigation into child sexual exploitation taking place on Meta’s platforms. According to the lawsuit, they “found numerous posts and accounts on Instagram that depicted or promoted choking, slapping, tying up, engaging in sex with, and otherwise abusing little girls”. According to the filing, all of the images and videos the investigators found were reported to Meta. According to the suit, the company removed only half of the reported images. “Investigators found that content that was removed frequently reappeared or that Meta recommended alternative, equally problematic content to users – demonstrating both that Meta is capable of identifying this content but incapable of effectively dealing with it,” the lawsuit states."
    },
    {
        "id": "0ac44696-cd57-43fc-b8c9-59694fef50ab",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/11/hyperverse-stephen-harrison-crypto-investors",
        "title": "‘I do feel bad about this’: Englishman who posed as HyperVerse CEO says sorry to investors who lost millions",
        "author": "",
        "published_date": "2024-01-10T14:00:35+00:00",
        "content": "The man who posed as the chief executive of the collapsed crypto scheme HyperVerse has confirmed he was paid to act the part, receiving 180,000 Thai baht (about A$7,500 or £4,000) over nine months and a free suit as payment. Stephen Harrison, an Englishman living in Thailand who posed as chief executive Steven Reece Lewis for the launch of HyperVerse in late 2021 and early 2022, has told Guardian Australia he was “shocked” to learn the company had presented him as having fake credentials to promote the scheme. He said he felt sorry for those who had lost money in relation to the scheme – which he said he had no role in – an amount Chainalysis estimates at US$1.3bn in 2022 alone. “I am sorry for these people,” he said. “Because they believed some idea with me at the forefront and believed in what I said, and God knows what these people have lost. And I do feel bad about this.   Related: HyperVerse crypto promoter ‘Bitcoin Rodney’ arrested and charged in US    “I do feel deeply sorry for these people, I really do. You know, it’s horrible for them. I just hope that there is some resolution. I know it’s hard to get the money back off these people or whatever, but I just hope there can be some justice served in all of this where they can get to the bottom of this.” He said he wanted to make clear he had “certainly not pocketed” any of the money lost by investors. Harrison, who at the time was a freelance television presenter engaged in unpaid football commentary, said he had been approached and offered the HyperVerse work by a friend of a friend. He said he was new to the industry and had been open to picking up more work and experience as a corporate “presenter”. “I was told I was acting out a role to represent the business and many people do this,” Harrison said. He said he trusted his agent and accepted that. After reading through the scripts he said he was initially suspicious about the company he was hired to represent because he was unfamiliar with the crypto industry, but said he had been reassured by his agent that the company was legitimate. He said he had also done some of his own online research into the organisation and found articles about the Australian blockchain entrepreneur and HyperTech chairman Sam Lee. “I went away and I actually looked at the company because I was concerned that it could be a scam,” Harrison said. “So I looked online a bit and everything seemed OK, so I rolled with it.” Lee founded the Australian bitcoin company Blockchain Global with Ryan Xu, which collapsed in 2021 owing creditors $58m. Lee and Xu have been referred to the Australian Securities and Investments Commission for potential breaches of the Corporations Act in relation to Blockchain Global, but Asic has said it does not intend to take action at this time. Harrison said he had been told in his second recording that he needed to use a fake name. “I asked why, and they were like, ‘Well, you know, you’re an actor, you’re acting the role you’re presenting the business,’ and … my agent said, ‘Many people do this in the business. This is perfectly normal.’” In the videos, Harrison spoke generally about the opportunities offered for people who invested in the HyperVerse but did not explicitly ask for funds or make any claims about potential returns. He said he had only learned of the fake credentials that accompanied the launch presentation he featured in through Guardian Australia’s investigation that confirmed none of the organisations cited had any record of a Steven Reece Lewis. This included claims that he held degrees from Leeds and Cambridge universities and had worked for more than 10 years in the fintech industry. “I was absolutely shocked at what I saw,” Harrison said. “I never went online and checked about Steven Reece Lewis. I looked on YouTube occasionally, way back when they put the presentations up, but apart from that I was detached from this role.  “When I read that in the papers, I was like, blooming heck, they make me sound so highly educated – I have GCSEs, I’m certainly not on that level. They painted a good picture of me but they never told me any of this.” He said all of the other claimed qualifications and work experience cited in the presentation were also false. After Guardian Australia’s report, a US-based YouTuber, Jack Gamble, revealed Harrison’s true identity. This prompted Harrison to come forward to share what he knew.   Related: Investors lose millions as crypto schemes operate unchecked in Australia   According to Harrison’s contract for the role, an unsigned copy of which has been seen by Guardian Australia, he was employed as “Presenter Talent” by an Indonesian-based talent agency called Mass Focus Ltd. Guardian Australia can find no record of a company of this name on the Indonesian company register. Harrison said that aside from scripted online interactions he never spoke with anyone from HyperVerse, dealing solely with his local contact. He emphasised that he had no contact at any point with Lee or Xu. The terms of the contract were for a three-month renewable retainer, during which he would be paid 20,000 Thai baht a month for a maximum of six hours work a month. Harrison said most months he worked for only one to two hours, with three videos in one month the most work he undertook. He was sent a script before each recording. At one point, he said, there was a plan to fly him to the US for an event, but this had never happened. He was also provided with one wool and cashmere suit, two business shirts, two ties and a pair of shoes for the part. He said the filming had taken place in makeshift studios in Bangkok, including at an apartment in the Ekkamai district and later at the Empire tower in Sathon. Harrison said he had no oversight of the Twitter account run under the fake chief executive’s name, nor the messages of support for the scheme obtained from a clutch of celebrities, including the Apple founder Steve Wozniak and the actor Chuck Norris. He said that after his contract was twice renewed, running from September 2021 to June 2022, he requested that the Twitter account be shut down. The date of his final payslip coincided with the last date the Twitter account was active."
    },
    {
        "id": "e24094f9-14e8-4492-8a9b-ebda48aa7e8b",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/09/microsoft-investment-openai-chatgpt-european-union-eu-merger-investigation",
        "title": "Microsoft’s investment in OpenAI may face EU scrutiny, officials say",
        "author": "",
        "published_date": "2024-01-09T16:01:39+00:00",
        "content": "Microsoft’s multibillion-dollar investment in the ChatGPT developer OpenAI could face a merger investigation in the European Union, officials have said. Microsoft is the largest minority investor in OpenAI Global LLC, a “capped profit” subsidiary company that is controlled by OpenAI Inc, the non-profit majority owner of the organisation. Its investment, given in the form of cloud-computing credits as well as cash, officially gives it no control of the company itself, but the possibility of a maximum of a 100-times return on its capital. The European Commission said on Tuesday it was “checking whether Microsoft’s investment in OpenAI might be reviewable under the EU merger regulation”. OpenAI’s unusual corporate structure was thrust into the limelight last year, when its chief executive, Sam Altman, was ousted and then reappointed in a bitter struggle with the non-profit’s board. Despite having no legal sway over the outcome, Microsoft intervened heavily in the dispute, in part by offering Altman and any other OpenAI staff a job in its own organisation. Since the conflict, OpenAI’s own description of Microsoft’s involvement has changed. In November, a chart on the company’s website detailing its structure described Microsoft as a “minority owner”. That chart was then quietly updated to read “minority economic interest” instead.   Related: UK watchdog to examine Microsoft’s partnership with OpenAI   Before that update, the UK’s Competition and Markets Authority said in December it was gathering information about the precise relationship between Microsoft and OpenAI. The EU’s investigation into AI and virtual worlds will also involve “looking into some of the agreements that have been concluded between large digital market players and generative AI developers and providers”, to investigate their impact on market dynamics. Last month Dragoș Tudorache, an MEP and the lead on the artificial intelligence committee in the European parliament, told the Guardian it was vital the EU did not make the mistakes of the 2000s when emerging tech companies, including Facebook, were allowed to dictate the terms of their service. Without regulation on the content or effective control of acquisitions and mergers, they were able to dominate the market by snapping up companies and also competing with traditional media without paying for content, undermining the business models of rivals."
    },
    {
        "id": "6eea52d2-f0a0-436c-9c52-1626b80274ee",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/09/amazon-workers-birmingham-fulfilment-centre-uk-strike-gmb-union",
        "title": "Amazon workers at new Birmingham site to go on strike over pay",
        "author": "",
        "published_date": "2024-01-09T15:22:14+00:00",
        "content": "Amazon faces fresh industrial action in the UK, after members of the GMB union at a new fulfilment centre in Birmingham voted to strike over pay and conditions later this month. The GMB says “up to 100” workers will take part in the strike at the warehouse, which opened in October – a small fraction of the total workforce – but it hopes to use the action to boost the profile of the dispute and recruit more supporters. GMB members at the site have chosen 25 January as their first strike day, exactly one year after a stoppage by workers at Amazon’s Coventry warehouse, in what was the first action of its kind against the company in the UK. Rachel Fagan, a local GMB organiser, said: “The industrial chaos Amazon faces isn’t going to disappear; it’s growing every day. One year on from the first strike day the message from GMB members at Amazon is the same: recognise our union and end poverty pay.” Membership in Coventry has increased significantly since that first strike, to more than 1,200, according to the GMB, which has also secured a mandate for industrial action at a site in Rugeley, Staffordshire. The GMB had hoped to win formal recognition in Coventry, but withdrew its application after the regulator, the central arbitration committee, backed Amazon’s claim that the workforce had swelled to 2,700. At the time, the union accused Amazon of drafting in more than 1,000 new workers to skew the decision. The company insisted any new recruitment was due to the demands of the business.   Related: ‘What do we want? £15!’ Hundreds join Amazon picket line for Black Friday strike   Amazon staff in Coventry have taken 28 days of industrial action since the start of last year, culminating in a Black Friday rally in November at which they were joined by fellow Amazon workers from Italy, Germany and the US. The basic rate of pay has increased several times over that period and is due to rise to £12.30 an hour in April, although that falls well short of the £15 an hour the strikers are demanding. Amazon denies the increases have any link with the industrial action, insisting it has always reviewed pay regularly. An Amazon spokesperson said: “Just 19 GMB members out of our 2,000 Sutton Coldfield fulfilment centre colleagues have voted in favour of industrial action. It will cause zero disruption for our customers. “We regularly review our pay to ensure we offer competitive wages and benefits. We also work hard to provide great benefits, a positive work environment and excellent career opportunities.” "
    },
    {
        "id": "6d92e246-cfbe-406d-8351-22082e5d665a",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/09/ai-wars-dating-social-media-fraud-eu-crime-artificial-intelligence-europol",
        "title": "AI fuelling dating and social media fraud, EU police agency says",
        "author": "",
        "published_date": "2024-01-09T14:00:02+00:00",
        "content": "Artificial intelligence, combined with wars in Ukraine and the Middle East, is fuelling a boom in fraud on dating and social media apps, officials at Europol have said. Speaking to the Guardian, the agency’s top financial crime experts said scripts generated by artificial intelligence enable criminals to target multiple victims at once. Sebastian Bley, the head of the Europol economic crime team in The Hague, said: “There is a trend of more and more cases of people saying ‘I’m a doctor in a war zone’ and asking for funds to be transferred out of that zone. They say they also need to get the family out.” Burkhard Mühl, the overall head of the financial and economic crime unit at Europol, said: “It can involve tens of thousands of euros in one case.” They also warned of a rise in “bogus boss” scams, with fraudsters spinning an elaborate web of fake websites, fake CVs and investor profiles to target a gamut of victims ranging from personal investors to tax authorities. The impact can be devastating, resulting in the loss of life savings and in some cases suicide. The EU’s law enforcement agency, whose mission is to prevent and combat international and organised crime, analyses trends in fraud. It said AI is making it easier for scammers to increase their success rate, and there is an increase of “abuse using large language tools like ChatGPT,” Mühl said. “With large language tools you can write thousands of tailored messages in different languages with different targets, different stories, and you can do that with your laptop from wherever,” he added. The rise in dating fraud comes less than two years after a Netflix documentary, The Tinder Swindler, brought global attention to a man who posed as the son of a wealthy diamond mogul and conned several women out of money, one losing nearly €250,000 (£215,000) in loans. No matter how compelling their story is, “never send money to a person you haven’t personally met”, Mühl said. Bley and Mühl spoke of the devastating consequences and in some cases “revictimisation”, when fraudulent financial advisers offered help to get money back. “People sometimes lose the life savings, investing in all kinds of products. We don’t have statistics really drilling into those questions in detail but we hear from our investigators about cases that are quite dramatic – people jumping out of the window because they lost pension savings in investment schemes,” Mühl said. The best solution is reporting the crime to the police, Bley and Mühl said, but they believe investor fraud is underreported because of the shame people feel. “It’s not only the financial damage that hits the victims. It is also psychological damage,” Mühl added. General investor fraud through text message scams is said to have risen by 40% in the UK since the launch of ChatGPT, according to British authorities.   Related: The chilling rise of AI scams – podcast   AI is bringing previously unseen economies of scale to the world of fraud, Bley said. He added that traditional fraud takes a lot of time to make things up and lure victims into a drawn-out narrative until they establish trust. AI can cut this out, with industrial-scale fishing for victims possible. “If you personally meet somebody, you have to invest a lot of time making things up and to find the right victim,” Bley said. “Online, it’s easier to target a lot of people once you have feedback from somebody on the platform, something to continue with engagement. If you see it’s not going to go anywhere you just turn to the next one. It’s much easier to approach people.” Europol said it believed Europe was being targeted by criminals operating outside the EU because of the “high standards of living” and because “people have money to invest”. Last year authorities in India busted a bitcoin racket being operated out of a call centre in Delhi that was targeting Austrians. Fraudsters posed as Interpol investigators and called Austrians to advise them they were the subject of police investigations but if they went to a bitcoin ATM to exchange cash the inquiries would end. In a recent “bogus boss” case, criminals managed to steal €50m from a variety of companies and “major corporations that you would not consider to be victims”, Mühl said. The criminals were “very clever”, learned about company structures, set up fake websites and somehow were able to get in touch with investors, offering them “discount” shares."
    },
    {
        "id": "d07cd22a-06b0-4005-9575-93a2f1ff630a",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/09/is-the-consumer-electronics-show-still-relevant",
        "title": "TechScape: Is the Consumer Electronics Show still relevant?",
        "author": "",
        "published_date": "2024-01-09T11:45:13+00:00",
        "content": "The Consumer Electronics Show (CES), which starts today in Las Vegas, is an odd beast. It is the biggest technology event of the year, a sprawling conference that spills over multiple casinos and convention centres to dominate a city that is hard to overshadow. But for the better part of a decade it has been an afterthought for some of the world’s biggest businesses, led by Apple realising that if you can get the press to come to you, you don’t need to risk burying your product launches under hundreds of competing newslines. The result is that CES is no longer where you see the future, but where you learn how that future will get copied into a thousand cheap plastic knockoffs. There are, of course, exceptions. The TV industry, which remains stubbornly uncolonised by big tech, continues to present its cutting-edge designs. LG showed off its transparent TV. Samsung, more prosaically, demonstrated a glare-free OLED, while upstart projector company Xgimi is selling a three-in-one projector, bluetooth speaker and … ceiling lamp. PC makers are taking a similar approach, using the show floor to demonstrate both new concepts and the boring iterations on actual laptops that people will buy. Normally, the car people are out in huge numbers as well, showing off concept cars that blur the line between computer and driver and actual production vehicles that have a dumb touchscreen where there should be a dial to control the heating. This year, though, ongoing strikes at the big three American manufacturers have forced them to pull out, with Honda, Hyundai and BMW unlikely to completely fill the void. On top of those stalwarts, the show has waxed and waned over the years. The rise of the smartphone was the beginning of the bad times for CES. The biggest developers choosing to release their flagship devices at their own shows, leaving the conference the home of the also-rans and budget devices. Worse still, the smartphone all but killed the “gadget” stone dead. The bread and butter of CES was once the neat little tool that did one thing well. But it’s harder to set up a stand at a convention centre for your tool when it’s a 99p app. Still, once the sector got large enough to be an ecosystem in its own right, CES revived accordingly: the gadget became a bluetooth accessory, a smart home trinket, or a toy drone. This year, we could be seeing the beginning of a similar boom thanks to the growth of AI. Like smartphones, AI itself isn’t very CES-friendly. Even if OpenAI wanted to reveal GPT5 on stage in Las Vegas, it would be stretching the definition of “consumer electronics”. But AI is also getting built into existing products at a steady clip, reviving the “gadget” for a new era. Want an AI-powered bird feeder that sends you pictures of the various species at your table? CES has you covered. An iPhone dock that will pan around the room to keep you in shot? That’s there, too. A Volkswagen with built-in ChatGPT? That’s obviously happening. A domestic robot who’ll pour you a glass of wine and mop up any spills with a handy tea towel? Well, not yet. But at the rate the field is progressing, I wouldn’t bet against it for 2025. Side hustling One of the more satisfying things to have happened over the long decade I’ve been covering technology is watching governments slowly realise that they actually have power and can use it to do things. It’s a stunning insight, I know. And yet it is increasingly common for an industry to be overturned by, effectively, a state waking up one day and realising that it doesn’t really like the status quo. Case in point: HMRC now requires digital marketplaces to collect information on their users’ revenue. From our story:  Countless sellers who have never declared their income from these sites could soon be on HMRC’s radar and face the prospect of large bills, or even fines, if they do not adhere to the law. Dawn Register, head of tax dispute resolution at accountancy firm BDO, says HMRC could already request the information from UK-based digital platforms on an ad hoc basis, “but the new rules, coming into force from January 2024, mean that this information will start to flow automatically – and globally”.  The change snuck in below the radar for many, and has been a rude awakening for some who thought their side-hustle was tax exempt when it was actually more like tax evasion. (If this has affected you, our community team wants to hear from you.) But it’s yet more evidence, if it were needed, that governments have long underestimated their authority over the internet. It turns out that the rise of online marketplaces for selling goods and services does not, in fact, necessarily lead to a wave of transactions out of the reach of the taxman. Instead, it just leads to a decade-long gap as governments around the world slowly realise they can do something about it. Substack We’re all newsletter fans here, right? So I’d be remiss if I didn’t bring up the company’s latest battle, after its “chief writing officer” and co-founder Hamish McKenzie declared that it wouldn’t demonetise nazis on the platform. From our story:  In a note on the site published in December, the chief writing officer, Hamish McKenzie, said the firm “doesn’t like Nazis”, and wished “no one held these views”. But he said the company did not think that censorship – by demonetising sites that publish extreme views – was a solution to the problem, and instead made it worse.  The result has been a smattering of newsletters leaving for other platforms, a rather larger group of writers expressing their strong discontent with that stance, and a prominent group including Richard Dawkins and Bari Weiss coming out in support of McKenzie. (It’s worth noting here that, unusually for the internet, “nazi” is not hyperbole here: we’re talking about people more than happy to use a swastika to represent themselves.) Then, on Monday, the company changed its tune – but not its policies. In a statement to the Platformer substack newsletter, one of the most successful publications to have threatened to leave, the company said it had reviewed things and decided to remove five newsletters from its platform after coming to the conclusion that they violated existing policies against “credible threats of physical harm”. “If and when we become aware of other content that violates our guidelines, we will take appropriate action,” the co-founders said. The five newsletters they removed had no paying subscribers, and about a hundred “active readers” in total, they told the Verge. It doesn’t seem likely that will be the end of it, does it? Substack is speedrunning a process that every social network – every internet platform – has gone through at some point or another. First, we don’t need moderation (unless you’re a sex worker); then, we care about free speech so we won’t do moderation; then, we’re going to moderate only the most egregious and uncontroversial examples. Remember when Cloudflare defended providing services to 8Chan, until it didn’t? When Mark Zuckerberg, unprompted, explained that Holocaust denial was welcome on Facebook, until it wasn’t? Maybe Substack, uniquely, will hold the line there. Or maybe another controversy will come, sooner rather than later, and it will bend further. If you want to read the complete version of the newsletter please subscribe to receive TechScape in your inbox every Tuesday."
    },
    {
        "id": "43ce98d5-304c-4eba-a844-9d3781234f54",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/09/hyperverse-crypto-promoter-bitcoin-rodney-arrested-and-charged-in-us",
        "title": "HyperVerse crypto promoter ‘Bitcoin Rodney’ arrested and charged in US ",
        "author": "",
        "published_date": "2024-01-08T14:00:44+00:00",
        "content": "A promoter of the HyperVerse crypto investment scheme has been arrested and charged in the US for his alleged role in the scheme, with court documents claiming he was part of a network that made “fraudulent promotional presentations” to investors and potential investors. Rodney Burton, who goes by the name “Bitcoin Rodney”, was arrested in Florida on Friday and remains in custody pending transfer to Maryland, where the charges were laid. He has been charged with operating and conspiring to operate an unlicensed money transmitting business. The US Internal Revenue Service (IRS) alleges in court documents that a network of promoters of the Hyper schemes made “fraudulent promotional presentations” for an investment operation that generated revenue from bitcoin mining, which the IRS alleges did not exist. It is the first time charges have been laid against anyone involved in the HyperVerse scheme. A Guardian investigation has revealed the scheme’s links to Australian Sam Lee and his business partner Ryan Xu, two of the directors of the Australian blockchain company Blockchain Global, which collapsed owing creditors $58m. Lee was the chairman of the HyperTech group, while Xu was listed as the group’s founder. The two men were featured prominently in online promotional material for the Hyper schemes, with both speaking as part of the HyperVerse global launch event in December 2021.   Related: Chief executive of collapsed crypto fund HyperVerse does not appear to exist    The HyperVerse scheme was the subject of multiple consumer warnings from financial authorities around the world, but escaped the attention of regulators in Australia. Lee and Xu have been referred to the Australian Securities and Investments Commission (Asic) for potential breaches of the Corporations Act in relation to Blockchain Global, but Asic has said it does not intend to take action at this time. Neither Lee nor Xu is mentioned in the US court documents relating to the criminal complaint against Burton. Lee did not respond to a request for comment on the arrest; Xu could not be contacted. Burton’s appointed public defence lawyer also did not respond to the Guardian’s request for comment by time of publication. In an affidavit in support of the criminal complaint and arrest warrant lodged in the US district court of Maryland, IRS special agent Andrew Accardi alleges that Burton was a promoter of HyperFund, which also operated under the names HyperTech, HyperCapital, HyperVerse and HyperNation. The court documents, reported by the website courtwatch.news and seen by the Guardian, refer to all of the schemes collectively as HyperFund. The affidavit claims that an IRS analysis of Burton’s personal and company bank records shows that from June 2020 to January 2022, Burton received 562 wire transfers or cashier’s checks, totalling US$7,851,711, from individuals who wished to invest in HyperFund. The affidavit also outlines how the scheme allegedly worked. “HyperFund operated a purportedly legitimate decentralized finance, or ‘DeFi,’ cryptocurrency investment platform,” Accardi states. “A network of HyperFund promoters … made fraudulent promotional presentations to investors and potential investors. In those presentations, promoters touted HyperFund’s investment programs, including the purported returns that prospective investors could earn from investing with HyperFund. Potential investors were told that they could purchase ‘memberships’ in the ‘world’s most sustainable passive rewards program’.” The scheme falsely claimed that investors who purchased memberships would receive daily rewards of between 0.5% to 1% daily, until the investor’s initial investment doubled or tripled in value, the affidavit alleges. “To convince investors that HyperFund could make these daily payments of passive rewards, HyperFund claimed that its payments would be disbursed in part from revenues generated from large-scale crypto mining operations,” the affidavit says. “In fact, HyperFund did not have any such operations. To the extent investors saw the accrual of any rewards, those investors were paid with funds collected from more recent investors.” A scheme using the funds of later investors to pay off early investors is commonly known as a Ponzi scheme, although this term is not used in the court documents. Accardi further alleges that of the 562 payments documented, 342 were made after HyperFund began blocking people from being able to make withdrawals in mid-2021. “In other words, in and after July 2021, Burton took investors’ fiat currency and transferred worthless HU [hyper units] from his HyperFund account to investors’ HyperFund Accounts.”   Related: Investors lose millions as crypto schemes operate unchecked in Australia   Burton, a 54-year-old American, has promoted his wealth on various social media channels, including boasting in March 2022 that he bought two Rolls-Royce cars on the same day. He also recorded himself shopping for a $3.5m yacht, and claims to have bought a diamond-encrusted Audemars Piguet watch worth $1.4m. In February 2021, Burton posted pictures on social media of himself with Ryan Xu in Dubai, calling him a “multi billionaire” and an “amazing human”. In a video posted online of Xu speaking at a dinner with Burton for lunar new year, Xu says that “we are going to change the world together”. Xu says he decided to travel to Dubai to meet HyperFund promoters because it was a “very special moment”. “You guys, for so many hours, fly here to Dubai, so I think I should make my move and at least make everyone think we are family, we are united and we are going to change the world together.” When asked about the future of HyperFund, Xu says: “We are trying to revolutionise the entire financial system.” “If we come together and join the same platform, and we can have the same idea and then we can have the same consensus, and we keep recruiting our army, and one day if our army reaches one billion people, then the whole world belongs to us. That is our vision.” Burton says in the video the plan is “super exciting”. During the Dubai trip, Burton boasted in social media posts about eating steak enrobed with 24-carat gold and claimed he received $1,000 from the “owner and founder of my current crypto program” for a lunar new year gift, along with “a nice Mont Blanc pen from our management team”. He also went on a shopping spree in Dubai, showing purchases from a range of luxury brands, including Louis Vuitton and Balenciaga."
    },
    {
        "id": "367c2f0c-2668-4400-8ced-41728ba390ed",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/08/ai-tools-chatgpt-copyrighted-material-openai",
        "title": "‘Impossible’ to create AI tools like ChatGPT without copyrighted material, OpenAI says",
        "author": "",
        "published_date": "2024-01-08T13:40:19+00:00",
        "content": "The developer OpenAI has said it would be impossible to create tools like its groundbreaking chatbot ChatGPT without access to copyrighted material, as pressure grows on artificial intelligence firms over the content used to train their products. Chatbots such as ChatGPT and image generators like Stable Diffusion are “trained” on a vast trove of data taken from the internet, with much of it covered by copyright – a legal protection against someone’s work being used without permission. Last month, the New York Times sued OpenAI and Microsoft, which is a leading investor in OpenAI and uses its tools in its products, accusing them of “unlawful use” of its work to create their products. In a submission to the House of Lords communications and digital select committee, OpenAI said it could not train large language models such as its GPT-4 model – the technology behind ChatGPT – without access to copyrighted work. “Because copyright today covers virtually every sort of human expression – including blogposts, photographs, forum posts, scraps of software code, and government documents – it would be impossible to train today’s leading AI models without using copyrighted materials,” said OpenAI in its submission, first reported by the Telegraph.   Related: George RR Martin and John Grisham among group of authors suing OpenAI   It added that limiting training materials to out-of-copyright books and drawings would produce inadequate AI systems: “Limiting training data to public domain books and drawings created more than a century ago might yield an interesting experiment, but would not provide AI systems that meet the needs of today’s citizens.” Responding to the NYT lawsuit in a blog post published to its website on Monday, OpenAI said: “We support journalism, partner with news organisations, and believe the New York Times lawsuit is without merit.” Previously, the company said it respected “the rights of content creators and owners”. AI companies’ defence of using copyrighted material tends to lean on the legal doctrine of “fair use”, which allows use of content in certain circumstances without seeking the owner’s permission. In its submission, OpenAI said it believed that “legally, copyright law does not forbid training”. The NYT lawsuit has followed numerous other legal complaints against OpenAI. John Grisham, Jodi Picoult and George RR Martin were among 17 authors who sued OpenAI in September alleging “systematic theft on a mass scale”. Getty Images, which owns one of the largest photo libraries in the world, is suing the creator of Stable Diffusion, Stability AI, in the US and in England and Wales for alleged copyright breaches. In the US, a group of music publishers including Universal Music are suing Anthropic, the Amazon-backed company behind the Claude chatbot, accusing it of misusing “innumerable” copyrighted song lyrics to train its model. Elsewhere in its House of Lords submission, in response to a question about AI safety, OpenAI said it supported independent analysis of its security measures. The submission said it backed “red-teaming” of AI systems, where third-party researchers test the safety of a product by emulating the behaviour of rogue actors. OpenAI is among the companies that have agreed to work with governments on safety testing their most powerful models before and after their deployment, after an agreement struck at a global safety summit in the UK last year."
    },
    {
        "id": "f0be43b2-6a4b-4912-a870-2a2f20d269a2",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/07/artificial-intelligence-surveillance-workers",
        "title": "‘Constantly monitored’: the pushback against AI surveillance at work",
        "author": "",
        "published_date": "2024-01-07T12:00:19+00:00",
        "content": "From algorithms firing staff without human intervention to software keeping tabs on bathroom breaks, technologies including artificial intelligence are already upsetting workers and unsettling workplaces.   Related: Beware the ‘botshit’: why generative AI is such a real and imminent threat to the way we live | André Spicer   At call centers, AI systems record and grade how workers handle calls, often giving failing grades for not sticking to the script. Some corporate software spies on workers to see whether they ever write the word “union” in their emails. As technologies grow ever more sophisticated in monitoring, surveilling and speeding up workers, many workplace experts say US businesses, labor unions and government are not doing nearly enough to protect workers from tech’s downsides. “Workers are being constantly monitored, and AI-based monitoring tools can make mistakes that can translate into unfair pay cuts or firings,” said Virginia Doellgast, a professor of employment relations at Cornell. “Workers often don’t know what monitoring tools are being used, what data the tools are collecting or how that data is used to evaluate their performance.” In Europe, unlike in the US and Canada, many unions have been pushing for years for protections against some of the more intrusive ways that AI tools track and manage workers. “This issue has yet to be put at the center of the radar for unions in North America,” said Valerio De Stefano, a labor law professor at York University in Toronto, who has written extensively on AI’s use in the workplace. “Unions in Europe are more aware of the uses of technologies from the surveillance standpoint. This is not something that unions in North America have focused on.” At some German companies, labor experts say, workers have won protections that could become models for US and Canadian workers. At Deutsche Telekom, Germany’s largest telecommunications company, workers have won a prohibition against algorithms firing workers without any human involvement as well as a ban on using data collected by digital monitoring to discipline or dismiss individual workers. “In Europe, workers have stronger rights to obtain information and participate in decision-making,” Doellgast said. “In the US, where there is a union present, workers have some information rights about AI and hopefully a voice in how it’s used. Where unions aren’t present, workers have no information rights, and all they see is the effects of the technologies on them.”  Mindful of the downsides of AI and algorithmic management, US labor unions are starting to push harder for protections. For instance, at some call centers, the Communications Workers of America union has won not only requirements that managers notify workers whenever recording their calls, but also guarantees that management will only record calls for training purposes to help improve employee performance – and not for evaluating or disciplining workers. Dan Reynolds, the Communications Workers’ assistant research director, said the union has long been concerned about how new technologies affect jobs. “AI is a new technology often used to speed up the work, deskill the work, make workplaces more stressful and make jobs more demanding,” he said. “Our concern about AI isn’t just its effect on the number of jobs, but how it will affect the quality of jobs.” “Our goal is not to stop new technologies,” Reynolds continued, “but to make sure the gains of these new technologies are broadly and equitably shared.” Germany has laws requiring companies to notify their works councils about AI and other new technologies that they’re planning to adopt. Most German companies have work councils, which are worker-management committees that discuss everything from vacation schedules to the pace of work and the effects of AI. To prevent AI-based performance data from being used against individual workers, the work councils at Deutsche Telekom got the company to agree that performance data can be gathered only for groups of at least five employees. The company has also agreed not to use AI to gather certain personal information about employees, such as their political opinions or sexual orientations. Doellgast and De Stefano – who edited a recent academic journal about AI and work – say worker input about new technologies often reduces their invasiveness and other downsides for workers, while making the introduction of technologies smoother and more productive. Moreover, when workers have some say about new technologies, that often reduces employee resistance to those technologies. De Stefano pointed to some problems with using AI to hire and discipline workers. “These machines are, in many cases, unreliable,” he said. “They have certain discriminatory output, especially in hiring. These machines are basically benchmarked around a standard worker – normally white, prime-age, male workers. Anyone who doesn’t correspond to that benchmark risks being misjudged by these algorithms.” The AFL-CIO, the main US labor federation, has created a technology institute to develop expertise and policies on AI and other technologies. That institute is planning training sessions to educate union leaders and strategists about new technologies. “In sectors where performance monitoring and algorithmic management are present, you can have a lot of negative impact,” said Amanda Ballantyne, director of the AFL-CIO’s technology institute. “If you dive into a sector like Amazon, many workers wear wearables that track every movement their body makes, everywhere they go, how fast they complete tasks, how long they’re off task, how long they take in the bathroom. It’s [scientific management] on steroids.” Ballantyne said the communications workers union, the hotel workers union, Sag-Aftra and the Writers Guild have helped lead the way on new technologies. After their recent 148-day strike, the Writers Guild won protections that require studios to disclose whether any material given to writers was developed with the help of AI.  The US Chamber of Commerce says AI and analytics can have substantial benefits for workers and productivity, providing insights into worker performance and allowing for targeted coaching and training to improve performance. AI surveillance, the chamber says, can also help prevent workplace violence by, for instance, monitoring abnormal behavior in the workplace. While many workers criticize the use of “wearable” monitors, the chamber says “smart sensors and wearable devices can help” protect them, by detecting potential accidents, ergonomic risks, toxic chemicals and imminent heat stress. “While there are clear benefits” to AI, said Michael Richards, policy director of the chamber’s technology engagement center, “we understand there are legitimate concerns surrounding the use of the technology.” Employers “recognize that engaging in an inclusive dialogue about using new technologies is critical to fostering a culture of trust with employees”, he added. Annette Bernhardt, director of the technology and work program at the UC Berkeley Labor Center, pointed to the home-care sector as an example where new technologies make workers’ lives more stressful. Many home-care aides must meticulously follow instructions that apps send them, while reporting back each and every task they complete.  “We need strong labor standards around the use of these technologies,” Bernhardt said. “We need to support unions as they bargain around these technologies. Most important, we need to assure that workers have a seat at the table on these technologies from the outset, not just when they’re being implemented.” Bernhardt added: “When workers are at the table, it means better technology adoption for them and better results for employers.”"
    },
    {
        "id": "edee1762-0743-4da3-a795-8e4033fe70cf",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/05/youtuber-tom-scott-ends-things-you-might-not-know-series",
        "title": "‘I am so tired’: YouTuber Tom Scott ends Things You Might Not Know ",
        "author": "",
        "published_date": "2024-01-05T12:00:22+00:00",
        "content": "Ten years ago, Tom Scott held up his phone camera and recorded a 90-second video about traffic lights on bridleways. In Britain, we have pelican crossings, toucan crossings and puffin crossings, he explained, “and then, because all those are named after flying things, we have this: a pegasus crossing. And that is a thing you might not know.” And then, a day later, he posted another “thing you might not know”, about Battersea heliport. And then another, about flammable orange oil. And more and more. The cadence settled down to one a week, but the videos kept coming. And then, just as suddenly as they started, they stopped. On New Year’s Day, exactly a decade since the first Things You Might Not Know video was published, Scott posted his last. “I’ve been throwing stuff at the internet since 1999, and for many, many years, that stuff went almost nowhere,” he said on the valedictory posting. “I remember thinking so many times during all those years … will any of this stuff I’m making ever work? Well, this did.” He is not the first enormously successful YouTuber to step back from the platform. “Creator burnout” is a regularly discussed topic among influencers of all stripes – though often behind closed doors, as online celebrities have learned that seemingly devoted audiences might be unpleasantly unsympathetic to complaints about the grinding schedule of content generation. It has been five years since the beauty influencer Zoe Sugg posted her last Zoella video to the platform, and in the intervening years all but one of her fellow “Brit Crew” generation of YouTubers have slowly dropped off as well. But Scott stands out in quitting on his own terms. He has more than 6 million subscribers on the platform as well as a popular newsletter and podcast – despite which, some readers may be more likely to recognise him from his showing in last year’s Christmas University Challenge for York. “I never got to space. I never got to the ocean depths, and I never got to fly, harnessed, underneath a helicopter. I couldn’t find the excuse to do that one. But I never missed a week,” he said in the final video. If anything, Scott accelerated into the end, filming videos about the National Grid, training boats for cargo ship navigators, and a 10-minute examination of every mistake he has ever made on the channel. So why stop? “This is my dream job and I have a lot of fun doing it,” he says. “I know I’m incredibly lucky, but a dream job is still a job, and it’s a job that keeps getting bigger and more complicated and I am so tired. “I could keep making bigger and better things, keep climbing the ladder, build a business, hire full-time employees, and end up as a manager, and that would be great for someone who isn’t me. But I know I’m bad at that, and I’d hate every second of it. So option two. I could not do that.” Speaking to the Guardian, Scott said the vast changes to YouTube in the years since he started Things You Might Not Know helped settle his choice. “It feels like a lot of people who’ve been making long-form videos have recently announced … not retirements but reductions, step-backs. Everyone I know is noticing their views slowly falling, and therefore their ad revenue reducing.    “YouTube is this strange aberration in the history of internet platforms. No other company decided to share so much of their ad revenue with creators. Whoever made that decision, back when the YouTube Partner Programme started in 2007, must have been taking a wild bet. I’ve no idea how it happened. And it paid off! The result was a whole ‘creator economy’, a brand-new medium that could not have existed any other way, and which has allowed artistic creativity that would otherwise have been impossible. “But building your business on the back of someone else’s business is never a good idea – even if it is the only option. Between the decline in long-form viewers, the threat from junk zero-effort generative AI channels that are only going to get better, and the sheer mass of competing video options … I think it’s going to be a difficult few years.” The internet is very different from when Things You Might Not Know started, but Scott doesn’t think the success he found is necessarily harder to achieve than it was – or easier. “Back when I started, making money from something like this wasn’t really an option, unless you wanted to get hired by some advertising agency or spotted by some talent scout. So while I can’t predict the future, and I’ve been doing this so long that I have no idea what it’s like for people starting out today … I’m pretty sure passionate people will keep creating stuff anyway.”"
    },
    {
        "id": "0f473e05-1abe-4e62-a105-7a7123329f9e",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/04/self-driving-cars-exempt-traffic-tickets-san-francisco-autonomous-vehicle",
        "title": "Why are self-driving cars exempt from traffic tickets in San Francisco?",
        "author": "",
        "published_date": "2024-01-04T20:50:13+00:00",
        "content": "Autonomous vehicles in San Francisco are exempt from traffic tickets if there is nobody in the driver’s seat, according to the San Francisco police department (SFPD), underscoring ongoing legal and safety concerns surrounding the expanding technology. California law has not caught up to the cars, even though they are already on the road, say public safety agencies and experts.   Related: Cruise robotaxi service hid severity of accident, California officials claim   SFPD policy states that officers can make a traffic stop of autonomous vehicles (AVs) for violations, but can only issue a citation if there is a safety driver in the vehicle overseeing its operations. Since June 2022, autonomous vehicles have been permitted to operate without safety drivers as long as they are inside the city limits. Officers can issue citations to the registered owner of an unoccupied vehicle in absentia for non-moving violations such as parking or registration offenses but not violations like speeding, running a red light, driving in the wrong lane or making an illegal turn. “At this time, no citation for a moving violation can be issued if the AV is being operated in a driverless mode,” the SFPD said in a statement. The police did not elaborate as to why no traffic citations can be given to self-driving cars. The agency’s response comes after NBC Bay Area published an internal memo on the matter from the SFPD chief, Bill Scott. Driverless cars’ exemption from traffic citations highlights concerns experts have that laws have been slow to catch up to driverless technology. While other states that have approved autonomous vehicles – including Texas and Arizona – updated their laws to adapt to self-driving cars, California has not. Arizona changed its road safety protocols in 2018 to state that if an autonomous vehicle violates a traffic law, an “officer may issue a citation to the registered owner of the vehicle” even if no human is present. A 2017 update to Texas law states that the owner of an automated driver system is considered the operator of the vehicle in the case of a traffic violation, “regardless of whether the person is physically present in the vehicle while the vehicle is operating”. California, largely seen as the US’s largest market for robotaxis, does not have such legal updates. More than 500 autonomous vehicles between firms Waymo and Cruise were operational in San Francisco as of August even before the city approved an expansion, compared with 125 in Austin, Texas, for example. “It’s a big issue because California is where the majority of robotaxis are operating,” said Cassandra Burke Robertson, a professor at Case Western Reserve University school of law who has published analyses of liability law as it pertains to self-driving vehicles. “It’s not just about traffic tickets, but how we will regulate autonomous vehicles more broadly.” Another public safety agency threw up its hands at the question of traffic tickets for autonomous vehicles, disclaiming that it could not enforce laws that did not exist. In a statement, the California department of motor vehicles said that as part of the permitting process to hit the roads, an autonomous vehicle must be certified to comply with “all provisions of the California Vehicle Code”. Though the vehicle code requires that robotaxis comply with traffic laws, absent from it are clauses specifying how law enforcement should handle a misbehaving self-driving car and what penalties apply.  “The DMV recognizes that autonomous technology is an evolving technology and remains committed to enhancing the regulatory structure to reflect the continued development of the technology,” the agency said.    Related: Cruise recalls all self-driving cars after grisly accident and California ban   While the SFPD clarified it cannot issue tickets to autonomous vehicles, the California highway patrol did not respond to a request for comment on its own policies on robotaxis. The Los Angeles police department, which has allowed self-driving cars in a limited capacity as of October 2023, did not immediately respond to a request for comment about whether autonomous vehicles can be ticketed in the city. The traffic ticket issue comes as San Francisco grapples with safety concerns surrounding driverless vehicles, which first came to the city in a limited capacity in 2022. In August 2023, self-driving car companies Waymo and Cruise were approved to expand operations following a controversial vote, despite complaints about a number of issues including vehicles blocking emergency response teams. In the weeks and months that followed the proliferation of the cars, locals reported the vehicles were causing “mayhem” in the city, and Cruise was forced to pull its driverless cars off the road after one of its vehicles was involved in a high-profile crash that nearly killed a pedestrian in October. Officials have accused the General Motors subsidiary of withholding crucial information on the accident, during which the pedestrian was dragged under the car. Waymo did not immediately respond to a request for comment. Cruise, which is not currently operating in a fully driverless capacity in San Francisco, declined to comment. After Cruise pulled its cars from the streets of San Francisco, autonomous vehicle firms may be rethinking their strategies – which was a good thing for public safety, Robertson said. “Cruise tried to ramp up very quickly in 2023, and that’s when we started to hear reports of problems in San Francisco,” she said. “In 2024, they’re taking a step back and saying they are going to work closely with local officials.”"
    },
    {
        "id": "9a6a1ab2-c0ba-41b2-92c6-9d40e514e469",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/04/asic-faces-questions-over-failure-to-warn-consumers-about-hyperverse-crypto-scheme",
        "title": "Asic faces questions over failure to warn consumers about HyperVerse crypto scheme",
        "author": "",
        "published_date": "2024-01-04T14:00:06+00:00",
        "content": "The assistant treasurer, Stephen Jones, has questioned why Australia’s consumer watchdog did not issue a consumer warning against the HyperVerse crypto investment scheme in line with a number of overseas regulators. A Guardian Australia investigation has revealed widespread losses to the HyperVerse scheme, which escaped regulator attention in Australia despite one overseas authority warning it was a possible “scam” and another describing it as a “suspected pyramid scheme”. The HyperVerse scheme was run by an organisation called HyperTech. Australian blockchain entrepreneur Sam Lee was chairman of the HyperTech group, while his business partner Zijing “Ryan” Xu was listed as the group’s “founder”. The pair were also directors of the Australian bitcoin company Blockchain Global, which collapsed in 2021 owing creditors $58m. A report from US-based blockchain analysts Chainalysis estimates consumer losses to HyperVerse in 2022 amounted to US$1.3bn. HyperVerse was previously known as HyperFund and appears to have undergone various rebrandings as it sought to attract more members. Jones said he would be asking the Australian Securities and Investments Commission (Asic) why there was no consumer warning issued in Australia about the HyperTech schemes as occurred in the UK, New Zealand, Canada, Germany and Hungary, among others, as early as 2021.   Related: Chief executive of collapsed crypto fund HyperVerse does not appear to exist    Jones told Guardian Australia the schemes appeared to be selling “worthless investment products” and “tragically, a bunch of Australians got caught up in it”. “This type of scheme works by convincing innocent people to invest their money into a product that might not exist, with the only source of income being money from new investors,” Jones alleged. “I simply don’t know why a warning wasn’t issued. “It seemed pretty clear that there should have been concerns raised about … this operation.” HyperFund and HyperVerse were described in promotional material online as “membership schemes” in which people were asked to pay in cryptocurrency for subscription packages, with rewards accumulating in “hyper units” at a daily rate of 0.5%. Members were also incentivised to recruit new members. Investors were trained to build their “trees” and build a “community”, with people moving up a ranking system based on the number of people they brought into the scheme. While initial investors were able to cash out their hyper units, convert to other cryptocurrencies or withdraw funds, many later investors say they have lost their money.  Sign up for a weekly email featuring our best reads  In August 2022, the Hungarian central bank released a public statement comparing the system underpinning HyperVerse and HyperFund to a “suspected pyramid scheme”, “behind which there is no real economic activity, the only income of the system is the payments of new entrants”. A September 2021 public warning from New Zealand’s Financial Market Authority stated: “The FMA are concerned HyperFund may be operating a scam.” It later included HyperVerse in that warning. In HyperVerse and HyperFund, early investors were able to make withdrawals, but many later investors have said they lost their deposits. This has led people in online forums who invested in the schemes to accuse the company of using new membership funds to pay out returns for early investors, with no actual enterprise taking place. A separate investment platform promoted by Lee – called We Are All Satoshi – was the subject of a “desist and refrain order” from California’s Commissioner of Financial Protection and Innovation in September 2023. It alleged that We Are All Satoshi was a “fraudulent pyramid and Ponzi scheme”, and “does not sell or purport to sell any actual product and has no apparent source of revenue other than funds received from investors”. It named Lee as the “founder, CEO and chairman” of We Are All Satoshi and alleged he was targeting investors in the state, breaching multiple provisions of the state’s corporations code and ordering him to stop “until the qualification requirements” under California law were met. Xu is not named in the order and there is no suggestion he was involved in We Are All Satoshi. The Guardian has been unable to contact him for comment. Lee and Xu have not responded to questions from Guardian Australia about the schemes but Lee has previously denied allegations by investors that HyperVerse was a scam. “No, because if it was a scam, the website would be offline and I wouldn’t be even wasting my time trying to get the information from the community in order to hold corporate accountable,” he said in a February 2023 Zoom meeting with investors. Lee did not respond to questions from the Guardian about his involvement in the establishment and operation of HyperFund and HyperVerse before the publication of a previous Guardian Australia article.   Related: Investors lose millions as crypto schemes operate unchecked in Australia   In a WhatsApp message after the article was published he alleged it included “misstatements” about his role in running the Hyper schemes, but did not respond when asked what they were. He also claimed that “people on the internet continues [sic] to make things up”. Separately to the HyperTech group of investment schemes, Lee and Xu were also behind the collapsed crypto exchange platform Blockchain Global, which owes creditors $58m. In October the liquidator for Blockchain Global said in a publicly available report that last year he had referred Lee and Xu to Asic, alleging that they “may have contravened” the Corporations Act, and listing a range of potential breaches. The liquidator’s report makes a number of allegations about the running of the business by former directors and key personnel and states that he has been unable to progress his own examination of Lee and Xu as they now live overseas and he “was unable to effect service of the summonses on them”. Asic said it did not intend to take any further action at this time. Jones said he expected the regulator to use all available powers to investigate the schemes and investor losses and, if wrongdoing was found, hold those responsible to account. “I think it sends a powerful message if the regulators are going after them using every tool that is available to them to ensure that they are brought to account,” he said. Speaking generally, Jones indicated the government intended to do more to crack down on the distribution methods for unlicensed investment schemes, with consultation under way on a new code of conduct for social media companies, telecommunication companies and banks. “It’s about removing the distribution channels or locking down the distribution channels and putting obligations on the social media platforms … to pull down scam and fake investment promotions – that is all key,” Jones said. “This is about what the obligation should be upon all of those bodies to lift the bar and put in place more protective standards and there will be penalties and a liability if they haven’t met those standards.“We’re picking on those because they’re the key parts of the ecosystem.” He said Asic had taken down 3,000 websites in the past four months – about half of which were fake investment opportunities – comparing government efforts to tackle the proliferation of such schemes to a “game of whack-a-mole”."
    },
    {
        "id": "ff40ca9a-9a2e-4a3a-b133-6fba1fa5afca",
        "type": "article",
        "section": "technology",
        "url": "https://www.theguardian.com/technology/2024/jan/04/five-firms-to-watch-in-a-time-of-turbulence-humanforest-brewdog-saatchi-daaci-genflow",
        "title": "From HumanForest to BrewDog: five firms to watch in a time of turbulence",
        "author": "",
        "published_date": "2024-01-04T12:00:03+00:00",
        "content": "After a year in which industry was knocked off its axis by the coming of age of artificial intelligence and the transition to an online world continued apace, new businesses are emerging and old industries reinventing themselves to adapt. Here, we look at five companies making the most of these turbulent times. HumanForest It’s been a difficult year for the operators of electric scooters, bikes and mopeds: most notably in Paris, where its e-scooter rental scheme was shut down by city authorities after a popular vote. One big player, Tier, nominated here a year ago as a company to watch, also lost its business in London when trial licences were renewed. Increasingly, in the crowded streets of the UK, rental ebikes are looking a better bet than the e-scooter: a more familiar mode of transport for occasional users, feeling safer and with the bonus of sitting rather than standing. One barometer of ebikes’ potential is a small London firm, HumanForest, starting to claw out a bigger niche. It is aiming to undercut rivals by extending “free” rides through selected advertising in its app, and trump them with ethical credentials as a B Corp, in a sector where zero emissions are a basic USP. Forest bike rides doubled last year after it won licences to operate in several of the capital’s boroughs, and the company now plans to expand into France after raising more backing from venture capital in a recent funding round. Quite how pure it can stay on an ad-based model – the latest big advertiser is the Mars corporation, rather than Greenpeace – remains to be seen. Gwyn Topham M&C Saatchi As the advertising industry faces a once-in-15-years downturn, the embattled agency M&C Saatchi is one to watch. Founded in 1995 by Maurice and Charles Saatchi, the agency enjoyed decades of success and international expansion, with memorable work including the 1997 “demon eyes” poster for the Conservatives. However, an accounting scandal four years ago brought the group to its knees and led to the resignation of Lord Maurice Saatchi and several board directors. The group resisted two takeover approaches, although one bidder remains its largest shareholder, and frantic restructuring has ensued to try to right the ship. In September, Zillah Byng-Thorne, who successfully and ruthlessly transformed the fortunes of the magazine publisher Future, moved up to executive chair. The hunt is on for a new global chief executive. “In Zillah we trust,” crowed one analyst in a note to investors. M&C Saatchi’s share price remains almost 40% down on its pre-accounting scandal level. In the face of the worst recession since the financial crisis of 2008, this year could be make or break for one of Britain’s most famous advertising agencies. Mark Sweney Daaci It was a transformative year for artificial intelligence, with large language models such as ChatGPT making the technology ubiquitous across business. But AI’s use in the music industry has been contentious, from Nick Cave’s both-barrels condemnation and the quirky AIsis (an imagined heyday Oasis album with generated Liam Gallagher vocals) to a YouTube deal with John Legend and Sia to offer versions of their singing voices for creator videos. Music purists say the use of AI is a cheat, undermining human creativity. Advocates say technological innovation has always been present in music. Into this row pitch London-based Daaci. A year ago, the composition and editing platform joined Abbey Road Red, a music tech incubation programme that is part of the renowned London studio. Daaci is based on a PhD project conducted with the UK Research and Innovation AI centres for doctoral training, and its technology can pass a Turing intelligence test. Daaci’s technology has multiple functions: to create music with varying levels of user input into its genre, mood and instrumentation. It can also detect when specific parts of songs have been used, allowing artists and composers to be paid when their work is reused. The company is targeting growing industries, including social media marketing and video game production. Its founders say its tech differs from rivals as it doesn’t ingest existing back catalogues and generate whole songs, but has been built by composers, artists and producers to create everything from parts of tracks to whole libraries. The chief executive, Rachel Lyske, a vocalist and composer, said it was “interpreting what has come from a composer’s brain” rather than trying to imitate something already recorded. More features are expected to be released next year. Alex Lawson BrewDog Will 2024 be the year that the self-consciously brash Aberdeenshire-based beer brand BrewDog floats on the stock market? The maker of Punk IPA has been pondering a Punk IPO for some time, with London and New York possible destinations, but reputational problems have created an unwelcome backdrop to plans for a public offering. In 2021, staff accused BrewDog’s chief executive and co-founder, James Watt, of presiding over a “culture of fear” in which workers were mistreated. The company disputed some of the allegations, apologised for others, and lost its B Corp status, the label given to the kind of company BrewDog publicly aspires to be – modern, inclusive and respectful of labour rights. BrewDog has been uncharacteristically shy of controversy for a while, though, which will please would-be investors who prefer a quiet life. Its chairman, Allan Leighton, is practically the definition of the City guru capable of steering a private company into the public sphere. It is still achieving strong global growth, including via a tie-up with Budweiser in China. Everything is in place for a float that could see BrewDog’s “punks” – the equity investors who crowdfunded its initial growth – cash in at last, alongside their private equity co-investors, of course. Watt, who once threw taxidermy cats from a helicopter over the City of London in a publicity stunt intended to mock the financial world’s “fat cats”, could now be in line for his own big payday. Rob Davies Genflow Biosciences Longevity is not just for billionaires, says Dr Eric Leire, the founder and chief executive of Genflow Biosciences, which in December 2022 became the first anti-ageing biotech to list in Europe. The London-listed and headquartered company, which has its labs in Belgium and an office in Cambridge, Massachusetts, is developing gene therapies for longer, healthier lives. The company’s work is based on a rare mutation of the SIRT6 gene found in people who live beyond the age of 100, which was discovered by Dr Vera Gorbunova’s team at the University of Rochester in 2019. She chairs Genflow’s scientific advisory board. “Some people are lucky to have the good gene, and what we want to do is share that luck with everybody,” says Leire, who previously worked for Pfizer and Schering-Plough. Genflow plans to test an experimental therapy on people with an age-related liver disease called NASH in early 2025. It is also developing a therapy for Werner syndrome, a rare accelerated ageing disease, by which SIRT6 messenger RNA delivers the gene to the skin. Genflow is also launching a programme to prevent age-related loss of muscle mass, funded by new research grants. It started trading on the OTCQB venture market in the US in the summer, to expand its reach to US investors. As an early-stage biotech, Genflow is a risky investment; its London-listed shares have lost nearly 90% of their value in the first year of trading. But it is certainly one to watch. The longevity science sector is forecast to reach $2bn (£1.6bn) by 2030, according to Insight Analytics. Julia Kollewe"
    }
]